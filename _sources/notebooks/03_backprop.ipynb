{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "217dd62c",
   "metadata": {},
   "source": [
    "# 03: Back Propagation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f48ee4d",
   "metadata": {},
   "source": [
    "![](../img/03_forward_backward.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef41a7e6",
   "metadata": {},
   "source": [
    "The training loop for a neural network involves:\n",
    "\n",
    "1. A **forward pass**: Feed the input features ($x_1$, $x_2$) through all the layers of the network to compute our predictions, $\\hat{y}$.\n",
    "\n",
    "2. Compute the **loss** (or cost), $\\mathcal{L}(y, \\hat{y})$, a function of the predicted values $\\hat{y}$ and the actual values $y$.\n",
    "\n",
    "3. A **backward pass** (or _back propagation_): Feed the loss $\\mathcal{L}$ back through the network to compute the rate of change of the loss (i.e. the derivative) with respect to the network parameters (the weights and biases for each node, $w$, $b$)\n",
    "\n",
    "4. Given their derivatives, update the network parameters ($w$, $b$) using an algorithm like gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aff8139",
   "metadata": {},
   "source": [
    "## Forward Pass: Recap\n",
    "\n",
    "Each node computes a linear combination of the output of all the nodes in the previous layer, for example:\n",
    "$$\n",
    "z_1^{[1]} = w_{1,1}^{[1]} x_1 + w_{2,1}^{[1]} x_2 + b_1^{[1]}\n",
    "$$\n",
    "\n",
    "This is passed to an activation function, $g$, (assumed to be the same function in all layers here), to create the final output, or \"activation\", of each node:\n",
    "\n",
    "$$\n",
    "a_3^{[2]} = g(z_3^{[2]})\n",
    "$$\n",
    "\n",
    "For example, $\\hat{y}$, can be expressed in terms of the activation of the final layer as follows:\n",
    "\n",
    "$$\n",
    "\\hat{y} = a_1^{[3]} = g\\left(w_{1,1}^{[3]} a_{1}^{[2]} + w_{2,1}^{[3]} a_{2}^{[2]} + w_{3,1}^{[3]} a_{3}^{[2]} + b_1^{[3]}\\right)\n",
    "$$\n",
    "\n",
    "I'm going to introduce a more efficient syntax in a moment but terms not introduced above mean:\n",
    "- $w_{j,k}^{[l]}$: The weight between node $j$ in layer $l-1$ and node $k$ in layer $l$.\n",
    "- $a_k^{[l]}$: The activation of node $k$ in layer $l$\n",
    "- $b_k^{[l]}$: The bias term for node $k$ in layer $l$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3117a35",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "Let's consider a simpler network, with one input, two hidden nodes, and one output:\n",
    "\n",
    "![](../img/03_backprop_example_params.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ed8bfe",
   "metadata": {},
   "source": [
    "Here I've also included a node after the network's output to represent the calculation of the loss, $\\mathcal{L}(y, \\hat{y})$, where $\\hat{y} = g(z_1^{[2]})$ is the predicted value from the network and $y$ the true value.\n",
    "\n",
    "This network has seven parameters: $w_1^{[1]}$, $w_2^{[1]}$, $b_1^{[1]}$, $b_2^{[1]}$, $w_1^{[2]}$, $w_2^{[2]}$, $b_1^{[2]}$\n",
    "\n",
    "In gradient descent we use the partial derivative of the loss function with respect to the parameters to update the network, making small changes to the parameters like:\n",
    "\n",
    "$$\n",
    "w_1^{[1]}  = w_1^{[1]} - \\alpha\\frac{\\partial \\mathcal{L}}{\\partial w_1^{[1]}}\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is the learning rate.\n",
    "\n",
    "So to perform gradient descent we need the derivatives for each parameter, i.e. we need to compute:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w_1^{[1]}},\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w_2^{[1]}},\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b_1^{[1]}},\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b_2^{[1]}},\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w_1^{[2]}},\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w_2^{[2]}},\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b_1^{[2]}}\n",
    "$$\n",
    "\n",
    "How can we compute all those terms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02243aaa",
   "metadata": {},
   "source": [
    "## Background: Chain Rule\n",
    "\n",
    "computation graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ac753b",
   "metadata": {},
   "source": [
    "$$\n",
    "h(x) = f(g(x))\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\mathrm{d} h(x)}{\\mathrm{d}x} = \\frac{\\mathrm{d} f(u)}{\\mathrm{d}u} \\frac{\\mathrm{d} g(x)}{\\mathrm{d}x} \\\\\n",
    "h'(x) = f'(u)g'(x)\n",
    "$$\n",
    "\n",
    "\n",
    "e.g.\n",
    "\n",
    "$$\n",
    "f(u) = u^2 \\\\\n",
    "g(x) = e^x + x  \\\\\n",
    "h(x) = f(g(x)) = (e^x + x)^2 \\\\\n",
    "$$\n",
    "\n",
    "$$\n",
    "g'(x) = e^x + 1 \\\\\n",
    "f'(u) = 2u \\\\\n",
    "u = g(x) = e^x + x \\\\\n",
    "h'(x) = 2(e^x + x)(e^x + 1) \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3abc900",
   "metadata": {},
   "source": [
    "### Multi-variate chain rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f5e155",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b460a82f",
   "metadata": {},
   "source": [
    "## Back Propagation\n",
    "\n",
    "Here's the example network again, but with each edge (arrow) labelled by the partial derviative between the two connected nodes:\n",
    "\n",
    "![](../img/03_backprop_example_diffs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfac8ca",
   "metadata": {},
   "source": [
    "## Multiple Paths\n",
    "\n",
    "![](../img/03_backprop_multipath.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434c30e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
