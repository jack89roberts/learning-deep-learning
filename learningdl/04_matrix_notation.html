

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>04: Matrix Notation &#8212; Learning Deep Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'learningdl/04_matrix_notation';</script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="05: Gradients and Activation Functions" href="05_activation.html" />
    <link rel="prev" title="03: Back Propagation" href="03_backprop.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Learning Deep Learning [WIP!]
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Fully Connected Neural Networks</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01_linear_regression.html">01: Linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_logistic_regression.html">02: Logistic Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_backprop.html">03: Back Propagation</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">04: Matrix Notation</a></li>
<li class="toctree-l1"><a class="reference internal" href="05_activation.html">05: Gradients and Activation Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="06_nonlinear_regression.html">06: Non-linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="07_multiclass.html">07: Multiple Outputs</a></li>
<li class="toctree-l1"><a class="reference internal" href="08_regularisation.html">08 Regularisation</a></li>
<li class="toctree-l1"><a class="reference internal" href="09_optimisers.html">09: Optimisation Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="10_experiments.html">10: Training and Tuning Networks</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/jack89roberts/learning-deep-learning" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/jack89roberts/learning-deep-learning/edit/main/learningdl/04_matrix_notation.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/jack89roberts/learning-deep-learning/issues/new?title=Issue%20on%20page%20%2Flearningdl/04_matrix_notation.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/learningdl/04_matrix_notation.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>04: Matrix Notation</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-network-and-definitions">Example Network and Definitions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#notation-cheat-sheet">Notation Cheat Sheet</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#forward-pass">Forward Pass</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#value-for-a-single-node-and-data-sample-dot-products">Value for a single node and data sample: Dot products</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#values-for-all-nodes-in-a-layer-for-all-data-in-a-batch-matrix-multiplication">Values for all nodes in a layer for all data in a batch: Matrix multiplication</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#broadcasting">Broadcasting</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#backward-pass">Backward Pass</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-equations">Key Equations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#terms-needed-for-partial-mathcal-l-partial-mathbf-z-l">Terms needed for <span class="math notranslate nohighlight">\(\partial \mathcal{L} / \partial \mathbf{Z}^{[l]}\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#intuition-why-are-these-the-right-equations">Intuition - Why are these the right equations?</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#checking-dimensions">Checking Dimensions</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#activations">Activations</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-predictor">Linear Predictor</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#bias">Bias</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#weights">Weights</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-matrices">Why matrices?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-gpus">Why GPUs?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-about-tpus">What about TPUs?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="matrix-notation">
<h1>04: Matrix Notation<a class="headerlink" href="#matrix-notation" title="Permalink to this heading">#</a></h1>
<p>So far we’ve explicitly written individual terms for every parameter, data feature, and intermediate value in the network in the examples. This makes it clear exactly what is going on at each step, but it gets clumsy, long, and inefficient.</p>
<p>Here we show how we can represent networks with matrices, creating a much simpler (if less explicit) notation.</p>
<section id="example-network-and-definitions">
<h2>Example Network and Definitions<a class="headerlink" href="#example-network-and-definitions" title="Permalink to this heading">#</a></h2>
<p>We’ll refer back to the network below throughout this section. It has an unspecified number of layers (and nodes in each layer), but we assume 2 input data features, 3 nodes in the first layer, and 1 output node for the examples.</p>
<p><img alt="" src="../_images/04_matrix_network.png" /></p>
</section>
<section id="notation-cheat-sheet">
<h2>Notation Cheat Sheet<a class="headerlink" href="#notation-cheat-sheet" title="Permalink to this heading">#</a></h2>
<p>We have labelled the notation we’re going to start to use to represent the values/parameters in whole layers, rather than at individual nodes/connections. The terms are:</p>
<p><strong>Size of the network and dataset:</strong></p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(L\)</span> - the number of layers in the network</p></li>
<li><p><span class="math notranslate nohighlight">\(n^{[l]}\)</span> - the number of nodes in layer <span class="math notranslate nohighlight">\([l]\)</span></p>
<ul>
<li><p><span class="math notranslate nohighlight">\(n^{[0]}\)</span> - the number of input features (the number of nodes <span class="math notranslate nohighlight">\(x_i\)</span> in the input layer)</p></li>
<li><p><span class="math notranslate nohighlight">\(n^{[L]}\)</span> - the number of outputs (one in all the networks we’ve considered so far)</p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(m\)</span> - the number of data samples per gradient descent iteration (aka the batch size)</p></li>
</ul>
<p><strong>Parameters of, and computed values in, the network:</strong></p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf{W}^{[l]}\)</span> - the weights (arrows) between all the activations in layer <span class="math notranslate nohighlight">\([l-1]\)</span> and layer <span class="math notranslate nohighlight">\([l]\)</span>, dimensions: <span class="math notranslate nohighlight">\((n^{[l]}, n^{[l-1]})\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{b}^{[l]}\)</span> - the bias for all the nodes in layer <span class="math notranslate nohighlight">\([l]\)</span>, dimensions: <span class="math notranslate nohighlight">\((n^{[l]}, 1)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{Z}^{[l]}\)</span> - the linear predictor for all the nodes in layer <span class="math notranslate nohighlight">\([l]\)</span> for all the data samples, dimensions: <span class="math notranslate nohighlight">\((n^{[l]}, m)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{A}^{[l]}\)</span> - the activation for all the nodes in layer <span class="math notranslate nohighlight">\([l]\)</span> for all the data samples, dimensions: <span class="math notranslate nohighlight">\((n^{[l]}, m)\)</span></p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\mathbf{X} = \mathbf{A}^{[0]}\)</span> - we can represent the input dataset as the activations of the zeroth layer</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{\hat{y}} = \mathbf{A}^{[L]}\)</span> - the activations of the last layer are the predictions <span class="math notranslate nohighlight">\(\hat{y}\)</span></p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{L}(\mathbf{y}, \mathbf{\hat{y}})\)</span> - the total loss (cost) for all the data samples, dimensions: (1)</p></li>
</ul>
<p><strong>Matrix/vector operations:</strong></p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf{p . q}\)</span> is the dot product of a row vector <span class="math notranslate nohighlight">\(\mathbf{p}\)</span> with dimensions <span class="math notranslate nohighlight">\((1, j)\)</span>, and a column vector <span class="math notranslate nohighlight">\(\mathbf{q}\)</span> with dimensions <span class="math notranslate nohighlight">\((j, 1)\)</span>, and the result is a scalar value</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{PQ}\)</span> is matrix multiplication, where <span class="math notranslate nohighlight">\(\mathbf{P}\)</span> has dimensions <span class="math notranslate nohighlight">\((i, k)\)</span>, <span class="math notranslate nohighlight">\(\mathbf{Q}\)</span> has dimensions <span class="math notranslate nohighlight">\((k, j)\)</span>, and <span class="math notranslate nohighlight">\(\mathbf{PQ}\)</span> has dimensions <span class="math notranslate nohighlight">\((i, j)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{P * Q}\)</span> is element-wise multiplication, where <span class="math notranslate nohighlight">\(\mathbf{P}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{Q}\)</span> have the same dimensions <span class="math notranslate nohighlight">\((i, j)\)</span>, and <span class="math notranslate nohighlight">\(\mathbf{P * Q}\)</span> also has dimensions <span class="math notranslate nohighlight">\((i, j)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{P}^T\)</span> is the <em>transpose</em> of <span class="math notranslate nohighlight">\(\mathbf{P}\)</span>, where <span class="math notranslate nohighlight">\(\mathbf{P}\)</span> has dimensions <span class="math notranslate nohighlight">\((i, j)\)</span> and <span class="math notranslate nohighlight">\(\mathbf{P}^T\)</span> has dimensions <span class="math notranslate nohighlight">\((j, i)\)</span></p></li>
</ul>
</section>
<section id="forward-pass">
<h2>Forward Pass<a class="headerlink" href="#forward-pass" title="Permalink to this heading">#</a></h2>
<section id="value-for-a-single-node-and-data-sample-dot-products">
<h3>Value for a single node and data sample: Dot products<a class="headerlink" href="#value-for-a-single-node-and-data-sample-dot-products" title="Permalink to this heading">#</a></h3>
<p>In the network above, the value of <span class="math notranslate nohighlight">\(z_1^{[1](j)}\)</span> (the first node in the first hidden layer) for a single data point, <span class="math notranslate nohighlight">\((j)\)</span>, can be written as:</p>
<div class="math notranslate nohighlight">
\[
z_1^{[1](j)} = w_{1 \rightarrow 1}^{[1]} x_1^{(j)} + w_{2 \rightarrow 1}^{[1]} x_2^{(j)} + b_1^{[1]}
\]</div>
<p>The first two terms on the right (multiplying the inputs by the weights) can be expressed as a <em>dot product</em>:</p>
<div class="math notranslate nohighlight">
\[
z_1^{[1](j)} = \mathbf{w_{1}}^{[1]} \mathbf{. x}^{(j)} + b_1^{[1]}
\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf{w_1}^{[1]}\)</span> is a <em>row vector</em> of all the weights to the first node in the first layer</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{x}^{(j)}\)</span> is a <em>column vector</em> of all the data feature values for data sample <span class="math notranslate nohighlight">\((j)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(b_1^{[1]}\)</span> is the bias term for the first node in the first layer</p></li>
</ul>
<p>which can be expanded as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
z_1^{[1](j)} =
\begin{bmatrix}w_{1 \rightarrow 1}^{[1]} &amp; w_{2 \rightarrow 1}^{[1]}\end{bmatrix}
\begin{bmatrix}
x_{1}^{(j)} \\
x_{2}^{(j)} \\
\end{bmatrix}
 + b_1^{[1]}
\end{split}\]</div>
<p>and represents the same expression as the first equation above.</p>
</section>
<section id="values-for-all-nodes-in-a-layer-for-all-data-in-a-batch-matrix-multiplication">
<h3>Values for all nodes in a layer for all data in a batch: Matrix multiplication<a class="headerlink" href="#values-for-all-nodes-in-a-layer-for-all-data-in-a-batch-matrix-multiplication" title="Permalink to this heading">#</a></h3>
<p>By moving from vectors to matrices we can represent the terms for a whole layer (rather than a single node) and for a whole dataset in an efficient way.</p>
<p>Sticking to the the first layer in the network above as an example, the six weights (2 input nodes * 3 layer 1 nodes) can be represented as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{W}^{[1]} = 
\begin{bmatrix}
w_{1 \rightarrow 1}^{[1]} &amp; w_{2 \rightarrow 1}^{[1]} \\
w_{1 \rightarrow 2}^{[1]} &amp; w_{2 \rightarrow 2}^{[1]} \\
w_{1 \rightarrow 3}^{[1]} &amp; w_{2 \rightarrow 3}^{[1]}
\end{bmatrix}
\end{split}\]</div>
<p>where:</p>
<ul class="simple">
<li><p>each <em>row</em> corresponds to a node in the current layer (the 1st layer in this case) - there are <span class="math notranslate nohighlight">\(n^{[1]}\)</span> rows.</p></li>
<li><p>each <em>column</em> corresponds to a node in the previous layer (the zeroth layer in this case, aka the data inputs) -  there are <span class="math notranslate nohighlight">\(n^{[0]}\)</span> columns.</p></li>
<li><p>the values are the weights between those nodes (e.g. the value at row 3, column 2 is the weight between the 2nd input, <span class="math notranslate nohighlight">\(x_2\)</span>, and the 3rd node in the first layer).</p></li>
</ul>
<p>And the data as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{X} = \mathbf{A}^{[0]} =
\begin{bmatrix} 
x_{1}^{(1)} &amp; x_{1}^{(2)} &amp; x_{1}^{(3)} &amp; \dots &amp; x_{1}^{(m)} \\
x_{2}^{(1)} &amp; x_{2}^{(2)} &amp; x_{2}^{(3)} &amp; \dots &amp; x_{2}^{(m)} \\
\end{bmatrix}
\end{split}\]</div>
<p>where:</p>
<ul class="simple">
<li><p>each <em>row</em> corresponds to a feature in the data (or more generally a node activation in the previous layer) - there are <span class="math notranslate nohighlight">\(n^{[0]}\)</span> rows.</p></li>
<li><p>each <em>column</em> corresponds to a data sample - there are <span class="math notranslate nohighlight">\(m\)</span> columns.</p></li>
</ul>
<p>The linear predictor values for all the nodes in the first layer can then be expressed as:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{Z}^{[1]} = \mathbf{W}^{[1]} \mathbf{X} + \mathbf{b}^{[1]}
\]</div>
<p>The first term, <span class="math notranslate nohighlight">\(\mathbf{W}^{[1]} \mathbf{X}\)</span>, is a <em>matrix multiplication</em>, that (by the definition of matrix multiplication and given the way we have defined <span class="math notranslate nohighlight">\(\mathbf{W}^{[1]}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>), contains the dot product of the inputs and the weights for every node (each of the 3 nodes in the first layer) and every data sample:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{W}^{[1]} \mathbf{X} =
\begin{bmatrix}
\mathbf{w_{1}}^{[1]} . \mathbf{x}^{(1)} &amp; \mathbf{w_{1}}^{[1]} . \mathbf{x}^{(2)} &amp; \mathbf{w_{1}}^{[1]} . \mathbf{x}^{(3)} &amp; \dots &amp;  \mathbf{w_{1}}^{[1]} . \mathbf{x}^{(m)} \\
\mathbf{w_{2}}^{[1]} . \mathbf{x}^{(1)} &amp; \mathbf{w_{2}}^{[1]} . \mathbf{x}^{(2)} &amp; \mathbf{w_{2}}^{[1]} . \mathbf{x}^{(3)} &amp; \dots &amp;  \mathbf{w_{1}}^{[1]} . \mathbf{x}^{(m)} \\
\mathbf{w_{3}}^{[1]} . \mathbf{x}^{(1)} &amp; \mathbf{w_{3}}^{[1]} . \mathbf{x}^{(2)} &amp; \mathbf{w_{3}}^{[1]} . \mathbf{x}^{(3)} &amp; \dots &amp;  \mathbf{w_{1}}^{[1]} . \mathbf{x}^{(m)}
\end{bmatrix}
\end{split}\]</div>
</section>
<section id="broadcasting">
<h3>Broadcasting<a class="headerlink" href="#broadcasting" title="Permalink to this heading">#</a></h3>
<p><span class="math notranslate nohighlight">\(\mathbf{b}^{[l]}\)</span>, the last term in <span class="math notranslate nohighlight">\(\mathbf{Z}^{[1]}\)</span>, is a <em>column vector</em> containing the bias for each node in the layer:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{b}^{[l]} =
\begin{bmatrix} 
b_{1}^{[1]} \\
b_{2}^{[1]} \\
b_{3}^{[1]} \\
\end{bmatrix}
\end{split}\]</div>
<p>But now there’s a problem, we need to add <span class="math notranslate nohighlight">\(\mathbf{W}^{[1]} \mathbf{X}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{b}^{[l]}\)</span> but they have different dimensions - <span class="math notranslate nohighlight">\((3, m)\)</span> and <span class="math notranslate nohighlight">\((3, 1)\)</span> respectively.</p>
<p>We want to add the same bias values for each data sample. To do this we use <em>broadcasting</em> (which might be familiar to you from <code class="docutils literal notranslate"><span class="pre">numpy</span></code>). In other words, we create a <span class="math notranslate nohighlight">\((3, m)\)</span> matrix by copying <span class="math notranslate nohighlight">\(\mathbf{b}^{[l]}\)</span> <span class="math notranslate nohighlight">\(m\)</span> times:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{bmatrix} 
b_{1}^{[1]} &amp; b_{1}^{[1]} &amp; b_{1}^{[1]} &amp; \dots &amp; b_{1}^{[1]} \\
b_{2}^{[1]} &amp; b_{2}^{[1]} &amp; b_{2}^{[1]} &amp; \dots &amp; b_{2}^{[1]} \\
b_{3}^{[1]} &amp; b_{3}^{[1]} &amp; b_{3}^{[1]} &amp; \dots &amp; b_{3}^{[1]}
\end{bmatrix}
\end{split}\]</div>
<p>where each column is <span class="math notranslate nohighlight">\(\mathbf{b}^{[l]}\)</span>, and there are <span class="math notranslate nohighlight">\(m\)</span> columns. This matrix has the same dimensions as <span class="math notranslate nohighlight">\(\mathbf{W}^{[1]} \mathbf{X}\)</span>, so the two can now be added together.</p>
</section>
<section id="summary">
<h3>Summary<a class="headerlink" href="#summary" title="Permalink to this heading">#</a></h3>
<p>We’ve now seen all the structures and operations that are needed to represent a forward pass through the network for a whole batch of data.</p>
<p>The general form for computing the linear predictor in any layer is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{Z}^{[l]} = \mathbf{W}^{[l]} \mathbf{A}^{[l-1]} + \mathbf{b}^{[l]} \\
(n^{[l]}, m) = (n^{[l]}, n^{[l-1]}) \times (n^{[l-1]}, m) + (n^{[l]}, 1) \\
\end{split}\]</div>
<p>where the second line shows the dimensions of each term.</p>
<p>To compute the node activations <span class="math notranslate nohighlight">\(\mathbf{A^{[l]}}\)</span>, we apply the activation function <span class="math notranslate nohighlight">\(g\)</span> (element-wise):</p>
<div class="math notranslate nohighlight">
\[
\mathbf{A}^{[l]} = g^{[l]}(\mathbf{Z}^{[l]})
\]</div>
<p>Where <span class="math notranslate nohighlight">\(g^{[l]}\)</span> is the activation function for layer <span class="math notranslate nohighlight">\(l\)</span> (different layers may use different activation functions).</p>
</section>
</section>
<section id="backward-pass">
<h2>Backward Pass<a class="headerlink" href="#backward-pass" title="Permalink to this heading">#</a></h2>
<p>We’ll start with the final matrix equations for back propagation, then work our way back to understanding <em>why</em> they’re  correct.</p>
<section id="key-equations">
<h3>Key Equations<a class="headerlink" href="#key-equations" title="Permalink to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[\begin{split}
\frac{\partial \mathcal{L}}{\partial \mathbf{W}^{[l]}} = \frac{1}{m} \frac{\partial \mathcal{L}}{\partial \mathbf{Z}^{[l]}} \mathbf{A}^{[l-1]^T} \\
\frac{\partial \mathcal{L}}{\partial \mathbf{b}^{[l]}} = \frac{1}{m} \sum_{\mathrm{rows}} \frac{\partial \mathcal{L}}{\partial \mathbf{Z}^{[l]}} \\
\frac{\partial \mathcal{L}}{\partial \mathbf{Z}^{[l]}} = \frac{\partial \mathcal{L}}{\partial \mathbf{A}^{[l]}} * \frac{\partial \mathbf{A}^{[l]}}{\partial \mathbf{Z}^{[l]}}
\end{split}\]</div>
</section>
<section id="terms-needed-for-partial-mathcal-l-partial-mathbf-z-l">
<h3>Terms needed for <span class="math notranslate nohighlight">\(\partial \mathcal{L} / \partial \mathbf{Z}^{[l]}\)</span><a class="headerlink" href="#terms-needed-for-partial-mathcal-l-partial-mathbf-z-l" title="Permalink to this heading">#</a></h3>
<p>For all layers except the last (output) layer:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial \mathbf{A}^{[l]}} = \mathbf{W}^{[l+1]^T} \frac{\partial \mathcal{L}}{\partial \mathbf{Z}^{[l+1]}}
\]</div>
<p>So:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial \mathbf{Z}^{[l]}} = \mathbf{W}^{[l+1]^T} \frac{\partial \mathcal{L}}{\partial \mathbf{Z}^{[l+1]}} * \frac{\partial \mathbf{A}^{[l]}}{\partial \mathbf{Z}^{[l]}}
\]</div>
<p>Where <span class="math notranslate nohighlight">\(\partial \mathbf{A}^{[l]} / \partial \mathbf{Z}^{[l]}\)</span> is the derivative of the activation function. For example, for sigmoid activation:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathbf{A}^{[l]}}{\partial \mathbf{Z}^{[l]}} = \mathbf{A}^{[l]} * (1 - \mathbf{A}^{[l]})
\]</div>
<p>For the output layer (<span class="math notranslate nohighlight">\(L\)</span>), the form of <span class="math notranslate nohighlight">\(\partial \mathcal{L}/\partial \mathbf{A}^{[L]}\)</span> depends on the the loss function used. Often the overall expression for <span class="math notranslate nohighlight">\(\partial \mathcal{L} / \partial \mathbf{Z}^{[L]}\)</span> (which you get by multiplying the loss and final activation function derivatives) has a convenient form. For log loss and sigmoid activation it’s:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial \mathbf{Z}^{[L]}} = \mathbf{A}^{[L]} - \mathbf{Y}
\]</div>
<p>For mean squared error loss and linear activation it’s:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial \mathbf{Z}^{[L]}} = 2\left(\mathbf{Z}^{[L]} - \mathbf{Y}\right)
\]</div>
</section>
<section id="intuition-why-are-these-the-right-equations">
<h3>Intuition - Why are these the right equations?<a class="headerlink" href="#intuition-why-are-these-the-right-equations" title="Permalink to this heading">#</a></h3>
<p>You may have already noticed similarities between the equations in matrix form above and the back propagation equations without using matrix notation in the previous notebook. They are the same equations, of course, but it can be difficult to see how this emerges through matrix multiplication and other operations. I try to solidify the link between the two forms here.</p>
<section id="checking-dimensions">
<h4>Checking Dimensions<a class="headerlink" href="#checking-dimensions" title="Permalink to this heading">#</a></h4>
<p>The notation cheat sheet earlier in this notebook gives the dimensions of all the terms we’re using. Derivatives take on the same dimension as the term in the denominator, e.g.</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial \mathbf{W}^{[l]}}
\]</div>
<p>is a matrix with the same dimensions as <span class="math notranslate nohighlight">\(\mathbf{W}^{[l]}\)</span>, namely <span class="math notranslate nohighlight">\((n^{[l]}, n^{[l-1]})\)</span>, where each element is the derivative of the loss with respect to a single weight.</p>
<p>In the full expression for the weight derivatives in a layer, we can check the dimensions make mathematical sense:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\frac{\partial \mathcal{L}}{\partial \mathbf{W}^{[l]}} = \frac{1}{m} \frac{\partial \mathcal{L}}{\partial \mathbf{Z}^{[l]}} \mathbf{A}^{[l-1]^T} \\
(n^{[l]}, n^{[l-1]}) = (1) * (n^{[l]}, m) \times (m, n^{[l-1]})
\end{split}\]</div>
<p>These are the correct dimensions for a valid matrix multiplication, which gives some reassurance that it’s sensible at least! We can repeat this for the other key equations:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\frac{\partial \mathcal{L}}{\partial \mathbf{b}^{[l]}} = \frac{1}{m} \sum_{\mathrm{rows}} \frac{\partial \mathcal{L}}{\partial \mathbf{Z}^{[l]}} \\
(n^{[l]}, 1) = (1) * \sum_{\mathrm{rows}} (n^{[l]}, m)
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\frac{\partial \mathcal{L}}{\partial \mathbf{Z}^{[l]}} = \frac{\partial \mathcal{L}}{\partial \mathbf{A}^{[l]}} * \frac{\partial \mathbf{A}^{[l]}}{\partial \mathbf{Z}^{[l]}} \\
(n^{[l]}, m) = (n^{[l]}, m) * (n^{[l]}, m)
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\frac{\partial \mathcal{L}}{\partial \mathbf{A}^{[l]}} = \mathbf{W}^{[l+1]^T} \frac{\partial \mathcal{L}}{\partial \mathbf{Z}^{[l+1]}} \\
(n^{[l]}, m) = (n^{[l]}, n^{[l+1]}) \times (n^{[l+1]}, m)
\end{split}\]</div>
</section>
<section id="activations">
<h4>Activations<a class="headerlink" href="#activations" title="Permalink to this heading">#</a></h4>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial \mathbf{A}^{[l]}} = \mathbf{W}^{[l+1]^T} \frac{\partial \mathcal{L}}{\partial \mathbf{Z}^{[l+1]}}
\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{W}^{[l+1]^T} = 
\begin{bmatrix}
w_{1 \rightarrow 1}^{[l+1]} &amp; \dots &amp; w_{1 \rightarrow n^{[l+1]}}^{[l+1]} \\
\vdots &amp; \ddots &amp; \vdots \\
w_{n^{[l]} \rightarrow 1}^{[l+1]} &amp; \dots &amp; w_{n^{[l]} \rightarrow n^{[l+1]}}^{[l+1]} \\
\end{bmatrix}
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\frac{\partial \mathcal{L}}{\partial \mathbf{Z}^{[l+1]}} = 
\begin{bmatrix}
\partial \mathcal{L}/\partial z_1^{[l+1](1)} &amp; \dots &amp; \partial \mathcal{L}/\partial z_1^{[l+1](m)} \\
\vdots &amp; \ddots &amp; \vdots \\
\partial \mathcal{L}/\partial z_{n^{[l+1]}}^{[l+1](1)} &amp; \dots &amp; \partial \mathcal{L}/\partial z_{n^{[l+1]}}^{[l+1](m)} \\
\end{bmatrix}
\end{split}\]</div>
<p>Gradient for the activation of node <span class="math notranslate nohighlight">\(k\)</span> in layer <span class="math notranslate nohighlight">\(l\)</span> for data sample <span class="math notranslate nohighlight">\(m\)</span>, i.e. element <span class="math notranslate nohighlight">\((k, m)\)</span> of <span class="math notranslate nohighlight">\(\partial \mathcal{L} / \partial \mathbf{A}^{[l]}\)</span>):</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial a_k^{[l](m)}} = w_{k \rightarrow 1}^{[l+1]} \frac{\partial \mathcal{L}}{\partial z_1^{[l+1](m)}} + \dots + w_{k \rightarrow n^{[l+1]}}^{[l+1]} \frac{\partial \mathcal{L}}{\partial z_{n^{[l+1]}}^{[l+1](m)}}
\]</div>
<p>This reads as:</p>
<ul class="simple">
<li><p>The loss gradient of the activation of node <span class="math notranslate nohighlight">\(k\)</span> in layer <span class="math notranslate nohighlight">\(l\)</span> for data sample <span class="math notranslate nohighlight">\(m\)</span> is the sum of…</p>
<ul>
<li><p>the loss gradients of the linear predictors for each node in the next layer…</p>
<ul>
<li><p>multiplied by the weights between node <span class="math notranslate nohighlight">\(k\)</span> in layer <span class="math notranslate nohighlight">\(l\)</span> and all the nodes in the next layer</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section id="linear-predictor">
<h4>Linear Predictor<a class="headerlink" href="#linear-predictor" title="Permalink to this heading">#</a></h4>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial \mathbf{Z}^{[l]}} = \frac{\partial \mathcal{L}}{\partial \mathbf{A}^{[l]}} * \frac{\partial \mathbf{A}^{[l]}}{\partial \mathbf{Z}^{[l]}}
\]</div>
<p>This follows from the chain rule (as seen in the previous notebook), and we’ve already derived/explained the two terms on the right above.</p>
</section>
<section id="bias">
<h4>Bias<a class="headerlink" href="#bias" title="Permalink to this heading">#</a></h4>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial \mathbf{b}^{[l]}} = \frac{1}{m} \sum_{\mathrm{rows}} \frac{\partial \mathcal{L}}{\partial \mathbf{Z}^{[l]}}
\]</div>
<p>In the previous notebook we derived that, for a single bias term for a single data point, <span class="math notranslate nohighlight">\(\partial \mathcal{L} / \partial b_k^{[l](m)} = \partial \mathcal{L} / \partial z_k^{[l](m)}\)</span>. The expression here computes the mean of the gradient for each bias term across the whole batch of data.</p>
</section>
<section id="weights">
<h4>Weights<a class="headerlink" href="#weights" title="Permalink to this heading">#</a></h4>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial \mathbf{W}^{[l]}} = \frac{1}{m} \frac{\partial \mathcal{L}}{\partial \mathbf{Z}^{[l]}} \mathbf{A}^{[l-1]^T}
\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\frac{\partial \mathcal{L}}{\partial \mathbf{Z}^{[l]}} = 
\begin{bmatrix}
\partial \mathcal{L}/\partial z_1^{[l](1)} &amp; \dots &amp; \partial \mathcal{L}/\partial z_1^{[l](m)} \\
\vdots &amp; \ddots &amp; \vdots \\
\partial \mathcal{L}/\partial z_{n^{[l]}}^{[l](1)} &amp; \dots &amp; \partial \mathcal{L}/\partial z_{n^{[l]}}^{[l](m)}
\end{bmatrix}
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{A}^{[l-1]^T} =
\begin{bmatrix} 
a_{1}^{[l-1](1)} &amp; \dots &amp; a_{n^{[l-1]}}^{[l-1](1)} \\
\vdots &amp; \ddots &amp; \vdots \\
a_{1}^{[l-1](m)} &amp; \dots &amp; a_{n^{[l-1]}}^{[l-1](m)}
\end{bmatrix}
\end{split}\]</div>
<p>The element <span class="math notranslate nohighlight">\((q, p)\)</span> of <span class="math notranslate nohighlight">\(\partial \mathcal{L} / \partial \mathbf{W}^{[l]}\)</span> (row <span class="math notranslate nohighlight">\(q\)</span>, column <span class="math notranslate nohighlight">\(p\)</span>) contains the gradient of the weight between node <span class="math notranslate nohighlight">\(p\)</span> in layer <span class="math notranslate nohighlight">\([l-1]\)</span> and node <span class="math notranslate nohighlight">\(q\)</span> in layer <span class="math notranslate nohighlight">\([l]\)</span> (i.e. the loss gradient of <span class="math notranslate nohighlight">\(w_{p \rightarrow q}^{[l]}\)</span>), averaged across the whole batch of data:</p>
<div class="math notranslate nohighlight">
\[
\left[ \frac{\partial \mathcal{L}}{\partial \mathbf{W}^{[l]}} \right]_{(q, p)}  = 
\frac{\partial \mathcal{L}}{\partial w_{p \rightarrow q}^{[l]}} = \frac{1}{m} \left( \frac{\partial \mathcal{L}}{\partial z_q^{[l](1)}} a_{p}^{[l-1](1)} + \dots + \frac{\partial \mathcal{L}}{\partial z_q^{[l](m)}} a_{p}^{[l-1](m)} \right)
\]</div>
<p>For why terms like <span class="math notranslate nohighlight">\((\partial \mathcal{L} / \partial z_q^{[l](1)} ) a_{p}^{[l-1](1)}\)</span> are correct for computing the gradient for the weights, you can again refer back to the single data point examples in the previous notebook.</p>
</section>
</section>
</section>
<section id="why-matrices">
<h2>Why matrices?<a class="headerlink" href="#why-matrices" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>The equations are much shorter/neater</p></li>
<li><p>There are efficient algorithms/libraries readily available for vector/matrix operations, everything from <code class="docutils literal notranslate"><span class="pre">numpy</span></code> in Python to recent developments like <a class="reference external" href="https://www.deepmind.com/blog/discovering-novel-algorithms-with-alphatensor">DeepMind’s AlphaTensor</a>.</p></li>
<li><p>Matrix multiplication (for example) can be run in parallel, including on GPUs</p></li>
</ul>
<section id="why-gpus">
<h3>Why GPUs?<a class="headerlink" href="#why-gpus" title="Permalink to this heading">#</a></h3>
<p>Here’s a rough comparison between CPUs and GPUs in terms of their performance and suitability for certain tasks:</p>
<ul class="simple">
<li><p>CPUs (Central Processing Units)</p>
<ul>
<li><p>Few cores/threads (low throughput)</p></li>
<li><p>Low latency (each core/thread is fast)</p></li>
<li><p>Good at context switching/performing different tasks sequentially</p></li>
</ul>
</li>
<li><p>GPUs (Graphics Processing Units)</p>
<ul>
<li><p>Hundreds/thousands of cores/threads (high throughput)</p></li>
<li><p>High latency (each core/thread is slower than a CPU)</p></li>
<li><p>Good at performing single, simple (especially floating point) tasks in parallel</p></li>
<li><p>Some added cost (time) in transferring data to GPU memory and in setting up/initiating tasks</p></li>
</ul>
</li>
</ul>
<p>When training a neural network we need to perform some general linear algebra operations many times (e.g. for each data sample, each layer/node in the network etc.) And many of those operations can be performed in parallel, for example each element in the result of a matrix multiplication can be computed in parallel as it doesn’t depend on any of the other elements (only the input matrices). Deep learning workflows are well suited to GPUs as a result.</p>
<p>Much of the potential of GPUs for deep learning has been unlocked by Nvidia’s development of CUDA, which is a toolkit and set of drivers for writing parallel code to run on GPUs.</p>
</section>
<section id="what-about-tpus">
<h3>What about TPUs?<a class="headerlink" href="#what-about-tpus" title="Permalink to this heading">#</a></h3>
<p><a class="reference external" href="https://cloud.google.com/tpu/docs/tpus">Tensor Processing Units, or “TPUs”</a> (originally developed by Google), and now GPUs with <a class="reference external" href="https://www.nvidia.com/en-gb/data-center/tensor-cores/">Tensor Cores</a> (originally developed by Nvidia) are more specialised hardware optimised for machine/deep learning workflows. Or, more precisely, they’re optimised for linear algebra, matrix operations, and their use in neural networks. They can be an order of magnitude faster than GPUs, especially for very large models, but are less common and more expensive than traditional GPUs.</p>
</section>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://www.coursera.org/learn/neural-networks-deep-learning/home/info"><em>“Neural Networks and Deep Learning”</em>, DeepLearning.AI, Andrew Ng (Coursera)</a></p></li>
<li><p><a class="reference external" href="https://blogs.nvidia.com/blog/2012/09/10/what-is-cuda-2/"><em>“What is CUDA”</em>, NVIDIA</a></p></li>
<li><p><a class="reference external" href="https://blogs.nvidia.com/blog/2009/12/16/whats-the-difference-between-a-cpu-and-a-gpu/"><em>“What’s the Difference Between a CPU and a GPU?
“</em>, NVIDIA</a></p></li>
<li><p><a class="reference external" href="https://cloud.google.com/tpu/docs/tpus"><em>“Cloud Tensor Processing Units (TPUs)”</em>, Google</a></p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./learningdl"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="03_backprop.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">03: Back Propagation</p>
      </div>
    </a>
    <a class="right-next"
       href="05_activation.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">05: Gradients and Activation Functions</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-network-and-definitions">Example Network and Definitions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#notation-cheat-sheet">Notation Cheat Sheet</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#forward-pass">Forward Pass</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#value-for-a-single-node-and-data-sample-dot-products">Value for a single node and data sample: Dot products</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#values-for-all-nodes-in-a-layer-for-all-data-in-a-batch-matrix-multiplication">Values for all nodes in a layer for all data in a batch: Matrix multiplication</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#broadcasting">Broadcasting</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#backward-pass">Backward Pass</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-equations">Key Equations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#terms-needed-for-partial-mathcal-l-partial-mathbf-z-l">Terms needed for <span class="math notranslate nohighlight">\(\partial \mathcal{L} / \partial \mathbf{Z}^{[l]}\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#intuition-why-are-these-the-right-equations">Intuition - Why are these the right equations?</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#checking-dimensions">Checking Dimensions</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#activations">Activations</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-predictor">Linear Predictor</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#bias">Bias</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#weights">Weights</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-matrices">Why matrices?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-gpus">Why GPUs?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-about-tpus">What about TPUs?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Jack Roberts
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>