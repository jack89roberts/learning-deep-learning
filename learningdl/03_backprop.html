

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>03: Back Propagation &#8212; Learning Deep Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'learningdl/03_backprop';</script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="04: Matrix Notation" href="04_matrix_notation.html" />
    <link rel="prev" title="02: Logistic Regression" href="02_logistic_regression.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Learning Deep Learning [WIP!]
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Fully Connected Neural Networks</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01_linear_regression.html">01: Linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_logistic_regression.html">02: Logistic Regression</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">03: Back Propagation</a></li>
<li class="toctree-l1"><a class="reference internal" href="04_matrix_notation.html">04: Matrix Notation</a></li>
<li class="toctree-l1"><a class="reference internal" href="05_activation.html">05: Gradients and Activation Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="06_nonlinear_regression.html">06: Non-linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="07_multiclass.html">07: Multiple Outputs</a></li>
<li class="toctree-l1"><a class="reference internal" href="08_regularisation.html">08 Regularisation</a></li>
<li class="toctree-l1"><a class="reference internal" href="09_optimisers.html">09: Optimisation Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="10_experiments.html">10: Training and Tuning Networks</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/jack89roberts/learning-deep-learning" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/jack89roberts/learning-deep-learning/edit/main/learningdl/03_backprop.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/jack89roberts/learning-deep-learning/issues/new?title=Issue%20on%20page%20%2Flearningdl/03_backprop.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/learningdl/03_backprop.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>03: Back Propagation</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#forward-pass-recap">Forward Pass: Recap</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-the-parameters-how-not-to-do-it">Learning the Parameters (how not to do it)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-network-as-a-composed-function">Neural Network as a Composed Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#chain-rule-case-1-f-is-a-function-of-g-and-g-is-a-function-of-x">Chain Rule Case 1: <span class="math notranslate nohighlight">\(f\)</span> is a function of <span class="math notranslate nohighlight">\(g\)</span>, and <span class="math notranslate nohighlight">\(g\)</span> is a function of <span class="math notranslate nohighlight">\(x\)</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example">Example</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Back Propagation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-1-computing-the-gradient-for-b-1-2">Example 1: Computing the gradient for <span class="math notranslate nohighlight">\(b_1^{[2]}\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-2-computing-the-gradient-for-w-2-1">Example 2: Computing the gradient for <span class="math notranslate nohighlight">\(w_2^{[1]}\)</span></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multiple-paths">Multiple Paths</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#chain-rule-case-2-f-is-a-function-of-g-and-h-which-are-both-functions-of-x">Chain Rule Case 2: <span class="math notranslate nohighlight">\(f\)</span> is a function of <span class="math notranslate nohighlight">\(g\)</span> and <span class="math notranslate nohighlight">\(h\)</span>, which are both functions of <span class="math notranslate nohighlight">\(x\)</span></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Example</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#back-to-the-neural-network">Back to the neural network</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#back-propagation-and-efficiency">Back Propagation and Efficiency</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#computation-graphs-and-auto-differentiation">Computation Graphs and Auto-Differentiation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#manual-implementation">Manual Implementation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#forward-pass">Forward Pass</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#backward-pass">Backward Pass</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#graph-implementation">Graph Implementation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Forward Pass</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Backward Pass</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch-implementation">Pytorch Implementation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Forward Pass</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Backward Pass</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="back-propagation">
<h1>03: Back Propagation<a class="headerlink" href="#back-propagation" title="Permalink to this heading">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">copy</span> <span class="kn">import</span> <span class="n">deepcopy</span>
<span class="kn">from</span> <span class="nn">math</span> <span class="kn">import</span> <span class="n">prod</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
</pre></div>
</div>
</div>
</div>
<p><img alt="" src="../_images/03_forward_backward.png" /></p>
<p>The training loop for a neural network involves:</p>
<ol class="arabic simple">
<li><p>A <strong>forward pass</strong>: Feed the input features (<span class="math notranslate nohighlight">\(x_1\)</span>, <span class="math notranslate nohighlight">\(x_2\)</span>) through all the layers of the network to compute our predictions, <span class="math notranslate nohighlight">\(\hat{y}\)</span>.</p></li>
<li><p>Compute the <strong>loss</strong> (or cost), <span class="math notranslate nohighlight">\(\mathcal{L}(y, \hat{y})\)</span>, a function of the predicted values <span class="math notranslate nohighlight">\(\hat{y}\)</span> and the actual values <span class="math notranslate nohighlight">\(y\)</span>.</p></li>
<li><p>A <strong>backward pass</strong> (or <em>back propagation</em>): Feed the loss <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> back through the network to compute the rate of change of the loss (i.e. the derivative) with respect to the network parameters (the weights and biases for each node, <span class="math notranslate nohighlight">\(w\)</span>, <span class="math notranslate nohighlight">\(b\)</span>)</p></li>
<li><p>Given their derivatives, update the network parameters (<span class="math notranslate nohighlight">\(w\)</span>, <span class="math notranslate nohighlight">\(b\)</span>) using an algorithm like gradient descent.</p></li>
</ol>
<section id="forward-pass-recap">
<h2>Forward Pass: Recap<a class="headerlink" href="#forward-pass-recap" title="Permalink to this heading">#</a></h2>
<p>Each node computes a linear combination of the output of all the nodes in the previous layer, for example:</p>
<div class="math notranslate nohighlight">
\[
z_1^{[1]} = w_{1 \rightarrow 1}^{[1]} x_1 + w_{2 \rightarrow 1}^{[1]} x_2 + b_1^{[1]}
\]</div>
<p>This is passed to an activation function, <span class="math notranslate nohighlight">\(g\)</span>, (assumed to be the same function in all layers here), to create the final output, or “activation”, of each node:</p>
<div class="math notranslate nohighlight">
\[
a_3^{[2]} = g(z_3^{[2]})
\]</div>
<p>For example, <span class="math notranslate nohighlight">\(\hat{y}\)</span>, can be expressed in terms of the activation of the final layer as follows:</p>
<div class="math notranslate nohighlight">
\[
\hat{y} = a_1^{[3]} = g\left(w_{1 \rightarrow 1}^{[3]} a_{1}^{[2]} + w_{2 \rightarrow 1}^{[3]} a_{2}^{[2]} + w_{3 \rightarrow 1}^{[3]} a_{3}^{[2]} + b_1^{[3]}\right)
\]</div>
<p>The terms not introduced above mean:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(w_{j \rightarrow k}^{[l]}\)</span>: The weight between node <span class="math notranslate nohighlight">\(j\)</span> in layer <span class="math notranslate nohighlight">\(l-1\)</span> and node <span class="math notranslate nohighlight">\(k\)</span> in layer <span class="math notranslate nohighlight">\(l\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(a_k^{[l]}\)</span>: The activation of node <span class="math notranslate nohighlight">\(k\)</span> in layer <span class="math notranslate nohighlight">\(l\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(b_k^{[l]}\)</span>: The bias term for node <span class="math notranslate nohighlight">\(k\)</span> in layer <span class="math notranslate nohighlight">\(l\)</span></p></li>
</ul>
</section>
<section id="learning-the-parameters-how-not-to-do-it">
<h2>Learning the Parameters (how not to do it)<a class="headerlink" href="#learning-the-parameters-how-not-to-do-it" title="Permalink to this heading">#</a></h2>
<p>Let’s consider a simpler network, with one input, two hidden nodes, and one output:</p>
<p><img alt="" src="../_images/03_backprop_example_params.png" /></p>
<p>Here I’ve also included a node after the network’s output to represent the calculation of the loss, <span class="math notranslate nohighlight">\(\mathcal{L}(y, \hat{y})\)</span>, where <span class="math notranslate nohighlight">\(\hat{y} = g(z_1^{[2]})\)</span> is the predicted value from the network and <span class="math notranslate nohighlight">\(y\)</span> the true value.</p>
<p>This network has seven parameters: <span class="math notranslate nohighlight">\(w_1^{[1]}\)</span>, <span class="math notranslate nohighlight">\(w_2^{[1]}\)</span>, <span class="math notranslate nohighlight">\(b_1^{[1]}\)</span>, <span class="math notranslate nohighlight">\(b_2^{[1]}\)</span>, <span class="math notranslate nohighlight">\(w_1^{[2]}\)</span>, <span class="math notranslate nohighlight">\(w_2^{[2]}\)</span>, <span class="math notranslate nohighlight">\(b_1^{[2]}\)</span></p>
<p>How can we learn suitable values for all these parameters?</p>
<p>Let’s consider a partially trained network with the example architecture above, using sigmoid activation and log loss, to learn to classify points which belong to one class if <span class="math notranslate nohighlight">\(-0.5 &lt; x &lt; 0.5\)</span> and to another class otherwise:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">X</span><span class="p">))</span>


<span class="k">class</span> <span class="nc">Network</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">biases</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">weights</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">biases</span> <span class="o">=</span> <span class="n">biases</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">y</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">A</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">)</span>  <span class="c1"># stores activations from previous layer</span>
        <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">)):</span>  <span class="c1"># iterate over layers</span>
            <span class="n">Z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="n">l</span><span class="p">]</span> <span class="o">@</span> <span class="n">A</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">[</span><span class="n">l</span><span class="p">]</span>
            <span class="n">A</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">A</span>

    <span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># log loss</span>
        <span class="n">yhat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">()</span>
        <span class="k">return</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">))</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">yhat</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">yhat</span><span class="p">))</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">y</span><span class="o">==</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">y</span><span class="o">==</span><span class="mi">0</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Class 0&quot;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">y</span><span class="o">==</span><span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">y</span><span class="o">==</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;r&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Class 1&quot;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">network</span><span class="o">.</span><span class="n">forward</span><span class="p">()</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Neural Network&quot;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>


<span class="n">n</span> <span class="o">=</span> <span class="mi">51</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">51</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">((</span><span class="n">X</span> <span class="o">&lt;</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="n">X</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">))</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>

<span class="n">weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">7.8751</span><span class="p">,</span> <span class="o">-</span><span class="mf">8.3499</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">5.0297</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.0937</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)]</span>
<span class="n">biases</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">3.9181</span><span class="p">,</span> <span class="mf">3.8380</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.6347</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)]</span>
<span class="n">network</span> <span class="o">=</span> <span class="n">Network</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">biases</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">network</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Original loss: </span><span class="si">{</span><span class="n">network</span><span class="o">.</span><span class="n">loss</span><span class="p">()</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Original loss: 12.017
</pre></div>
</div>
<img alt="../_images/24a6386ede913f598a21370ecb83ba6ceecea35bbeaaba2ed41825b4cc29b3dc.png" src="../_images/24a6386ede913f598a21370ecb83ba6ceecea35bbeaaba2ed41825b4cc29b3dc.png" />
</div>
</div>
<p>We learn good values for the parameters iteratively. At each iteration, we ask the question - in what direction should each parameter be adjusted to reduce the loss?</p>
<p>In gradient descent we use the partial derivative of the loss function with respect to the parameters to update the network, making small changes to the parameters like:</p>
<div class="math notranslate nohighlight">
\[
w_1^{[1]}  = w_1^{[1]} - \alpha\frac{\partial \mathcal{L}}{\partial w_1^{[1]}}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\alpha\)</span> is the learning rate.</p>
<p>One way to estimate this is to compute the loss for a range of <span class="math notranslate nohighlight">\(w_1^{[1]}\)</span> values around its initial value, leaving the values of all the other parameters fixed:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">perturb</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">layer_idx</span><span class="p">,</span> <span class="n">node_idx</span><span class="p">,</span> <span class="n">param_type</span><span class="p">,</span> <span class="n">deltas</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">0.005</span><span class="p">,</span> <span class="mf">0.005</span><span class="p">,</span> <span class="mi">11</span><span class="p">)):</span>
    <span class="n">losses</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">deltas</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">delta_idx</span><span class="p">,</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">deltas</span><span class="p">):</span>
        <span class="n">n</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">network</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">param_type</span> <span class="o">==</span> <span class="s2">&quot;weight&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">layer_idx</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">n</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="n">layer_idx</span><span class="p">][</span><span class="n">node_idx</span><span class="p">]</span> <span class="o">+=</span> <span class="n">d</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">n</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="n">layer_idx</span><span class="p">][</span><span class="mi">0</span><span class="p">,</span> <span class="n">node_idx</span><span class="p">]</span> <span class="o">+=</span> <span class="n">d</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">n</span><span class="o">.</span><span class="n">biases</span><span class="p">[</span><span class="n">layer_idx</span><span class="p">][</span><span class="n">node_idx</span><span class="p">]</span> <span class="o">+=</span> <span class="n">d</span>
        <span class="n">losses</span><span class="p">[</span><span class="n">delta_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">n</span><span class="o">.</span><span class="n">loss</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">deltas</span><span class="p">,</span> <span class="n">losses</span>
    

<span class="k">def</span> <span class="nf">plot_perturb</span><span class="p">(</span><span class="n">layer_idx</span><span class="p">,</span> <span class="n">node_idx</span><span class="p">,</span> <span class="n">param_type</span><span class="p">,</span> <span class="n">deltas</span><span class="p">,</span> <span class="n">losses</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.002</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">ax</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
    <span class="n">orig_loss</span> <span class="o">=</span> <span class="n">network</span><span class="o">.</span><span class="n">loss</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">deltas</span><span class="p">,</span> <span class="n">losses</span> <span class="o">-</span> <span class="n">orig_loss</span><span class="p">,</span> <span class="s2">&quot;o&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">param_type</span> <span class="o">==</span> <span class="s2">&quot;weight&quot;</span><span class="p">:</span>
        <span class="n">symbol</span> <span class="o">=</span> <span class="s2">&quot;w&quot;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">symbol</span> <span class="o">=</span> <span class="s2">&quot;b&quot;</span>
    <span class="n">title_str</span> <span class="o">=</span> <span class="sa">rf</span><span class="s2">&quot;$</span><span class="si">{</span><span class="n">symbol</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">node_idx</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="sa">r</span><span class="s2">&quot;^{[&quot;</span> <span class="o">+</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">layer_idx</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="sa">r</span><span class="s2">&quot;]}$&quot;</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title_str</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Change in parameter&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Change in loss&quot;</span><span class="p">)</span>
    
    <span class="n">coef</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">polyfit</span><span class="p">(</span><span class="n">deltas</span><span class="p">,</span> <span class="n">losses</span> <span class="o">-</span> <span class="n">network</span><span class="o">.</span><span class="n">loss</span><span class="p">(),</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">dx</span> <span class="o">=</span> <span class="o">-</span><span class="n">coef</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">lr</span>
    <span class="n">dy</span> <span class="o">=</span> <span class="n">dx</span> <span class="o">*</span> <span class="n">coef</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">ann</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">dx</span><span class="p">,</span> <span class="n">dy</span><span class="p">),</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">linewidth</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;r&quot;</span><span class="p">),</span> <span class="n">annotation_clip</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">ann</span><span class="o">.</span><span class="n">arrow_patch</span><span class="o">.</span><span class="n">set_clip_box</span><span class="p">(</span><span class="n">ax</span><span class="o">.</span><span class="n">bbox</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dx</span>


<span class="n">layer_idx</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">node_idx</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">param_type</span> <span class="o">=</span> <span class="s2">&quot;weight&quot;</span>
<span class="n">deltas</span><span class="p">,</span> <span class="n">losses</span> <span class="o">=</span> <span class="n">perturb</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">layer_idx</span><span class="p">,</span> <span class="n">node_idx</span><span class="p">,</span> <span class="n">param_type</span><span class="p">)</span>
<span class="n">plot_perturb</span><span class="p">(</span><span class="n">layer_idx</span><span class="p">,</span> <span class="n">node_idx</span><span class="p">,</span> <span class="n">param_type</span><span class="p">,</span> <span class="n">deltas</span><span class="p">,</span> <span class="n">losses</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/315995a4c0d2b02711bd39cf5cbf8295b981650276bc70ac2b678b3f34e17a83.png" src="../_images/315995a4c0d2b02711bd39cf5cbf8295b981650276bc70ac2b678b3f34e17a83.png" />
</div>
</div>
<p>Each blue point represents the change in the loss (on the whole dataset) for a change in <span class="math notranslate nohighlight">\(w_1^{[1]}\)</span>. The red arrow shows the recommended change to make to the parameter value in this iteration, in this case decreasing its value (like rolling a ball down a hill).</p>
<p>To perform gradient descent for the whole network we need the derivatives for each parameter, i.e. we need to compute:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial w_1^{[1]}},
\frac{\partial \mathcal{L}}{\partial w_2^{[1]}},
\frac{\partial \mathcal{L}}{\partial b_1^{[1]}},
\frac{\partial \mathcal{L}}{\partial b_2^{[1]}},
\frac{\partial \mathcal{L}}{\partial w_1^{[2]}},
\frac{\partial \mathcal{L}}{\partial w_2^{[2]}},
\frac{\partial \mathcal{L}}{\partial b_1^{[2]}}
\]</div>
<p>We can repeat the same procedure, computing the loss for several values of the parameter of interest whilst leaving all the others fixed:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">layer_idx</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">node_idx</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">param_type</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;weight&quot;</span><span class="p">,</span> <span class="s2">&quot;weight&quot;</span><span class="p">,</span> <span class="s2">&quot;bias&quot;</span><span class="p">,</span> <span class="s2">&quot;bias&quot;</span><span class="p">,</span> <span class="s2">&quot;weight&quot;</span><span class="p">,</span> <span class="s2">&quot;weight&quot;</span><span class="p">,</span> <span class="s2">&quot;bias&quot;</span><span class="p">]</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

<span class="n">shifts</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">layer</span><span class="p">,</span> <span class="n">node</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">layer_idx</span><span class="p">)),</span> <span class="n">layer_idx</span><span class="p">,</span> <span class="n">node_idx</span><span class="p">,</span> <span class="n">param_type</span><span class="p">):</span>
    <span class="n">deltas</span><span class="p">,</span> <span class="n">losses</span> <span class="o">=</span> <span class="n">perturb</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">layer</span><span class="p">,</span> <span class="n">node</span><span class="p">,</span> <span class="n">param</span><span class="p">)</span>
    <span class="n">param_shift</span> <span class="o">=</span> <span class="n">plot_perturb</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">node</span><span class="p">,</span> <span class="n">param</span><span class="p">,</span> <span class="n">deltas</span><span class="p">,</span> <span class="n">losses</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
    <span class="n">shifts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">param_shift</span><span class="p">)</span>
    
<span class="n">ax</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;off&quot;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/ccfa76869b7fa898e696ae3216e97b87fe9ff11d947837d6fa2c57760718ded2.png" src="../_images/ccfa76869b7fa898e696ae3216e97b87fe9ff11d947837d6fa2c57760718ded2.png" />
</div>
</div>
<p>After one iteration, making the changes to the parameters indicated by the red arrows above, the loss is reduced slightly:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">new_network</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">network</span><span class="p">)</span>
<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">layer</span><span class="p">,</span> <span class="n">node</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">layer_idx</span><span class="p">)),</span> <span class="n">layer_idx</span><span class="p">,</span> <span class="n">node_idx</span><span class="p">,</span> <span class="n">param_type</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">param</span> <span class="o">==</span> <span class="s2">&quot;weight&quot;</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">layer</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">new_network</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="n">layer</span><span class="p">][</span><span class="n">node</span><span class="p">]</span> <span class="o">+=</span> <span class="n">shifts</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">new_network</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="n">layer</span><span class="p">][</span><span class="mi">0</span><span class="p">,</span> <span class="n">node</span><span class="p">]</span> <span class="o">+=</span> <span class="n">shifts</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">new_network</span><span class="o">.</span><span class="n">biases</span><span class="p">[</span><span class="n">layer</span><span class="p">][</span><span class="n">node</span><span class="p">]</span> <span class="o">+=</span> <span class="n">shifts</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>

        
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Original loss: </span><span class="si">{</span><span class="n">network</span><span class="o">.</span><span class="n">loss</span><span class="p">()</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="se">\n</span><span class="s2">New Loss: </span><span class="si">{</span><span class="n">new_network</span><span class="o">.</span><span class="n">loss</span><span class="p">()</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Original loss: 12.017
New Loss: 12.010
</pre></div>
</div>
</div>
</div>
<p>Or after many iterations (thousands):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">27.6292</span><span class="p">,</span> <span class="mf">29.6453</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">18.5647</span><span class="p">,</span> <span class="o">-</span><span class="mf">16.8853</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)]</span>
<span class="n">biases</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">13.8410</span><span class="p">,</span> <span class="mf">14.6499</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">7.7300</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)]</span>
<span class="n">network</span> <span class="o">=</span> <span class="n">Network</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">biases</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Trained network loss: </span><span class="si">{</span><span class="n">network</span><span class="o">.</span><span class="n">loss</span><span class="p">()</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">network</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Trained network loss: 0.347
</pre></div>
</div>
<img alt="../_images/9d9756a8c4f832a605fb37ac26fe1c49f03f7dd767a7fcbbeb99c4d1115edaec.png" src="../_images/9d9756a8c4f832a605fb37ac26fe1c49f03f7dd767a7fcbbeb99c4d1115edaec.png" />
</div>
</div>
<p>This works but it’s incredibly inefficient. For each parameter in the network we’re computing the loss multiple times, which could be thousands, millions or even billions of times for larger networks (per iteration of gradient descent!)</p>
</section>
<section id="neural-network-as-a-composed-function">
<h2>Neural Network as a Composed Function<a class="headerlink" href="#neural-network-as-a-composed-function" title="Permalink to this heading">#</a></h2>
<p>Neural networks are a big composed function (each layer is a function of the output of the previous layer):</p>
<p><img alt="" src="../_images/03_backprop_composedfunction.png" /></p>
<p>You might remember that calculating the derivative of composed functions is what the chain rule is for in calculus. Here’s a reminder:</p>
</section>
<section id="chain-rule-case-1-f-is-a-function-of-g-and-g-is-a-function-of-x">
<h2>Chain Rule Case 1: <span class="math notranslate nohighlight">\(f\)</span> is a function of <span class="math notranslate nohighlight">\(g\)</span>, and <span class="math notranslate nohighlight">\(g\)</span> is a function of <span class="math notranslate nohighlight">\(x\)</span><a class="headerlink" href="#chain-rule-case-1-f-is-a-function-of-g-and-g-is-a-function-of-x" title="Permalink to this heading">#</a></h2>
<div class="math notranslate nohighlight">
\[
f = f(g(x))
\]</div>
<p>The chain rule states that the derivative of <span class="math notranslate nohighlight">\(w\)</span> with respect to <span class="math notranslate nohighlight">\(x\)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[
\frac{\mathrm{d} f}{\mathrm{d}x} = \frac{\mathrm{d} f}{\mathrm{d} g} \frac{\mathrm{d} g}{\mathrm{d} x}
\]</div>
<section id="example">
<h3>Example<a class="headerlink" href="#example" title="Permalink to this heading">#</a></h3>
<p>Find the derivative of <span class="math notranslate nohighlight">\(f(x) = (e^x + x)^2\)</span>:</p>
<p>Let:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
g(x) = e^x + x \\
f(g) = g^2 \\
\end{split}\]</div>
<p>Then by chain rule:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\frac{\mathrm{d} f}{\mathrm{d} g} = 2g \\
\frac{\mathrm{d} g}{\mathrm{d} x} = e^x + 1 \\
\frac{\mathrm{d} f}{\mathrm{d}x} = \frac{\mathrm{d} f}{\mathrm{d} g} \frac{\mathrm{d} g}{\mathrm{d} x} = 2g(e^x + 1) \\
\frac{\mathrm{d} f}{\mathrm{d}x} = 2(e^x + x)(e^x + 1) 
\end{split}\]</div>
</section>
</section>
<section id="id1">
<h2>Back Propagation<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h2>
<p>Back propagation is using the chain rule to compute the rate of change of the loss with the parameter values.</p>
<p>Here’s the example network again, but with each edge (arrow) labeled by the partial derviative between the two connected nodes:</p>
<p><img alt="" src="../_images/03_backprop_example_diffs.png" /></p>
<p>To compute the derivative of the loss with respect to any term in the network we can use the chain rule. Starting with the loss on the right, we move “backwards” through the network, multiplying the partial derivatives until we get to the term we want.</p>
<section id="example-1-computing-the-gradient-for-b-1-2">
<h3>Example 1: Computing the gradient for <span class="math notranslate nohighlight">\(b_1^{[2]}\)</span><a class="headerlink" href="#example-1-computing-the-gradient-for-b-1-2" title="Permalink to this heading">#</a></h3>
<p>Why the chain rule/why multiply backwards along arrows?</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathcal{L}\)</span> is a function of <span class="math notranslate nohighlight">\(a_1^{[2]}\)</span>,  <span class="math notranslate nohighlight">\(a_1^{[2]}\)</span> is a function of <span class="math notranslate nohighlight">\(z_1^{[2]}\)</span>, and <span class="math notranslate nohighlight">\(z_1^{[2]}\)</span> is a function of <span class="math notranslate nohighlight">\(b_1^{[2]}\)</span></p></li>
<li><p>We could write that like <span class="math notranslate nohighlight">\(\mathcal{L}\left(a_1^{[2]}\left(z_1^{[2]}\left(b_1^{[2]}\right)\right)\right)\)</span>, which has the form of “case 1” of the chain rule above except with 3 functions instead of 2, i.e. a composed function like <span class="math notranslate nohighlight">\(f\left(g\left(h\left(x\right)\right)\right)\)</span></p></li>
</ul>
<p>So, applying the chain rule and multiplying derivatives backwards along the diagram:</p>
<div class="math notranslate nohighlight">
\[
\frac{\color{red}{\partial \mathcal{L}}}{\color{blue}{\partial b_1^{[2]}}} = \frac{\partial z_1^{[2]}}{\color{blue}{\partial b_1^{[2]}}} \frac{\color{green}{\partial a_1^{[2]}}}{\partial z_1^{[2]}} \frac{\color{red}{\partial \mathcal{L}}}{\color{green}{\partial a_1^{[2]}}}
\]</div>
<p>Log loss for one data point (remembering that <span class="math notranslate nohighlight">\(\hat{y} = a_1^{[2]}\)</span>):</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathcal{L} = - y \log(a_1^{[2]}) - (1 - y)\log(1 - a_1^{[2]}) \\
\frac{\color{red}{\partial \mathcal{L}}}{\color{green}{\partial a_1^{[2]}}} = -\frac{y}{a_1^{[2]}} + \frac{1-y}{1-a_1^{[2]}}
\end{split}\]</div>
<p>If using a sigmoid activation function:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
a_1^{[2]} = \frac{1}{1+\exp(-z_1^{[2]})} \\
\frac{\color{green}{\partial a_1^{[2]}}}{\partial z_1^{[2]}} = a_1^{[2]} (1 - a_1^{[2]})
\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(z_1^{[2]}\)</span> is a linear combination of its inputs:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
z_1^{[2]} = w_1^{[2]}a_1^{[1]} + w_2^{[2]}a_2^{[1]} + b_1^{[2]} \\
\frac{\partial z_1^{[2]}}{\color{blue}{\partial b_1^{[2]}}} = 1
\end{split}\]</div>
<p>Substituting in these results above and simplifying gives:</p>
<div class="math notranslate nohighlight">
\[
\frac{\color{red}{\partial \mathcal{L}}}{\color{blue}{\partial b_1^{[2]}}} =
a_1^{[2]} - y
\]</div>
<p>Note that as <span class="math notranslate nohighlight">\(\frac{\partial z_1^{[2]}}{\color{blue}{\partial b_1^{[2]}}} = 1\)</span>, it’s also the case that <span class="math notranslate nohighlight">\(\frac{\color{green}{\partial a_1^{[2]}}}{\partial z_1^{[2]}} \frac{\color{red}{\partial \mathcal{L}}}{\color{green}{\partial a_1^{[2]}}} = a_1^{[2]} - y\)</span>, which is a result we can re-use below.</p>
</section>
<section id="example-2-computing-the-gradient-for-w-2-1">
<h3>Example 2: Computing the gradient for <span class="math notranslate nohighlight">\(w_2^{[1]}\)</span><a class="headerlink" href="#example-2-computing-the-gradient-for-w-2-1" title="Permalink to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[
\frac{\color{red}{\partial \mathcal{L}}}{\color{magenta}{\partial w_2^{[1]}}} =
\frac{\color{gray}{\partial z_2^{[1]}}}{\color{magenta}{\partial w_2^{[1]}}}
\frac{\color{orange}{\partial a_2^{[1]}}}{\color{gray}{\partial z_2^{[1]}}}
\frac{\partial z_1^{[2]}}{\color{orange}{\partial a_2^{[1]}}}
\frac{\color{green}{\partial a_1^{[2]}}}{\partial z_1^{[2]}}
\frac{\color{red}{\partial \mathcal{L}}}{\color{green}{\partial a_1^{[2]}}}
\]</div>
<p>We’ve seen the form of all the derivatives above in the first example, except for the first and third terms:</p>
<p><strong>First term:</strong></p>
<div class="math notranslate nohighlight">
\[\begin{split}
z_2^{[1]} = w_2^{[1]}x + b_2^{[1]} \\
\frac{\color{gray}{\partial z_2^{[1]}}}{\color{magenta}{\partial w_2^{[1]}}} = x = a_1^{[0]}
\end{split}\]</div>
<p>For the weights after the first layer, the inputs <span class="math notranslate nohighlight">\(x\)</span> are replaced by node activations <span class="math notranslate nohighlight">\(a\)</span>. We can relabel <span class="math notranslate nohighlight">\(x = a_1^{[0]}\)</span> to make the general trend clearer.</p>
<p><strong>Second term:</strong></p>
<p>Derivative of the activation function. If using sigmoid it’s (as seen above):</p>
<div class="math notranslate nohighlight">
\[
\frac{\color{orange}{\partial a_2^{[1]}}}{\color{gray}{\partial z_2^{[1]}}} = a_2^{[1]} (1 - a_2^{[1]})
\]</div>
<p><strong>Third term:</strong></p>
<div class="math notranslate nohighlight">
\[\begin{split}
z_1^{[2]} = w_1^{[2]} a_1^{[1]} + w_2^{[2]} a_2^{[1]} + b_1^{[2]} \\
\frac{\partial z_1^{[2]}}{\color{orange}{\partial a_2^{[1]}}} = w_2^{[2]}
\end{split}\]</div>
<p><strong>Last two terms:</strong></p>
<p>We already computed this above:</p>
<div class="math notranslate nohighlight">
\[
\frac{\color{green}{\partial a_1^{[2]}}}{\partial z_1^{[2]}}
\frac{\color{red}{\partial \mathcal{L}}}{\color{green}{\partial a_1^{[2]}}} =
a_1^{[2]} - y
\]</div>
<p><strong>Overall:</strong></p>
<p>Putting it all together:</p>
<div class="math notranslate nohighlight">
\[
\frac{\color{red}{\partial \mathcal{L}}}{\color{magenta}{\partial w_2^{[1]}}} =
a_1^{[0]} a_2^{[1]} (1 - a_2^{[1]}) w_2^{[2]} \left(a_1^{[2]} - y\right)
\]</div>
<p>Note we already derived the value of the last term, <span class="math notranslate nohighlight">\(\color{red}{\partial \mathcal{L}} / \partial z_1^{[2]}\)</span>, in Example 1, so we don’t need to re-compute it here.</p>
</section>
</section>
<section id="multiple-paths">
<h2>Multiple Paths<a class="headerlink" href="#multiple-paths" title="Permalink to this heading">#</a></h2>
<p>There is one case not covered by the simplified network and examples above - where you have multiple paths from the output (loss) back to the term of interest. Such as this:</p>
<p><img alt="" src="../_images/03_backprop_multipath.png" /></p>
<p>In this case we need another form of the chain rule.</p>
<section id="chain-rule-case-2-f-is-a-function-of-g-and-h-which-are-both-functions-of-x">
<h3>Chain Rule Case 2: <span class="math notranslate nohighlight">\(f\)</span> is a function of <span class="math notranslate nohighlight">\(g\)</span> and <span class="math notranslate nohighlight">\(h\)</span>, which are both functions of <span class="math notranslate nohighlight">\(x\)</span><a class="headerlink" href="#chain-rule-case-2-f-is-a-function-of-g-and-h-which-are-both-functions-of-x" title="Permalink to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[
f = f(g(x), h(x))
\]</div>
<p>To find the derivative of <span class="math notranslate nohighlight">\(f\)</span> with respect to <span class="math notranslate nohighlight">\(x\)</span>, the chain rule states that you must sum over its (partial) derivatives for each input (<span class="math notranslate nohighlight">\(g\)</span>, <span class="math notranslate nohighlight">\(h\)</span>):</p>
<div class="math notranslate nohighlight">
\[
\frac{\mathrm{d} f}{\mathrm{d}x} = \frac{\partial f}{\partial g} \frac{\mathrm{d} g}{\mathrm{d} x} + \frac{\partial f}{\partial h} \frac{\mathrm{d} h}{\mathrm{d} x}
\]</div>
<p>This is the <em>multi-variable</em> chain rule.</p>
<section id="id2">
<h4>Example<a class="headerlink" href="#id2" title="Permalink to this heading">#</a></h4>
<p>Find the derivative of:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
f(x) = x^2(3x+1) - \sin(x^2) \\
\end{split}\]</div>
<p>which can be written as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
f(g, h) = h g - \sin(h) \\
g(x) = 3x + 1 \\
h(x) = x^2
\end{split}\]</div>
<p>Then by chain rule:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\frac{\partial f}{\partial g} = h\\
\frac{\mathrm{d} g}{\mathrm{d} x} = 3 \\
\frac{\partial f}{\partial h} = g - \cos(h) \\
\frac{\mathrm{d} h}{\mathrm{d} x} = 2x \\
\frac{\mathrm{d} f}{\mathrm{d}x} = \frac{\partial f}{\partial g} \frac{\mathrm{d} g}{\mathrm{d} x} + \frac{\partial f}{\partial h} \frac{\mathrm{d} h}{\mathrm{d} x} =
3h + 2x\left(g-\cos(h)\right) \\
\frac{\mathrm{d} f}{\mathrm{d}x} = 3x^2 + 2x\left(3x + 1 -\cos(x^2)\right)
\end{split}\]</div>
</section>
</section>
<section id="back-to-the-neural-network">
<h3>Back to the neural network<a class="headerlink" href="#back-to-the-neural-network" title="Permalink to this heading">#</a></h3>
<p>To compute <span class="math notranslate nohighlight">\(\partial \mathcal{L}/{\partial b_1^{[1]}}\)</span> we must sum over all paths between the loss and the paremeter of interest:</p>
<p><img alt="" src="../_images/03_backprop_multipath_sum.png" /></p>
<p>Why?</p>
<p>This also follows from the multi-variable chain rule (case 2 from earlier):</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathcal{L}\)</span> is a function of <span class="math notranslate nohighlight">\(a_1^{[3]}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(a_1^{[3]}\)</span> is a function of <span class="math notranslate nohighlight">\(z_1^{[3]}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(z_1^{[3]}\)</span> is a function of <span class="math notranslate nohighlight">\(a_1^{[2]}\)</span> <em>and</em> <span class="math notranslate nohighlight">\(a_2^{[2]}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(a_1^{[2]}\)</span> <em>and</em> <span class="math notranslate nohighlight">\(a_2^{[2]}\)</span> are functions of <span class="math notranslate nohighlight">\(b_1^{[1]}\)</span> (through <span class="math notranslate nohighlight">\(z_1^{[2]}\)</span> and <span class="math notranslate nohighlight">\(z_2^{[2]}\)</span>)</p></li>
</ul>
<p>This could be written:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}\left(a_1^{[3]}\left(z_1^{[3]}\left(a_1^{[2]}\left(b_1^{[1]}\right), a_2^{[2]}\left(b_1^{[1]}\right)\right)\right)\right)
\]</div>
<p>which has the form:</p>
<div class="math notranslate nohighlight">
\[
f\left(g\left(x\right), h\left(x\right)\right)
\]</div>
<p>where:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
f(g, h) = \mathcal{L}\left(a_1^{[3]}\left(z_1^{[3]}\left(g, h\right)\right)\right) \\
g(x) = a_1^{[2]}(x) \\
h(x) = a_2^{[2]}(x) \\
x = b_1^{[1]}
\end{split}\]</div>
<p>(here <span class="math notranslate nohighlight">\(x\)</span> is just an arbitrary label for a variable, <em>not</em> the input data)</p>
</section>
<section id="back-propagation-and-efficiency">
<h3>Back Propagation and Efficiency<a class="headerlink" href="#back-propagation-and-efficiency" title="Permalink to this heading">#</a></h3>
<p>It’s important to note that:</p>
<ul class="simple">
<li><p>The derivatives in layer <span class="math notranslate nohighlight">\(l\)</span> depend on the derivatives in layer <span class="math notranslate nohighlight">\(l + 1\)</span> (so we pass gradients “backwards” through the network)</p></li>
<li><p>Each term is a fairly simple combination of quantities that must be computed during the forward pass (like the activation values in hidden layers)</p></li>
<li><p>We get the derivatives for every parameter in the network by back propagating only <strong>once</strong> (only one forward pass to compute the loss and then one backwards pass to compute all the gradients)</p></li>
</ul>
<p>These properties of back propagation form the basis for efficient implementations in major frameworks (pytorch, Tensorflow, JAX etc.), mostly via:</p>
<ul class="simple">
<li><p>Matrix operations</p></li>
<li><p>Computation graphs</p></li>
<li><p>Caching intermediate values</p></li>
<li><p>Automatic differentiation</p></li>
</ul>
</section>
</section>
<section id="computation-graphs-and-auto-differentiation">
<h2>Computation Graphs and Auto-Differentiation<a class="headerlink" href="#computation-graphs-and-auto-differentiation" title="Permalink to this heading">#</a></h2>
<p>In the background, large frameworks like pytorch use computation graphs and “auto-differentiation” to be able to compute gradients efficiently.</p>
<p>Here’s an example of a simple logistic regression (one layer) network in the form of a <em>computation graph</em>:</p>
<a class="reference internal image-reference" href="../_images/03_computation_graph.png"><img alt="Computation graph" src="../_images/03_computation_graph.png" style="width: 500px;" /></a>
<ul class="simple">
<li><p>Each node represents either an input variable/parameter (white/clear background), or an operation that applies to one or more of the previously defined values. In this graph the operations are summation (<span class="math notranslate nohighlight">\(+\)</span>), multiplication (<span class="math notranslate nohighlight">\(*\)</span>), sigmoid (<span class="math notranslate nohighlight">\(g\)</span>), and log loss (<span class="math notranslate nohighlight">\(\mathcal{L}\)</span>).</p></li>
<li><p>The values of all the nodes on a row must be known before the next row can be calculated.</p></li>
</ul>
<p>In the computation graph we’ll store:</p>
<ul class="simple">
<li><p>The relationships between all the nodes (how they are connected and the operations that are performed on them)</p></li>
<li><p>The value of each node for a given input</p></li>
<li><p>The gradient of each node for a given input, with respect to the final node</p></li>
</ul>
<p>Having all the node values and the relationships between the nodes let’s us compute the gradient at each node efficiently.</p>
<section id="manual-implementation">
<h3>Manual Implementation<a class="headerlink" href="#manual-implementation" title="Permalink to this heading">#</a></h3>
<section id="forward-pass">
<h4>Forward Pass<a class="headerlink" href="#forward-pass" title="Permalink to this heading">#</a></h4>
<p>When doing a forward (top to bottom) pass through the network we store the values computed at all nodes (i.e. including the intermediate values on each row):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Forward pass</span>

<span class="c1"># =================</span>
<span class="c1"># Row 1 in diagram</span>
<span class="c1"># =================</span>
<span class="n">x1</span> <span class="o">=</span> <span class="mf">1.5</span>
<span class="n">x2</span> <span class="o">=</span> <span class="mf">2.5</span>

<span class="n">w1</span> <span class="o">=</span> <span class="o">-</span><span class="mi">4</span>
<span class="n">w2</span> <span class="o">=</span> <span class="mi">3</span>

<span class="c1"># =================</span>
<span class="c1"># Row 2 in diagram</span>
<span class="c1"># =================</span>
<span class="n">w1x1</span> <span class="o">=</span> <span class="n">w1</span> <span class="o">*</span> <span class="n">x1</span>
<span class="n">w2x2</span> <span class="o">=</span> <span class="n">w2</span> <span class="o">*</span> <span class="n">x2</span>

<span class="n">b</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>

<span class="c1"># =================</span>
<span class="c1"># Row 3 in diagram</span>
<span class="c1"># =================</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">w1x1</span> <span class="o">+</span> <span class="n">w2x2</span> <span class="o">+</span> <span class="n">b</span>

<span class="c1"># =================</span>
<span class="c1"># Row 4 in diagram</span>
<span class="c1"># =================</span>
<span class="n">yhat</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span>  <span class="c1"># sigmoid</span>

<span class="n">y</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1"># =================</span>
<span class="c1"># Row 5 in diagram</span>
<span class="c1"># =================</span>
<span class="n">L</span> <span class="o">=</span> <span class="o">-</span><span class="n">y</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">yhat</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">yhat</span><span class="p">)</span>  <span class="c1"># log loss</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;yhat = </span><span class="si">{</span><span class="n">yhat</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;L = </span><span class="si">{</span><span class="n">L</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>yhat = 0.6225
L = 0.4741
</pre></div>
</div>
</div>
</div>
</section>
<section id="backward-pass">
<h4>Backward Pass<a class="headerlink" href="#backward-pass" title="Permalink to this heading">#</a></h4>
<p>Now we can use the node values from the forward pass, our knowledge of the computation graph, and the chain rule to compute the gradients:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Backward pass</span>

<span class="c1"># =================</span>
<span class="c1"># Row 5 in diagram</span>
<span class="c1"># =================</span>
<span class="n">dL_dyhat</span> <span class="o">=</span> <span class="o">-</span><span class="n">y</span> <span class="o">/</span> <span class="n">yhat</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">yhat</span><span class="p">)</span>  <span class="c1"># derivative of log loss</span>

<span class="c1"># =================</span>
<span class="c1"># Row 4 in diagram</span>
<span class="c1"># =================</span>
<span class="n">dyhat_dz</span> <span class="o">=</span> <span class="n">yhat</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">yhat</span><span class="p">)</span>  <span class="c1"># derivative of sigmoid</span>

<span class="n">dL_dz</span> <span class="o">=</span> <span class="n">dyhat_dz</span> <span class="o">*</span> <span class="n">dL_dyhat</span>

<span class="c1"># =================</span>
<span class="c1"># Row 3 in diagram</span>
<span class="c1"># =================</span>
<span class="n">dz_dw1x1</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># summation nodes pass the same gradient backwards</span>
<span class="n">dz_dw2x2</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># e.g. z = w1x1 + w2x2 + b, dz/d(w2x2) = 1</span>
<span class="n">dz_db</span> <span class="o">=</span> <span class="mi">1</span>

<span class="n">dL_dw1x1</span> <span class="o">=</span> <span class="n">dz_dw1x1</span> <span class="o">*</span> <span class="n">dL_dz</span>
<span class="n">dL_dw2x2</span> <span class="o">=</span> <span class="n">dz_dw2x2</span> <span class="o">*</span> <span class="n">dL_dz</span>
<span class="n">dL_db</span> <span class="o">=</span> <span class="n">dz_db</span> <span class="o">*</span> <span class="n">dL_dz</span>

<span class="c1"># =================</span>
<span class="c1"># Row 2 in diagram</span>
<span class="c1"># =================</span>
<span class="n">dw1x1_dw1</span> <span class="o">=</span> <span class="n">x1</span>  <span class="c1"># multiplication node gradients take the value of the other input</span>
<span class="n">dw2x2_dw2</span> <span class="o">=</span> <span class="n">x2</span>  <span class="c1"># e.g. d(w2x2) / d(w2) = x2</span>

<span class="n">dL_dw1</span> <span class="o">=</span> <span class="n">dw1x1_dw1</span> <span class="o">*</span> <span class="n">dL_dw1x1</span>
<span class="n">dL_dw2</span> <span class="o">=</span> <span class="n">dw2x2_dw2</span> <span class="o">*</span> <span class="n">dL_dw2x2</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;dL_dw1 = </span><span class="si">{</span><span class="n">dL_dw1</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;dL_dw2 = </span><span class="si">{</span><span class="n">dL_dw2</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;dL_db = </span><span class="si">{</span><span class="n">dL_db</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>dL_dw1 = -0.5663
dL_dw2 = -0.9439
dL_db = -0.3775
</pre></div>
</div>
</div>
</div>
<p>This is an extremely verbose way of representing this, we’ll see matrix notation in the next notebook that will let us represent networks in a much more concise way.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># save some results for later</span>
<span class="n">yhat_manual</span> <span class="o">=</span> <span class="n">yhat</span>
<span class="n">L_manual</span> <span class="o">=</span> <span class="n">L</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="graph-implementation">
<h3>Graph Implementation<a class="headerlink" href="#graph-implementation" title="Permalink to this heading">#</a></h3>
<p>Let’s create a parent class <code class="docutils literal notranslate"><span class="pre">Node</span></code> that will store its value, gradients, and which other nodes it’s connected to (its parents and children):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">sort_nodes</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">visited</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="c1"># topological sort - used to ensure we compute the gradients in the</span>
    <span class="c1"># correct order (from the last node to the first node)</span>
    <span class="k">if</span> <span class="n">order</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">order</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">if</span> <span class="n">visited</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">visited</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">node</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">visited</span><span class="p">:</span>
        <span class="n">visited</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">node</span><span class="o">.</span><span class="n">parents</span><span class="p">:</span>
            <span class="n">sort_nodes</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="n">order</span><span class="p">,</span> <span class="n">visited</span><span class="o">=</span><span class="n">visited</span><span class="p">)</span>
        <span class="n">order</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">order</span>


<span class="k">class</span> <span class="nc">Node</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">value</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">gradient</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">parents</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">value</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gradient</span> <span class="o">=</span> <span class="n">gradient</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">label</span> <span class="o">=</span> <span class="n">label</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">children</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># set this node&#39;s inputs (parents)</span>
        <span class="k">if</span> <span class="n">parents</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">parents</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">parents</span> <span class="o">=</span> <span class="n">parents</span>

        <span class="c1"># now the parent nodes should include this node as a child</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parents</span><span class="p">:</span>
            <span class="n">p</span><span class="o">.</span><span class="n">children</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">rep</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">label</span><span class="si">}</span><span class="s2">: &quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">label</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span>
        <span class="n">val</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="s2">&quot;None&quot;</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">gradient</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="s2">&quot;None&quot;</span>
        <span class="k">return</span> <span class="n">rep</span> <span class="o">+</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">(value=</span><span class="si">{</span><span class="n">val</span><span class="si">}</span><span class="s2">, gradient=</span><span class="si">{</span><span class="n">grad</span><span class="si">}</span><span class="s2">)&quot;</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">ordered_nodes</span> <span class="o">=</span> <span class="n">sort_nodes</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">ordered_nodes</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;computing value of </span><span class="si">{</span><span class="n">n</span><span class="o">.</span><span class="n">label</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="n">n</span><span class="o">.</span><span class="n">_forward</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">ordered_nodes</span> <span class="o">=</span> <span class="n">sort_nodes</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">ordered_nodes</span><span class="p">:</span>
            <span class="n">n</span><span class="o">.</span><span class="n">gradient</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># reset gradients</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gradient</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># gradient with respect to self</span>
        <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="n">ordered_nodes</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">n</span><span class="o">.</span><span class="n">parents</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;computing gradient for parents of </span><span class="si">{</span><span class="n">n</span><span class="o">.</span><span class="n">label</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">label</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">n</span><span class="o">.</span><span class="n">parents</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
            <span class="n">n</span><span class="o">.</span><span class="n">_backward</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># modifies self.value using parent(s).value</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Value not set for </span><span class="si">{</span><span class="bp">self</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_backward</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># modifies parent(s).gradient using self.gradient</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parents</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Node has parents but no _backward implementation&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The class also provides methods <code class="docutils literal notranslate"><span class="pre">forward</span></code> and <code class="docutils literal notranslate"><span class="pre">backward</span></code> for computing the values and gradients of the whole graph, and unimplemented methods <code class="docutils literal notranslate"><span class="pre">_forward</span></code> and <code class="docutils literal notranslate"><span class="pre">_backward</span></code> for the specific node type:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">_forward</span></code> computes the node’s value (dependent on the values of the node’s parents)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">_backward</span></code> computes the gradient of the node’s parents (dependent on the accumulated gradient of the current node)</p></li>
</ul>
<p>The form of <code class="docutils literal notranslate"><span class="pre">_forward</span></code> and <code class="docutils literal notranslate"><span class="pre">_backward</span></code> depends on the type of node it is. Here we implement these types of nodes:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Add</span></code> nodes sum all their inputs (parent node values). In the backward pass they pass their gradient back to their parents (<code class="docutils literal notranslate"><span class="pre">d(a</span> <span class="pre">+</span> <span class="pre">b</span> <span class="pre">+</span> <span class="pre">c)/da</span> <span class="pre">=</span> <span class="pre">1</span></code>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Multiply</span></code> nodes take the product of all their inputs (parent node values). In the backward pass they pass their gradient multiplied by the values of the other parents (<code class="docutils literal notranslate"><span class="pre">d(abc)/da</span> <span class="pre">=</span> <span class="pre">bc</span></code>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Sigmoid</span></code> nodes have only one parent and compute the sigmoid activation. In the backward pass the parent receives the sigmoid node’s gradient multiplied by <code class="docutils literal notranslate"><span class="pre">1</span> <span class="pre">*</span> <span class="pre">(1</span> <span class="pre">-</span> <span class="pre">node_value)</span></code> (the derivative of the sigmoid function).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">LogLoss</span></code> nodes compute the log loss between two parents (the actual and predicted y values).</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Add</span><span class="p">(</span><span class="n">Node</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">value</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parents</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_backward</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># add propagates gradient backwards (dx/dx = 1)</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parents</span><span class="p">:</span>
            <span class="n">p</span><span class="o">.</span><span class="n">gradient</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient</span>


<span class="k">class</span> <span class="nc">Multiply</span><span class="p">(</span><span class="n">Node</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">prod</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">value</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parents</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_backward</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># multiply propagates value of other backwards (d(xy)/dx = y)</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parents</span><span class="p">:</span>
            <span class="n">p</span><span class="o">.</span><span class="n">gradient</span> <span class="o">+=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">value</span> <span class="o">/</span> <span class="n">p</span><span class="o">.</span><span class="n">value</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient</span>


<span class="k">class</span> <span class="nc">Sigmoid</span><span class="p">(</span><span class="n">Node</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># sigmoid(x) = 1 / (1 + exp(-x))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">parents</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">value</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">_backward</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1">#  derivative of sigmoid is sigmoid(x) * (1 - sigmoid(x))</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parents</span><span class="p">:</span>
            <span class="n">p</span><span class="o">.</span><span class="n">gradient</span> <span class="o">+=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">value</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">))</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient</span>


<span class="k">class</span> <span class="nc">LogLoss</span><span class="p">(</span><span class="n">Node</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># L = -y * log(yhat) - (1 - y) * log(1 - yhat)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">parents</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">value</span>
        <span class="n">yhat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">parents</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">value</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="o">-</span><span class="n">y</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">yhat</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">yhat</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_backward</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># dL_dyhat = -y / yhat + (1 - y) / (1 - yhat)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">parents</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">value</span>
        <span class="n">yhat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">parents</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">value</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">parents</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">gradient</span> <span class="o">+=</span> <span class="o">-</span><span class="p">(</span><span class="n">y</span> <span class="o">/</span> <span class="n">yhat</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">yhat</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<section id="id3">
<h4>Forward Pass<a class="headerlink" href="#id3" title="Permalink to this heading">#</a></h4>
<p>Here’s the simple example network constructed with our <code class="docutils literal notranslate"><span class="pre">Node</span></code> classes:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># =================</span>
<span class="c1"># Row 1 in diagram</span>
<span class="c1"># =================</span>
<span class="n">x1</span> <span class="o">=</span> <span class="n">Node</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;x1&quot;</span><span class="p">)</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">Node</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;x2&quot;</span><span class="p">)</span>

<span class="n">w1</span> <span class="o">=</span> <span class="n">Node</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;w1&quot;</span><span class="p">)</span>
<span class="n">w2</span> <span class="o">=</span> <span class="n">Node</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;w2&quot;</span><span class="p">)</span>

<span class="c1"># =================</span>
<span class="c1"># Row 2 in diagram</span>
<span class="c1"># =================</span>
<span class="n">w1x1</span> <span class="o">=</span> <span class="n">Multiply</span><span class="p">(</span><span class="n">parents</span><span class="o">=</span><span class="p">[</span><span class="n">w1</span><span class="p">,</span> <span class="n">x1</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;w1x1&quot;</span><span class="p">)</span>
<span class="n">w2x2</span> <span class="o">=</span> <span class="n">Multiply</span><span class="p">(</span><span class="n">parents</span><span class="o">=</span><span class="p">[</span><span class="n">w2</span><span class="p">,</span> <span class="n">x2</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;w2x2&quot;</span><span class="p">)</span>

<span class="n">b</span> <span class="o">=</span> <span class="n">Node</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;b&quot;</span><span class="p">)</span>

<span class="c1"># =================</span>
<span class="c1"># Row 3 in diagram</span>
<span class="c1"># =================</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">Add</span><span class="p">(</span><span class="n">parents</span><span class="o">=</span><span class="p">[</span><span class="n">w1x1</span><span class="p">,</span> <span class="n">w2x2</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;z&quot;</span><span class="p">)</span>

<span class="c1"># =================</span>
<span class="c1"># Row 4 in diagram</span>
<span class="c1"># =================</span>
<span class="n">yhat</span> <span class="o">=</span> <span class="n">Sigmoid</span><span class="p">(</span><span class="n">parents</span><span class="o">=</span><span class="p">[</span><span class="n">z</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;yhat&quot;</span><span class="p">)</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">Node</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>

<span class="c1"># =================</span>
<span class="c1"># Row 5 in diagram</span>
<span class="c1"># =================</span>
<span class="n">L</span> <span class="o">=</span> <span class="n">LogLoss</span><span class="p">(</span><span class="n">parents</span><span class="o">=</span><span class="p">[</span><span class="n">y</span><span class="p">,</span> <span class="n">yhat</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;L&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>To compute the output we set the input data and parameter values then call the forward method on the final node (the loss node):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x1</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="mf">1.5</span>
<span class="n">x2</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="mf">2.5</span>
<span class="n">w1</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="o">-</span><span class="mi">4</span>
<span class="n">w2</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">b</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.0</span>
<span class="n">y</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="mi">1</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">L</span><span class="o">.</span><span class="n">forward</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">------------</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">yhat</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot; | manual result was </span><span class="si">{</span><span class="n">yhat_manual</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot; | manual result was </span><span class="si">{</span><span class="n">L_manual</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>computing value of y
computing value of w1
computing value of x1
computing value of w1x1
computing value of w2
computing value of x2
computing value of w2x2
computing value of b
computing value of z
computing value of yhat
computing value of L

------------

yhat: Sigmoid(value=0.6225, gradient=0.0000)  | manual result was 0.6225
L: LogLoss(value=0.4741, gradient=0.0000)  | manual result was 0.4741
</pre></div>
</div>
</div>
</div>
</section>
<section id="id4">
<h4>Backward Pass<a class="headerlink" href="#id4" title="Permalink to this heading">#</a></h4>
<p>To compute the gradient we now just need to call the backward method on the loss node:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">L</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">------------</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">w1</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot; | manual result was </span><span class="si">{</span><span class="n">dL_dw1</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">w2</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;  | manual result was </span><span class="si">{</span><span class="n">dL_dw2</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;  | manual result was </span><span class="si">{</span><span class="n">dL_db</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>computing gradient for parents of L: [&#39;y&#39;, &#39;yhat&#39;]
computing gradient for parents of yhat: [&#39;z&#39;]
computing gradient for parents of z: [&#39;w1x1&#39;, &#39;w2x2&#39;, &#39;b&#39;]
computing gradient for parents of w2x2: [&#39;w2&#39;, &#39;x2&#39;]
computing gradient for parents of w1x1: [&#39;w1&#39;, &#39;x1&#39;]

------------

w1: Node(value=-4.0000, gradient=-0.5663)  | manual result was -0.5663
w2: Node(value=3.0000, gradient=-0.9439)   | manual result was -0.9439
b: Node(value=-1.0000, gradient=-0.3775)   | manual result was -0.3775
</pre></div>
</div>
</div>
</div>
<p>Both the forward and backward results match the manual implementation from before.</p>
</section>
</section>
<section id="pytorch-implementation">
<h3>Pytorch Implementation<a class="headerlink" href="#pytorch-implementation" title="Permalink to this heading">#</a></h3>
<p>Our <code class="docutils literal notranslate"><span class="pre">Node</span></code> implementation is similar (but much less efficient) to what frameworks like <code class="docutils literal notranslate"><span class="pre">pytorch</span></code> offer. We can do the same operations with pytorch’s tensor class, being careful to:</p>
<ul class="simple">
<li><p>Set <code class="docutils literal notranslate"><span class="pre">requires_grad=True</span></code> for the parameters we’re interested in the gradients of (<code class="docutils literal notranslate"><span class="pre">w1</span></code>, <code class="docutils literal notranslate"><span class="pre">w2</span></code>, and <code class="docutils literal notranslate"><span class="pre">b</span></code>) - we’ll come back to this later</p></li>
<li><p>Use torch’s implementations of sigmoid (<code class="docutils literal notranslate"><span class="pre">torch.sigmoid</span></code>) and log loss (<code class="docutils literal notranslate"><span class="pre">torch.nn.functional.binary_cross_entropy</span></code>).</p></li>
</ul>
<section id="id5">
<h4>Forward Pass<a class="headerlink" href="#id5" title="Permalink to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Forward pass</span>

<span class="c1"># =================</span>
<span class="c1"># Row 1 in diagram</span>
<span class="c1"># =================</span>
<span class="n">x1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.5</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">2.5</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">w1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">4.0</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">w2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">3.0</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># =================</span>
<span class="c1"># Row 2 in diagram</span>
<span class="c1"># =================</span>
<span class="n">w1x1</span> <span class="o">=</span> <span class="n">w1</span> <span class="o">*</span> <span class="n">x1</span>
<span class="n">w2x2</span> <span class="o">=</span> <span class="n">w2</span> <span class="o">*</span> <span class="n">x2</span>

<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">1.0</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># =================</span>
<span class="c1"># Row 3 in diagram</span>
<span class="c1"># =================</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">w1x1</span> <span class="o">+</span> <span class="n">w2x2</span> <span class="o">+</span> <span class="n">b</span>

<span class="c1"># =================</span>
<span class="c1"># Row 4 in diagram</span>
<span class="c1"># =================</span>
<span class="n">yhat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># =================</span>
<span class="c1"># Row 5 in diagram</span>
<span class="c1"># =================</span>
<span class="n">L</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">binary_cross_entropy</span><span class="p">(</span><span class="n">yhat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;yhat = </span><span class="si">{</span><span class="n">yhat</span><span class="si">}</span><span class="s2"> | manual result was </span><span class="si">{</span><span class="n">yhat_manual</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;L = </span><span class="si">{</span><span class="n">L</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> | manual result was </span><span class="si">{</span><span class="n">L_manual</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>yhat = tensor([0.6225], grad_fn=&lt;SigmoidBackward0&gt;) | manual result was 0.6225
L = 0.4741 | manual result was 0.4741
</pre></div>
</div>
</div>
</div>
<p>Note the values are the same as our own version above.</p>
</section>
<section id="id6">
<h4>Backward Pass<a class="headerlink" href="#id6" title="Permalink to this heading">#</a></h4>
<p>Now for the magic - in the background pytorch has built a computation graph from the variables we’ve defined and can compute the gradients (do a backward pass) for us automatically (for the parameters where we’ve specified <code class="docutils literal notranslate"><span class="pre">requires_grad=True</span></code>). You’ll notice that the <code class="docutils literal notranslate"><span class="pre">yhat</span></code> tensor above contains both its value and a function for computing gradients with respect to it (<code class="docutils literal notranslate"><span class="pre">&lt;SigmoidBackward0&gt;</span></code>).</p>
<p>To do the backward pass and compute the gradients we just need to do:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">L</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;dL_dw1 = </span><span class="si">{</span><span class="n">w1</span><span class="o">.</span><span class="n">grad</span><span class="si">}</span><span class="s2"> | manual result was </span><span class="si">{</span><span class="n">dL_dw1</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;dL_dw2 = </span><span class="si">{</span><span class="n">w2</span><span class="o">.</span><span class="n">grad</span><span class="si">}</span><span class="s2"> | manual result was </span><span class="si">{</span><span class="n">dL_dw2</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;dL_db = </span><span class="si">{</span><span class="n">b</span><span class="o">.</span><span class="n">grad</span><span class="si">}</span><span class="s2">  | manual result was </span><span class="si">{</span><span class="n">dL_db</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>dL_dw1 = tensor([-0.5663]) | manual result was -0.5663
dL_dw2 = tensor([-0.9439]) | manual result was -0.9439
dL_db = tensor([-0.3775])  | manual result was -0.3775
</pre></div>
</div>
</div>
</div>
<p>Again, these match the gradients in our own version, but with a lot fewer lines of code (for us) 🎉</p>
<p>You might remember seeing <code class="docutils literal notranslate"><span class="pre">.backward()</span></code> before in the linear/logistic regression notebooks - hopefully this gives you the intuition for what it’s doing!</p>
</section>
</section>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://www.coursera.org/learn/neural-networks-deep-learning/home/info"><em>“Neural Networks and Deep Learning”</em>, DeepLearning.AI, Andrew Ng (Coursera)</a></p></li>
<li><p><a class="reference external" href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi"><em>“Neural Networks”</em>, 3Blue1Brown (YouTube)</a></p></li>
<li><p><a class="reference external" href="https://www.youtube.com/watch?v=VMj-3S1tku0"><em>“The spelled-out intro to neural networks and backpropagation: building micrograd”</em>, Andrej Karpathy (YouTube)</a></p></li>
<li><p><a class="reference external" href="https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html"><em>“A Gentle Introduction to torch.autograd”</em> (PyTorch)</a></p></li>
<li><p><a class="reference external" href="http://colah.github.io/posts/2015-08-Backprop/"><em>“Calculus on Computational Graphs: Backpropagation”</em>, Christopher Olah</a></p></li>
<li><p><a class="reference external" href="https://simple-english-machine-learning.readthedocs.io/en/latest/neural-networks/computational-graphs.html"><em>“Computational graphs and gradient flows”</em>, Simple English Machine Learning</a></p></li>
<li><p><a class="reference external" href="https://math.hmc.edu/calculus/hmc-mathematics-calculus-online-tutorials/multivariable-calculus/multi-variable-chain-rule/"><em>“Multi-variable Chain Rule”</em>, Harvey Mudd College</a></p></li>
<li><p><a class="reference external" href="https://jonaslalin.com/2021/12/10/feedforward-neural-networks-part-1/"><em>“Feedforward Neural Networks in Depth, Part 1: Forward and Backward Propagations”</em>, Jonas Lalin</a></p></li>
<li><p><a class="reference external" href="http://neuralnetworksanddeeplearning.com/chap2.html"><em>“How the backpropagation algorithm works”</em>, Michael Nielsen (Neural Networks and Deep Learning)</a></p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./learningdl"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="02_logistic_regression.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">02: Logistic Regression</p>
      </div>
    </a>
    <a class="right-next"
       href="04_matrix_notation.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">04: Matrix Notation</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#forward-pass-recap">Forward Pass: Recap</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-the-parameters-how-not-to-do-it">Learning the Parameters (how not to do it)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-network-as-a-composed-function">Neural Network as a Composed Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#chain-rule-case-1-f-is-a-function-of-g-and-g-is-a-function-of-x">Chain Rule Case 1: <span class="math notranslate nohighlight">\(f\)</span> is a function of <span class="math notranslate nohighlight">\(g\)</span>, and <span class="math notranslate nohighlight">\(g\)</span> is a function of <span class="math notranslate nohighlight">\(x\)</span></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example">Example</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Back Propagation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-1-computing-the-gradient-for-b-1-2">Example 1: Computing the gradient for <span class="math notranslate nohighlight">\(b_1^{[2]}\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-2-computing-the-gradient-for-w-2-1">Example 2: Computing the gradient for <span class="math notranslate nohighlight">\(w_2^{[1]}\)</span></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multiple-paths">Multiple Paths</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#chain-rule-case-2-f-is-a-function-of-g-and-h-which-are-both-functions-of-x">Chain Rule Case 2: <span class="math notranslate nohighlight">\(f\)</span> is a function of <span class="math notranslate nohighlight">\(g\)</span> and <span class="math notranslate nohighlight">\(h\)</span>, which are both functions of <span class="math notranslate nohighlight">\(x\)</span></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Example</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#back-to-the-neural-network">Back to the neural network</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#back-propagation-and-efficiency">Back Propagation and Efficiency</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#computation-graphs-and-auto-differentiation">Computation Graphs and Auto-Differentiation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#manual-implementation">Manual Implementation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#forward-pass">Forward Pass</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#backward-pass">Backward Pass</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#graph-implementation">Graph Implementation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Forward Pass</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Backward Pass</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch-implementation">Pytorch Implementation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Forward Pass</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Backward Pass</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Jack Roberts
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>