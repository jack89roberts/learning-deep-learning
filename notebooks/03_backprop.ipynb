{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "217dd62c",
   "metadata": {},
   "source": [
    "# 03: Back Propagation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0bb85ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import prod\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f48ee4d",
   "metadata": {},
   "source": [
    "![](../img/03_forward_backward.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef41a7e6",
   "metadata": {},
   "source": [
    "The training loop for a neural network involves:\n",
    "\n",
    "1. A **forward pass**: Feed the input features ($x_1$, $x_2$) through all the layers of the network to compute our predictions, $\\hat{y}$.\n",
    "\n",
    "2. Compute the **loss** (or cost), $\\mathcal{L}(y, \\hat{y})$, a function of the predicted values $\\hat{y}$ and the actual values $y$.\n",
    "\n",
    "3. A **backward pass** (or _back propagation_): Feed the loss $\\mathcal{L}$ back through the network to compute the rate of change of the loss (i.e. the derivative) with respect to the network parameters (the weights and biases for each node, $w$, $b$)\n",
    "\n",
    "4. Given their derivatives, update the network parameters ($w$, $b$) using an algorithm like gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aff8139",
   "metadata": {},
   "source": [
    "## Forward Pass: Recap\n",
    "\n",
    "Each node computes a linear combination of the output of all the nodes in the previous layer, for example:\n",
    "\n",
    "$$\n",
    "z_1^{[1]} = w_{1 \\rightarrow 1}^{[1]} x_1 + w_{2 \\rightarrow 1}^{[1]} x_2 + b_1^{[1]}\n",
    "$$\n",
    "\n",
    "This is passed to an activation function, $g$, (assumed to be the same function in all layers here), to create the final output, or \"activation\", of each node:\n",
    "\n",
    "$$\n",
    "a_3^{[2]} = g(z_3^{[2]})\n",
    "$$\n",
    "\n",
    "For example, $\\hat{y}$, can be expressed in terms of the activation of the final layer as follows:\n",
    "\n",
    "$$\n",
    "\\hat{y} = a_1^{[3]} = g\\left(w_{1 \\rightarrow 1}^{[3]} a_{1}^{[2]} + w_{2 \\rightarrow 1}^{[3]} a_{2}^{[2]} + w_{3 \\rightarrow 1}^{[3]} a_{3}^{[2]} + b_1^{[3]}\\right)\n",
    "$$\n",
    "\n",
    "The terms not introduced above mean:\n",
    "\n",
    "- $w_{j \\rightarrow k}^{[l]}$: The weight between node $j$ in layer $l-1$ and node $k$ in layer $l$.\n",
    "- $a_k^{[l]}$: The activation of node $k$ in layer $l$\n",
    "- $b_k^{[l]}$: The bias term for node $k$ in layer $l$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3117a35",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "Let's consider a simpler network, with one input, two hidden nodes, and one output:\n",
    "\n",
    "![](../img/03_backprop_example_params.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce578c66",
   "metadata": {},
   "source": [
    "Here I've also included a node after the network's output to represent the calculation of the loss, $\\mathcal{L}(y, \\hat{y})$, where $\\hat{y} = g(z_1^{[2]})$ is the predicted value from the network and $y$ the true value.\n",
    "\n",
    "This network has seven parameters: $w_1^{[1]}$, $w_2^{[1]}$, $b_1^{[1]}$, $b_2^{[1]}$, $w_1^{[2]}$, $w_2^{[2]}$, $b_1^{[2]}$\n",
    "\n",
    "In gradient descent we use the partial derivative of the loss function with respect to the parameters to update the network, making small changes to the parameters like:\n",
    "\n",
    "$$\n",
    "w_1^{[1]}  = w_1^{[1]} - \\alpha\\frac{\\partial \\mathcal{L}}{\\partial w_1^{[1]}}\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is the learning rate.\n",
    "\n",
    "So to perform gradient descent we need the derivatives for each parameter, i.e. we need to compute:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w_1^{[1]}},\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w_2^{[1]}},\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b_1^{[1]}},\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b_2^{[1]}},\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w_1^{[2]}},\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w_2^{[2]}},\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b_1^{[2]}}\n",
    "$$\n",
    "\n",
    "How can we compute all those terms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02243aaa",
   "metadata": {},
   "source": [
    "## Chain Rule (for derivatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ac753b",
   "metadata": {},
   "source": [
    "### Case 1: $f$ is a function of $g$, and $g$ is a function of $x$\n",
    "\n",
    "$$\n",
    "f = f(g(x))\n",
    "$$\n",
    "\n",
    "The chain rule states that the derivative of $w$ with respect to $x$ is given by:\n",
    "\n",
    "$$\n",
    "\\frac{\\mathrm{d} f}{\\mathrm{d}x} = \\frac{\\mathrm{d} f}{\\mathrm{d} g} \\frac{\\mathrm{d} g}{\\mathrm{d} x}\n",
    "$$\n",
    "\n",
    "#### Example\n",
    "\n",
    "Find the derivative of $f(x) = (e^x + x)^2$:\n",
    "\n",
    "Let:\n",
    "\n",
    "$$\n",
    "g(x) = e^x + x \\\\\n",
    "f(g) = g^2 \\\\\n",
    "$$\n",
    "\n",
    "Then by chain rule:\n",
    "\n",
    "$$\n",
    "\\frac{\\mathrm{d} f}{\\mathrm{d} g} = 2g \\\\\n",
    "\\frac{\\mathrm{d} g}{\\mathrm{d} x} = e^x + 1 \\\\\n",
    "\\frac{\\mathrm{d} f}{\\mathrm{d}x} = \\frac{\\mathrm{d} f}{\\mathrm{d} g} \\frac{\\mathrm{d} g}{\\mathrm{d} x} = 2g(e^x + 1) \\\\\n",
    "\\frac{\\mathrm{d} f}{\\mathrm{d}x} = 2(e^x + x)(e^x + 1) \n",
    "$$\n",
    "\n",
    "### Case 2: $f$ is a function of $g$ and $h$, which are both functions of $x$\n",
    "\n",
    "$$\n",
    "f = f(g(x), h(x))\n",
    "$$\n",
    "\n",
    "To find the derivative of $f$ with respect to $x$, the chain rule states that you must sum over its (partial) derivatives for each input ($g$, $h$):\n",
    "\n",
    "$$\n",
    "\\frac{\\mathrm{d} f}{\\mathrm{d}x} = \\frac{\\partial f}{\\partial g} \\frac{\\mathrm{d} g}{\\mathrm{d} x} + \\frac{\\partial f}{\\partial h} \\frac{\\mathrm{d} h}{\\mathrm{d} x}\n",
    "$$\n",
    "\n",
    "This is the _multi-variable_ chain rule.\n",
    "\n",
    "#### Example\n",
    "\n",
    "Find the derivative of:\n",
    "\n",
    "$$\n",
    "f(x) = x^2(3x+1) - \\sin(x^2) \\\\\n",
    "$$\n",
    "\n",
    "which can be written as:\n",
    "\n",
    "$$\n",
    "f(g, h) = h g - \\sin(h) \\\\\n",
    "g(x) = 3x + 1 \\\\\n",
    "h(x) = x^2\n",
    "$$\n",
    "\n",
    "Then by chain rule:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial g} = h\\\\\n",
    "\\frac{\\mathrm{d} g}{\\mathrm{d} x} = 3 \\\\\n",
    "\\frac{\\partial f}{\\partial h} = g - \\cos(h) \\\\\n",
    "\\frac{\\mathrm{d} h}{\\mathrm{d} x} = 2x \\\\\n",
    "\\frac{\\mathrm{d} f}{\\mathrm{d}x} = \\frac{\\partial f}{\\partial g} \\frac{\\mathrm{d} g}{\\mathrm{d} x} + \\frac{\\partial f}{\\partial h} \\frac{\\mathrm{d} h}{\\mathrm{d} x} =\n",
    "3h + 2x\\left(g-\\cos(h)\\right) \\\\\n",
    "\\frac{\\mathrm{d} f}{\\mathrm{d}x} = 3x^2 + 2x\\left(3x + 1 -\\cos(x^2)\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b692ea9",
   "metadata": {},
   "source": [
    "## Back Propagation\n",
    "\n",
    "Here's the example network again, but with each edge (arrow) labeled by the partial derviative between the two connected nodes:\n",
    "\n",
    "![](../img/03_backprop_example_diffs.png)\n",
    "\n",
    "To compute the derivative of the loss with respect to any term in the network we can use the chain rule. Starting with the loss on the right, we move \"backwards\" through the network, multiplying the partial derivatives until we get to the term we want."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a14e58",
   "metadata": {},
   "source": [
    "### Example 1: Computing the gradient for $b_1^{[2]}$\n",
    "\n",
    "Why the chain rule/why multiply backwards along arrows?\n",
    "\n",
    "- $\\mathcal{L}$ is a function of $a_1^{[2]}$,  $a_1^{[2]}$ is a function of $z_1^{[2]}$, and $z_1^{[2]}$ is a function of $b_1^{[2]}$\n",
    "- We could write that like $\\mathcal{L}\\left(a_1^{[2]}\\left(z_1^{[2]}\\left(b_1^{[2]}\\right)\\right)\\right)$, which has the form of \"case 1\" of the chain rule above except with 3 functions instead of 2, i.e. a composed function like $f\\left(g\\left(h\\left(x\\right)\\right)\\right)$\n",
    "\n",
    "So, applying the chain rule and multiplying derivatives backwards along the diagram:\n",
    "\n",
    "$$\n",
    "\\frac{\\color{red}{\\partial \\mathcal{L}}}{\\color{blue}{\\partial b_1^{[2]}}} = \\frac{\\partial z_1^{[2]}}{\\color{blue}{\\partial b_1^{[2]}}} \\frac{\\color{green}{\\partial a_1^{[2]}}}{\\partial z_1^{[2]}} \\frac{\\color{red}{\\partial \\mathcal{L}}}{\\color{green}{\\partial a_1^{[2]}}}\n",
    "$$\n",
    "\n",
    "Log loss for one data point (remembering that $\\hat{y} = a_1^{[2]}$):\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = - y \\log(a_1^{[2]}) - (1 - y)\\log(1 - a_1^{[2]}) \\\\\n",
    "\\frac{\\color{red}{\\partial \\mathcal{L}}}{\\color{green}{\\partial a_1^{[2]}}} = -\\frac{y}{a_1^{[2]}} + \\frac{1-y}{1-a_1^{[2]}}\n",
    "$$\n",
    "\n",
    "If using a sigmoid activation function:\n",
    "\n",
    "$$\n",
    "a_1^{[2]} = \\frac{1}{1+\\exp(-z_1^{[2]})} \\\\\n",
    "\\frac{\\color{green}{\\partial a_1^{[2]}}}{\\partial z_1^{[2]}} = a_1^{[2]} (1 - a_1^{[2]})\n",
    "$$\n",
    "\n",
    "$z_1^{[2]}$ is a linear combination of its inputs:\n",
    "\n",
    "$$\n",
    "z_1^{[2]} = w_1^{[2]}a_1^{[1]} + w_2^{[2]}a_2^{[1]} + b_1^{[2]} \\\\\n",
    "\\frac{\\partial z_1^{[2]}}{\\color{blue}{\\partial b_1^{[2]}}} = 1\n",
    "$$\n",
    "\n",
    "So overall we could write the loss derivative with respect to the bias as:\n",
    "\n",
    "$$\n",
    "\\frac{\\color{red}{\\partial \\mathcal{L}}}{\\color{blue}{\\partial b_1^{[2]}}} =\n",
    "1 . \\frac{\\color{green}{\\partial a_1^{[2]}}}{\\partial z_1^{[2]}}\n",
    "\\frac{\\color{red}{\\partial \\mathcal{L}}}{\\color{green}{\\partial a_1^{[2]}}} =\n",
    "\\frac{\\color{red}{\\partial \\mathcal{L}}}{\\partial z_1^{[2]}}\n",
    "$$\n",
    "\n",
    "Substituting in the earlier results above and simplifying gives:\n",
    "\n",
    "$$\n",
    "\\frac{\\color{red}{\\partial \\mathcal{L}}}{\\color{blue}{\\partial b_1^{[2]}}} =\n",
    "\\frac{\\color{red}{\\partial \\mathcal{L}}}{\\partial z_1^{[2]}} =\n",
    "a_1^{[2]} - y\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7070a1",
   "metadata": {},
   "source": [
    "### Example 2: Computing the gradient for $w_2^{[1]}$\n",
    "\n",
    "$$\n",
    "\\frac{\\color{red}{\\partial \\mathcal{L}}}{\\color{magenta}{\\partial w_2^{[1]}}} =\n",
    "\\frac{\\color{gray}{\\partial z_2^{[1]}}}{\\color{magenta}{\\partial w_2^{[1]}}}\n",
    "\\frac{\\color{orange}{\\partial a_2^{[1]}}}{\\color{gray}{\\partial z_2^{[1]}}}\n",
    "\\frac{\\partial z_1^{[2]}}{\\color{orange}{\\partial a_2^{[1]}}}\n",
    "\\frac{\\color{green}{\\partial a_1^{[2]}}}{\\partial z_1^{[2]}}\n",
    "\\frac{\\color{red}{\\partial \\mathcal{L}}}{\\color{green}{\\partial a_1^{[2]}}}\n",
    "$$\n",
    "\n",
    "We've seen the form of all the derivatives above in the first example, except for the first and third terms:\n",
    "\n",
    "**First term:**\n",
    "\n",
    "$$\n",
    "z_2^{[1]} = w_2^{[1]}x + b_2^{[1]} \\\\\n",
    "\\frac{\\color{gray}{\\partial z_2^{[1]}}}{\\color{magenta}{\\partial w_2^{[1]}}} = x = a_1^{[0]}\n",
    "$$\n",
    "\n",
    "For the weights after the first layer, the inputs $x$ are replaced by node activations $a$. We can relabel $x = a_1^{[0]}$ to make the general trend clearer.\n",
    "\n",
    "**Third term:**\n",
    "\n",
    "$$\n",
    "z_1^{[2]} = w_1^{[2]} a_1^{[1]} + w_2^{[2]} a_2^{[1]} + b_1^{[2]} \\\\\n",
    "\\frac{\\partial z_1^{[2]}}{\\color{orange}{\\partial a_2^{[1]}}} = w_2^{[2]}\n",
    "$$\n",
    "\n",
    "**Overall:**\n",
    "\n",
    "The last four terms on the right side of the expression for the derivative can be simplified to $\\color{red}{\\partial \\mathcal{L}} / \\color{gray}{\\partial z_2^{[1]}}$. Then we have:\n",
    "\n",
    "$$\n",
    "\\frac{\\color{red}{\\partial \\mathcal{L}}}{\\color{magenta}{\\partial w_2^{[1]}}} =\n",
    "\\frac{\\color{gray}{\\partial z_2^{[1]}}}{\\color{magenta}{\\partial w_2^{[1]}}}\n",
    "\\frac{\\color{red}{\\partial \\mathcal{L}}}{\\color{gray}{\\partial z_2^{[1]}}}\n",
    "=\n",
    "a_1^{[0]}\\frac{\\color{red}{\\partial \\mathcal{L}}}{\\color{gray}{\\partial z_2^{[1]}}}\n",
    "$$\n",
    "\n",
    "We can also compute that $\\frac{\\color{gray}{\\partial z_2^{[1]}}}{\\color{magenta}{\\partial b_2^{[1]}}} = 1$ (see example 1), so it follows that $\\frac{\\color{red}{\\partial \\mathcal{L}}}{\\color{magenta}{\\partial b_2^{[1]}}} =\\frac{\\color{red}{\\partial \\mathcal{L}}}{\\color{gray}{\\partial z_2^{[1]}}}$.\n",
    "\n",
    "From the previous results (here and in example 1) we can derive $\\color{red}{\\partial \\mathcal{L}} / \\color{gray}{\\partial z_2^{[1]}}$:\n",
    "\n",
    "$$\n",
    "\\frac{\\color{red}{\\partial \\mathcal{L}}}{\\color{gray}{\\partial z_2^{[1]}}} =\n",
    "\\frac{\\color{orange}{\\partial a_2^{[1]}}}{\\color{gray}{\\partial z_2^{[1]}}}\n",
    "\\frac{\\partial z_1^{[2]}}{\\color{orange}{\\partial a_2^{[1]}}}\n",
    "\\frac{\\color{green}{\\partial a_1^{[2]}}}{\\partial z_1^{[2]}}\n",
    "\\frac{\\color{red}{\\partial \\mathcal{L}}}{\\color{green}{\\partial a_1^{[2]}}} =\n",
    "\\frac{\\color{orange}{\\partial a_2^{[1]}}}{\\color{gray}{\\partial z_2^{[1]}}}\n",
    "\\frac{\\partial z_1^{[2]}}{\\color{orange}{\\partial a_2^{[1]}}}\n",
    "\\frac{\\color{red}{\\partial \\mathcal{L}}}{\\partial z_1^{[2]}} \\\\\n",
    "\\frac{\\color{red}{\\partial \\mathcal{L}}}{\\color{gray}{\\partial z_2^{[1]}}} =\n",
    "a_2^{[1]} (1 - a_2^{[1]}) w_2^{[2]}\\frac{\\color{red}{\\partial \\mathcal{L}}}{\\partial z_1^{[2]}}\n",
    "$$\n",
    "\n",
    "Note we already derived the value of the last term, $\\color{red}{\\partial \\mathcal{L}} / \\partial z_1^{[2]}$, in Example 1, so we don't need to re-compute it here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05774e1e",
   "metadata": {},
   "source": [
    "### Multiple Paths\n",
    "\n",
    "There is one case not covered by the simplified network and examples above - where you have multiple paths from the output (loss) back to the term of interest. Such as this:\n",
    "\n",
    "![](../img/03_backprop_multipath.png)\n",
    "\n",
    "In this case you must sum all the possible paths.\n",
    "\n",
    "Why?\n",
    "\n",
    "This also follows from the multi-variable chain rule (case 2 from earlier):\n",
    "\n",
    "- $\\mathcal{L}$ is a function of $a_1^{[3]}$\n",
    "- $a_1^{[3]}$ is a function of $z_1^{[3]}$\n",
    "- $z_1^{[3]}$ is a function of $a_1^{[2]}$ _and_ $a_2^{[2]}$\n",
    "- $a_1^{[2]}$ _and_ $a_2^{[2]}$ are functions of $b_1^{[1]}$ (through $z_1^{[2]}$ and $z_2^{[2]}$)\n",
    "\n",
    "This could be written:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}\\left(a_1^{[3]}\\left(z_1^{[3]}\\left(a_1^{[2]}\\left(b_1^{[1]}\\right), a_2^{[2]}\\left(b_1^{[1]}\\right)\\right)\\right)\\right)\n",
    "$$\n",
    "\n",
    "which has the form:\n",
    "\n",
    "$$\n",
    "f\\left(g\\left(x\\right), h\\left(x\\right)\\right)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\n",
    "f(g, h) = \\mathcal{L}\\left(a_1^{[3]}\\left(z_1^{[3]}\\left(g, h\\right)\\right)\\right) \\\\\n",
    "g(x) = a_1^{[2]}(x) \\\\\n",
    "h(x) = a_2^{[2]}(x) \\\\\n",
    "x = b_1^{[1]}\n",
    "$$\n",
    "\n",
    "(here $x$ is just an arbitrary label for a variable, _not_ the input data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced42966",
   "metadata": {},
   "source": [
    "### Back Propagation and Efficiency\n",
    "\n",
    "It's important to note that:\n",
    "\n",
    "- The derivatives in layer $l$ depend on the derivatives in layer $l + 1$ (so we pass gradients \"backwards\" through the network)\n",
    "- Each term is a fairly simple combination of quantities that must be computed during the forward pass (like the activation values in hidden layers)\n",
    "\n",
    "These properties of back propagation form the basis for efficient implementations in major frameworks (pytorch, Tensorflow, JAX etc.), mostly via:\n",
    "\n",
    "- Matrix operations\n",
    "- Computation graphs\n",
    "- Caching intermediate values\n",
    "- Automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89377e4",
   "metadata": {},
   "source": [
    "## Computation Graphs and Auto-Differentiation\n",
    "\n",
    "In the background, large frameworks like pytorch use computation graphs and \"auto-differentiation\" to be able to compute gradients efficiently.\n",
    "\n",
    "Here's an example of a simple logistic regression (one layer) network in the form of a _computation graph_:\n",
    "\n",
    "<img src=\"../img/03_computation_graph.png\" alt=\"Computation graph\" width=\"500\">\n",
    "\n",
    "- Each node represents either an input variable/parameter (white/clear background), or an operation that applies to one or more of the previously defined values. In this graph the operations are summation ($+$), multiplication ($*$), sigmoid ($g$), and log loss ($\\mathcal{L}$).\n",
    "- The values of all the nodes on a row must be known before the next row can be calculated.\n",
    "\n",
    "In the computation graph we'll store:\n",
    "\n",
    "- The relationships between all the nodes (how they are connected and the operations that are performed on them)\n",
    "- The value of each node for a given input\n",
    "- The gradient of each node for a given input, with respect to the final node\n",
    "\n",
    "Having all the node values and the relationships between the nodes let's us compute the gradient at each node efficiently.\n",
    "\n",
    "### Manual Implementation\n",
    "\n",
    "#### Forward Pass\n",
    "\n",
    "When doing a forward (top to bottom) pass through the network we store the values computed at all nodes (i.e. including the intermediate values on each row):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b6587e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yhat = 0.6225\n",
      "L = 0.4741\n"
     ]
    }
   ],
   "source": [
    "# Forward pass\n",
    "\n",
    "# =================\n",
    "# Row 1 in diagram\n",
    "# =================\n",
    "x1 = 1.5\n",
    "x2 = 2.5\n",
    "\n",
    "w1 = -4\n",
    "w2 = 3\n",
    "\n",
    "# =================\n",
    "# Row 2 in diagram\n",
    "# =================\n",
    "w1x1 = w1 * x1\n",
    "w2x2 = w2 * x2\n",
    "\n",
    "b = -1\n",
    "\n",
    "# =================\n",
    "# Row 3 in diagram\n",
    "# =================\n",
    "z = w1x1 + w2x2 + b\n",
    "\n",
    "# =================\n",
    "# Row 4 in diagram\n",
    "# =================\n",
    "yhat = 1 / (1 + np.exp(-z))  # sigmoid\n",
    "\n",
    "y = 1\n",
    "\n",
    "# =================\n",
    "# Row 5 in diagram\n",
    "# =================\n",
    "L = -y * np.log(yhat) - (1 - y) * np.log(1 - yhat)  # log loss\n",
    "\n",
    "print(f\"yhat = {yhat:.4f}\")\n",
    "print(f\"L = {L:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e752ea1",
   "metadata": {},
   "source": [
    "#### Backward Pass\n",
    "\n",
    "Now we can use the node values from the forward pass, our knowledge of the computation graph, and the chain rule to compute the gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1561380a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dL_dw1 = -0.5663\n",
      "dL_dw2 = -0.9439\n",
      "dL_db = -0.3775\n"
     ]
    }
   ],
   "source": [
    "# Backward pass\n",
    "\n",
    "# =================\n",
    "# Row 5 in diagram\n",
    "# =================\n",
    "dL_dyhat = -y / yhat + (1 - y) / (1 - yhat)  # derivative of log loss\n",
    "\n",
    "# =================\n",
    "# Row 4 in diagram\n",
    "# =================\n",
    "dyhat_dz = yhat * (1 - yhat)  # derivative of sigmoid\n",
    "\n",
    "dL_dz = dyhat_dz * dL_dyhat\n",
    "\n",
    "# =================\n",
    "# Row 3 in diagram\n",
    "# =================\n",
    "dz_dw1x1 = 1  # summation nodes pass the same gradient backwards\n",
    "dz_dw2x2 = 1  # e.g. z = w1x1 + w2x2 + b, dz/d(w2x2) = 1\n",
    "dz_db = 1\n",
    "\n",
    "dL_dw1x1 = dz_dw1x1 * dL_dz\n",
    "dL_dw2x2 = dz_dw2x2 * dL_dz\n",
    "dL_db = dz_db * dL_dz\n",
    "\n",
    "# =================\n",
    "# Row 2 in diagram\n",
    "# =================\n",
    "dw1x1_dw1 = x1  # multiplication node gradients take the value of the other input\n",
    "dw2x2_dw2 = x2  # e.g. d(w2x2) / d(w2) = x2\n",
    "\n",
    "dL_dw1 = dw1x1_dw1 * dL_dw1x1\n",
    "dL_dw2 = dw2x2_dw2 * dL_dw2x2\n",
    "\n",
    "print(f\"dL_dw1 = {dL_dw1:.4f}\")\n",
    "print(f\"dL_dw2 = {dL_dw2:.4f}\")\n",
    "print(f\"dL_db = {dL_db:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5754f7",
   "metadata": {},
   "source": [
    "This is an extremely verbose way of representing this, we'll see matrix notation in the next notebook that will let us represent networks in a much more concise way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d832ee4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save some results for later\n",
    "yhat_manual = yhat\n",
    "L_manual = L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0ad018",
   "metadata": {},
   "source": [
    "### Graph Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c915409",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_nodes(node, order=None, visited=None):\n",
    "    # topological sort\n",
    "    if order is None:\n",
    "        order = []\n",
    "    if visited is None:\n",
    "        visited = set()\n",
    "\n",
    "    if node not in visited:\n",
    "        visited.add(node)\n",
    "        for p in node.parents:\n",
    "            sort_nodes(p, order=order, visited=visited)\n",
    "        order.append(node)\n",
    "\n",
    "    return order\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(\n",
    "        self,\n",
    "        value=None,\n",
    "        gradient=0,\n",
    "        parents=None,\n",
    "        label=None,\n",
    "    ):\n",
    "        self.value = value\n",
    "        self.gradient = gradient\n",
    "        self.label = label\n",
    "        self.children = []\n",
    "\n",
    "        # set this node's inputs (parents)\n",
    "        if parents is None:\n",
    "            self.parents = []\n",
    "        else:\n",
    "            self.parents = parents\n",
    "\n",
    "        # now the parent nodes should include this node as a child\n",
    "        for p in self.parents:\n",
    "            p.children.append(self)\n",
    "\n",
    "    def __repr__(self):\n",
    "        rep = f\"{self.label}: \" if self.label is not None else \"\"\n",
    "        val = f\"{self.value:.4f}\" if self.value is not None else \"None\"\n",
    "        grad = f\"{self.gradient:.4f}\" if self.gradient is not None else \"None\"\n",
    "        return rep + f\"{type(self).__name__}(value={val}, gradient={grad})\"\n",
    "\n",
    "    def forward(self):\n",
    "        ordered_nodes = sort_nodes(self)\n",
    "        for n in ordered_nodes:\n",
    "            print(f\"computing value of {n.label}\")\n",
    "            n._forward()\n",
    "\n",
    "    def backward(self):\n",
    "        ordered_nodes = sort_nodes(self)\n",
    "        for n in ordered_nodes:\n",
    "            n.gradient = 0  # reset gradients\n",
    "        self.gradient = 1  # gradient with respect to self\n",
    "        for n in reversed(ordered_nodes):\n",
    "            if len(n.parents) > 0:\n",
    "                print(\n",
    "                    f\"computing gradient for parents of {n.label}: {[p.label for p in n.parents]}\"\n",
    "                )\n",
    "            n._backward()\n",
    "\n",
    "    def _forward(self):\n",
    "        # modifies self.value using parent(s).value\n",
    "        if self.value is None:\n",
    "            raise ValueError(f\"Value not set for {self}\")\n",
    "\n",
    "    def _backward(self):\n",
    "        # modifies parent(s).gradient using self.gradient\n",
    "        if len(self.parents) > 0:\n",
    "            raise ValueError(\"Node has parents but no _backward implementation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2c78151",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Add(Node):\n",
    "    def _forward(self):\n",
    "        self.value = sum(p.value for p in self.parents)\n",
    "\n",
    "    def _backward(self):\n",
    "        # add propagates gradient backwards (dx/dx = 1)\n",
    "        for p in self.parents:\n",
    "            p.gradient += self.gradient\n",
    "\n",
    "\n",
    "class Multiply(Node):\n",
    "    def _forward(self):\n",
    "        self.value = prod(p.value for p in self.parents)\n",
    "\n",
    "    def _backward(self):\n",
    "        # multiply propagates value of other backwards (d(xy)/dx = y)\n",
    "        for p in self.parents:\n",
    "            p.gradient += (self.value / p.value) * self.gradient\n",
    "\n",
    "\n",
    "class Sigmoid(Node):\n",
    "    def _forward(self):\n",
    "        # sigmoid(x) = 1 / (1 + exp(-x))\n",
    "        self.value = 1 / (1 + np.exp(-self.parents[0].value))\n",
    "\n",
    "    def _backward(self):\n",
    "        #  derivative of sigmoid is sigmoid(x) * (1 - sigmoid(x))\n",
    "        for p in self.parents:\n",
    "            p.gradient += (self.value * (1 - self.value)) * self.gradient\n",
    "\n",
    "\n",
    "class LogLoss(Node):\n",
    "    def _forward(self):\n",
    "        # L = -y * log(yhat) - (1 - y) * log(1 - yhat)\n",
    "        y = self.parents[0].value\n",
    "        yhat = self.parents[1].value\n",
    "        self.value = -y * np.log(yhat) - (1 - y) * np.log(1 - yhat)\n",
    "\n",
    "    def _backward(self):\n",
    "        # dL_dyhat = -y / yhat + (1 - y) / (1 - yhat)\n",
    "        y = self.parents[0].value\n",
    "        yhat = self.parents[1].value\n",
    "        self.parents[1].gradient += -(y / yhat) + (1 - y) / (1 - yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361799d3",
   "metadata": {},
   "source": [
    "#### Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58d73e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = Node(label=\"x1\")\n",
    "x2 = Node(label=\"x2\")\n",
    "\n",
    "w1 = Node(label=\"w1\")\n",
    "w2 = Node(label=\"w2\")\n",
    "\n",
    "w1x1 = Multiply(parents=[w1, x1], label=\"w1x1\")\n",
    "w2x2 = Multiply(parents=[w2, x2], label=\"w2x2\")\n",
    "\n",
    "b = Node(label=\"b\")\n",
    "\n",
    "z = Add(parents=[w1x1, w2x2, b], label=\"z\")\n",
    "\n",
    "yhat = Sigmoid(parents=[z], label=\"yhat\")\n",
    "\n",
    "y = Node(label=\"y\")\n",
    "\n",
    "L = LogLoss(parents=[y, yhat], label=\"L\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33cfc4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1.value = 1.5\n",
    "x2.value = 2.5\n",
    "w1.value = -4\n",
    "w2.value = 3\n",
    "b.value = -1.0\n",
    "y.value = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87c357e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing value of y\n",
      "computing value of w1\n",
      "computing value of x1\n",
      "computing value of w1x1\n",
      "computing value of w2\n",
      "computing value of x2\n",
      "computing value of w2x2\n",
      "computing value of b\n",
      "computing value of z\n",
      "computing value of yhat\n",
      "computing value of L\n",
      "\n",
      "------------\n",
      "\n",
      "yhat: Sigmoid(value=0.6225, gradient=0.0000)  | manual result was 0.6225\n",
      "L: LogLoss(value=0.4741, gradient=0.0000)  | manual result was 0.4741\n"
     ]
    }
   ],
   "source": [
    "L.forward()\n",
    "print(\"\\n------------\\n\")\n",
    "\n",
    "print(yhat, f\" | manual result was {yhat_manual:.4f}\")\n",
    "print(L, f\" | manual result was {L_manual:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e520c128",
   "metadata": {},
   "source": [
    "#### Backward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec3fc661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing gradient for parents of L: ['y', 'yhat']\n",
      "computing gradient for parents of yhat: ['z']\n",
      "computing gradient for parents of z: ['w1x1', 'w2x2', 'b']\n",
      "computing gradient for parents of w2x2: ['w2', 'x2']\n",
      "computing gradient for parents of w1x1: ['w1', 'x1']\n",
      "\n",
      "------------\n",
      "\n",
      "w1: Node(value=-4.0000, gradient=-0.5663)  | manual result was -0.5663\n",
      "w2: Node(value=3.0000, gradient=-0.9439)   | manual result was -0.9439\n",
      "b: Node(value=-1.0000, gradient=-0.3775)   | manual result was -0.3775\n"
     ]
    }
   ],
   "source": [
    "L.backward()\n",
    "print(\"\\n------------\\n\")\n",
    "\n",
    "print(w1, f\" | manual result was {dL_dw1:.4f}\")\n",
    "print(w2, f\"  | manual result was {dL_dw2:.4f}\")\n",
    "print(b, f\"  | manual result was {dL_db:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234165e8",
   "metadata": {},
   "source": [
    "### Pytorch Implementation\n",
    "\n",
    "We can manually do the same operations in pytorch and with pytorch's tensor class, being careful to:\n",
    "\n",
    "- Set `requires_grad=True` for the parameters we're interested in the gradients of (`w1`, `w2`, and `b`) - we'll come back to this later\n",
    "- Use torch's implementations of sigmoid (`torch.sigmoid`) and log loss (`torch.nn.functional.binary_cross_entropy`).\n",
    "\n",
    "#### Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb134666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yhat = tensor([0.6225], grad_fn=<SigmoidBackward0>) | manual result was 0.6225\n",
      "L = 0.4741 | manual result was 0.4741\n"
     ]
    }
   ],
   "source": [
    "# Forward pass\n",
    "\n",
    "# =================\n",
    "# Row 1 in diagram\n",
    "# =================\n",
    "x1 = torch.tensor([1.5], requires_grad=False)\n",
    "x2 = torch.tensor([2.5], requires_grad=False)\n",
    "\n",
    "w1 = torch.tensor([-4.0], requires_grad=True)\n",
    "w2 = torch.tensor([3.0], requires_grad=True)\n",
    "\n",
    "# =================\n",
    "# Row 2 in diagram\n",
    "# =================\n",
    "w1x1 = w1 * x1\n",
    "w2x2 = w2 * x2\n",
    "\n",
    "b = torch.tensor([-1.0], requires_grad=True)\n",
    "\n",
    "# =================\n",
    "# Row 3 in diagram\n",
    "# =================\n",
    "z = w1x1 + w2x2 + b\n",
    "\n",
    "# =================\n",
    "# Row 4 in diagram\n",
    "# =================\n",
    "yhat = torch.sigmoid(z)\n",
    "\n",
    "y = torch.tensor([1.0], requires_grad=False)\n",
    "\n",
    "# =================\n",
    "# Row 5 in diagram\n",
    "# =================\n",
    "L = nn.functional.binary_cross_entropy(yhat, y)\n",
    "\n",
    "print(f\"yhat = {yhat} | manual result was {yhat_manual:.4f}\")\n",
    "print(f\"L = {L:.4f} | manual result was {L_manual:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125bb76f",
   "metadata": {},
   "source": [
    "Note the values are the same as our own version above.\n",
    "\n",
    "#### Backward Pass\n",
    "\n",
    "Now for the magic - in the background pytorch has built a computation graph from the variables we've defined and can compute the gradients (do a backward pass) for us automatically (for the parameters where we've specified `requires_grad=True`). You'll notice that the `yhat` tensor above contains both its value and a function for computing gradients with respect to it (`<SigmoidBackward0>`).\n",
    "\n",
    "To do the backward pass and compute the gradients we just need to do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c756aa9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dL_dw1 = tensor([-0.5663]) | manual result was -0.5663\n",
      "dL_dw2 = tensor([-0.9439]) | manual result was -0.9439\n",
      "dL_db = tensor([-0.3775])  | manual result was -0.3775\n"
     ]
    }
   ],
   "source": [
    "L.backward()\n",
    "\n",
    "print(f\"dL_dw1 = {w1.grad} | manual result was {dL_dw1:.4f}\")\n",
    "print(f\"dL_dw2 = {w2.grad} | manual result was {dL_dw2:.4f}\")\n",
    "print(f\"dL_db = {b.grad}  | manual result was {dL_db:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446aea94",
   "metadata": {},
   "source": [
    "Again, these match the gradients in our own version, but with a lot fewer lines of code (for us) 🎉\n",
    "\n",
    "You might remember seeing `.backward()` before in the linear/logistic regression notebooks - hopefully this gives you the intuition for what it's doing! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa47ab96",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [_\"Neural Networks and Deep Learning\"_, DeepLearning.AI, Andrew Ng (Coursera)](https://www.coursera.org/learn/neural-networks-deep-learning/home/info)\n",
    "- [_\"Neural Networks\"_, 3Blue1Brown (YouTube)](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)\n",
    "- [_\"The spelled-out intro to neural networks and backpropagation: building micrograd\"_, Andrej Karpathy (YouTube)](https://www.youtube.com/watch?v=VMj-3S1tku0)\n",
    "- [_\"A Gentle Introduction to torch.autograd\"_ (PyTorch)](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)\n",
    "- [_\"Calculus on Computational Graphs: Backpropagation\"_, Christopher Olah](http://colah.github.io/posts/2015-08-Backprop/)\n",
    "- [_\"Computational graphs and gradient flows\"_, Simple English Machine Learning](https://simple-english-machine-learning.readthedocs.io/en/latest/neural-networks/computational-graphs.html)\n",
    "- [_\"Multi-variable Chain Rule\"_, Harvey Mudd College](https://math.hmc.edu/calculus/hmc-mathematics-calculus-online-tutorials/multivariable-calculus/multi-variable-chain-rule/)\n",
    "- [_\"Feedforward Neural Networks in Depth, Part 1: Forward and Backward Propagations\"_, Jonas Lalin](https://jonaslalin.com/2021/12/10/feedforward-neural-networks-part-1/)\n",
    "- [_\"How the backpropagation algorithm works\"_, Michael Nielsen (Neural Networks and Deep Learning)](http://neuralnetworksanddeeplearning.com/chap2.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
