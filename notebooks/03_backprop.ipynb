{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "217dd62c",
   "metadata": {},
   "source": [
    "# 03: Back Propagation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f48ee4d",
   "metadata": {},
   "source": [
    "![](../img/03_forward_backward.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef41a7e6",
   "metadata": {},
   "source": [
    "The training loop for a neural network involves:\n",
    "\n",
    "1. A **forward pass**: Feed the input features ($x_1$, $x_2$) through all the layers of the network to compute our predictions, $\\hat{y}$.\n",
    "\n",
    "2. Compute the **loss** (or cost), $\\mathcal{L}(y, \\hat{y})$, a function of the predicted values $\\hat{y}$ and the actual values $y$.\n",
    "\n",
    "3. A **backward pass** (or _back propagation_): Feed the loss $\\mathcal{L}$ back through the network to compute the rate of change of the loss (i.e. the derivative) with respect to the network parameters (the weights and biases for each node, $w$, $b$)\n",
    "\n",
    "4. Given their derivatives, update the network parameters ($w$, $b$) using an algorithm like gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aff8139",
   "metadata": {},
   "source": [
    "## Forward Pass: Recap\n",
    "\n",
    "Each node computes a linear combination of the output of all the nodes in the previous layer, for example:\n",
    "\n",
    "$$\n",
    "z_1^{[1]} = w_{1,1}^{[1]} x_1 + w_{2,1}^{[1]} x_2 + b_1^{[1]}\n",
    "$$\n",
    "\n",
    "This is passed to an activation function, $g$, (assumed to be the same function in all layers here), to create the final output, or \"activation\", of each node:\n",
    "\n",
    "$$\n",
    "a_3^{[2]} = g(z_3^{[2]})\n",
    "$$\n",
    "\n",
    "For example, $\\hat{y}$, can be expressed in terms of the activation of the final layer as follows:\n",
    "\n",
    "$$\n",
    "\\hat{y} = a_1^{[3]} = g\\left(w_{1,1}^{[3]} a_{1}^{[2]} + w_{2,1}^{[3]} a_{2}^{[2]} + w_{3,1}^{[3]} a_{3}^{[2]} + b_1^{[3]}\\right)\n",
    "$$\n",
    "\n",
    "I'm going to introduce a more efficient syntax in a moment but terms not introduced above mean:\n",
    "- $w_{j,k}^{[l]}$: The weight between node $j$ in layer $l-1$ and node $k$ in layer $l$.\n",
    "- $a_k^{[l]}$: The activation of node $k$ in layer $l$\n",
    "- $b_k^{[l]}$: The bias term for node $k$ in layer $l$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3117a35",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "Let's consider a simpler network, with one input, two hidden nodes, and one output:\n",
    "\n",
    "![](../img/03_backprop_example_params.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8c659e",
   "metadata": {},
   "source": [
    "Here I've also included a node after the network's output to represent the calculation of the loss, $\\mathcal{L}(y, \\hat{y})$, where $\\hat{y} = g(z_1^{[2]})$ is the predicted value from the network and $y$ the true value.\n",
    "\n",
    "This network has seven parameters: $w_1^{[1]}$, $w_2^{[1]}$, $b_1^{[1]}$, $b_2^{[1]}$, $w_1^{[2]}$, $w_2^{[2]}$, $b_1^{[2]}$\n",
    "\n",
    "In gradient descent we use the partial derivative of the loss function with respect to the parameters to update the network, making small changes to the parameters like:\n",
    "\n",
    "$$\n",
    "w_1^{[1]}  = w_1^{[1]} - \\alpha\\frac{\\partial \\mathcal{L}}{\\partial w_1^{[1]}}\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is the learning rate.\n",
    "\n",
    "So to perform gradient descent we need the derivatives for each parameter, i.e. we need to compute:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w_1^{[1]}},\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w_2^{[1]}},\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b_1^{[1]}},\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b_2^{[1]}},\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w_1^{[2]}},\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w_2^{[2]}},\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b_1^{[2]}}\n",
    "$$\n",
    "\n",
    "How can we compute all those terms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02243aaa",
   "metadata": {},
   "source": [
    "## Background: Chain Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ac753b",
   "metadata": {},
   "source": [
    "$$\n",
    "h(x) = f(g(x))\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\mathrm{d} h(x)}{\\mathrm{d}x} = \\frac{\\mathrm{d} f(u)}{\\mathrm{d}u} \\frac{\\mathrm{d} g(x)}{\\mathrm{d}x} \\\\\n",
    "h'(x) = f'(u)g'(x)\n",
    "$$\n",
    "\n",
    "\n",
    "e.g.\n",
    "\n",
    "$$\n",
    "f(u) = u^2 \\\\\n",
    "g(x) = e^x + x  \\\\\n",
    "h(x) = f(g(x)) = (e^x + x)^2 \\\\\n",
    "$$\n",
    "\n",
    "$$\n",
    "g'(x) = e^x + 1 \\\\\n",
    "f'(u) = 2u \\\\\n",
    "u = g(x) = e^x + x \\\\\n",
    "h'(x) = 2(e^x + x)(e^x + 1) \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91902104",
   "metadata": {},
   "source": [
    "### Multi-variate chain rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f5e155",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6767fe7",
   "metadata": {},
   "source": [
    "## Back Propagation\n",
    "\n",
    "Here's the example network again, but with each edge (arrow) labeled by the partial derviative between the two connected nodes:\n",
    "\n",
    "![](../img/03_backprop_example_diffs.png)\n",
    "\n",
    "To compute the derivative of the loss with respect to any term in the network we can use the chain rule. Starting with the loss on the right, we move \"backwards\" through the network, multiplying the partial derivatives until we get to the term we want."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86283ecc",
   "metadata": {},
   "source": [
    "### Example 1: Computing the gradient for $b_1^{[2]}$\n",
    "\n",
    "$$\n",
    "\\frac{\\color{red}{\\partial \\mathcal{L}}}{\\color{blue}{\\partial b_1^{[2]}}} = \\frac{\\color{red}{\\partial \\mathcal{L}}}{\\color{green}{\\partial a_1^{[2]}}} \\frac{\\color{green}{\\partial a_1^{[2]}}}{\\partial z_1^{[2]}} \\frac{\\partial z_1^{[2]}}{\\color{blue}{\\partial b_1^{[2]}}}\n",
    "$$\n",
    "\n",
    "Log loss for one data point (remembering that $\\hat{y} = a_1^{[2]}$):\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = - y \\log(a_1^{[2]}) - (1 - y)\\log(1 - a_1^{[2]}) \\\\\n",
    "\\frac{\\color{red}{\\partial \\mathcal{L}}}{\\color{green}{\\partial a_1^{[2]}}} = -\\frac{y}{a_1^{[2]}} + \\frac{1-y}{1-a_1^{[2]}}\n",
    "$$\n",
    "\n",
    "If using a sigmoid activation function:\n",
    "\n",
    "$$\n",
    "a_1^{[2]} = \\frac{1}{1+\\exp(-z_1^{[2]})} \\\\\n",
    "\\frac{\\color{green}{\\partial a_1^{[2]}}}{\\partial z_1^{[2]}} = a_1^{[2]} (1 - a_1^{[2]})\n",
    "$$\n",
    "\n",
    "$z_1^{[2]}$ is a linear combination of its inputs:\n",
    "\n",
    "$$\n",
    "z_1^{[2]} = w_1^{[2]}a_1^{[1]} + w_2^{[2]}a_2^{[1]} + b_1^{[2]} \\\\\n",
    "\\frac{\\partial z_1^{[2]}}{\\color{blue}{\\partial b_1^{[2]}}} = 1\n",
    "$$\n",
    "\n",
    "So overall we could write the loss derivative with respect to the bias as:\n",
    "\n",
    "$$\n",
    "\\frac{\\color{red}{\\partial \\mathcal{L}}}{\\color{blue}{\\partial b_1^{[2]}}} = \\frac{\\color{red}{\\partial \\mathcal{L}}}{\\color{green}{\\partial a_1^{[2]}}} \\frac{\\color{green}{\\partial a_1^{[2]}}}{\\partial z_1^{[2]}}. 1 = \\frac{\\color{red}{\\partial \\mathcal{L}}}{\\partial z_1^{[2]}}\n",
    "$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "08caf266",
   "metadata": {},
   "source": [
    "### Example 2: Computing the gradient for $w_2^{[1]}$\n",
    "\n",
    "$$\n",
    "\\frac{\\color{red}{\\partial \\mathcal{L}}}{\\color{magenta}{\\partial w_2^{[1]}}} =\n",
    "\\frac{\\color{red}{\\partial \\mathcal{L}}}{\\color{green}{\\partial a_1^{[2]}}}\n",
    "\\frac{\\color{green}{\\partial a_1^{[2]}}}{\\partial z_1^{[2]}}\n",
    "\\frac{\\partial z_1^{[2]}}{\\color{orange}{\\partial a_2^{[1]}}}\n",
    "\\frac{\\color{orange}{\\partial a_2^{[1]}}}{\\color{gray}{\\partial z_2^{[1]}}}\n",
    "\\frac{\\color{gray}{\\partial z_2^{[1]}}}{\\color{magenta}{\\partial w_2^{[1]}}}\n",
    "$$\n",
    "\n",
    "We've seen the form of all the derivatives above in the first example, except for the last term:\n",
    "\n",
    "$$\n",
    "z_2^{[1]} = w_2^{[1]}x + b_2^{[1]} \\\\\n",
    "\\frac{\\color{gray}{\\partial z_2^{[1]}}}{\\color{magenta}{\\partial w_2^{[1]}}} = x\n",
    "$$\n",
    "\n",
    "For the weights after the first layer, the inputs $x$ are replaced by node activations $a$. We can relabel $x = a_1^{[0]}$ to make the general trend clearer.\n",
    "\n",
    "The first four terms on the right side of the expression for the derivative can be simplified to $\\color{red}{\\partial \\mathcal{L}} / \\color{gray}{\\partial z_2^{[1]}}$. Then we have:\n",
    "\n",
    "$$\n",
    "\\frac{\\color{red}{\\partial \\mathcal{L}}}{\\color{magenta}{\\partial w_2^{[1]}}} =\n",
    "\\frac{\\color{red}{\\partial \\mathcal{L}}}{\\color{gray}{\\partial z_2^{[1]}}}\n",
    "\\frac{\\color{gray}{\\partial z_2^{[1]}}}{\\color{magenta}{\\partial w_2^{[1]}}}\n",
    "=\n",
    "\\frac{\\color{red}{\\partial \\mathcal{L}}}{\\color{gray}{\\partial z_2^{[1]}}}\n",
    "a_1^{[0]}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b916ea",
   "metadata": {},
   "source": [
    "### Multiple Paths\n",
    "\n",
    "There is one case not covered by the simplified network and examples above - where you have multiple paths from the output (loss) back to the term of interest. Such as this:\n",
    "\n",
    "![](../img/03_backprop_multipath.png)\n",
    "\n",
    "In this case you must sum all the possible paths (this also follows from the multi-variate chain rule)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a3e0b2",
   "metadata": {},
   "source": [
    "### Back Propagation and Efficiency\n",
    "\n",
    "It's important to note that:\n",
    "\n",
    "- The derivatives in the two examples share many terms in common (e.g. the derivative of the loss with respect to the final output)\n",
    "- Each term is a fairly simple combination of quantities that must be computed during the forward pass (like the activation values in hidden layers)\n",
    "\n",
    "These properties of back propagation form the basis for efficient implementations in major frameworks (pytorch, Tensorflow, JAX etc.), mostly via:\n",
    "\n",
    "- Matrix operations\n",
    "- Computation graphs\n",
    "- Caching intermediate values\n",
    "- Automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b989646",
   "metadata": {},
   "source": [
    "## Neural Network Matrix Notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434c30e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78e8bda1",
   "metadata": {},
   "source": [
    "## Background: Vectors, Matrices, and NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a72162",
   "metadata": {},
   "source": [
    "### Dot Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8096db5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a423639",
   "metadata": {},
   "source": [
    "### Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1566ae26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b958f927",
   "metadata": {},
   "source": [
    "### Broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d7166f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa349a39",
   "metadata": {},
   "source": [
    "## Computation Graphs and Auto-Differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987cd398",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
