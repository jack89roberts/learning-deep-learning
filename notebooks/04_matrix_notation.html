
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>04: Matrix Notation &#8212; Learning Deep Learning</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="05: ReLU and Other Activation Functions" href="05_relu.html" />
    <link rel="prev" title="03: Back Propagation" href="03_backprop.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Learning Deep Learning</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Learning Deep Learning [WIP!]
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="01_linear_regression.html">
   01: Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="02_logistic_regression.html">
   02: Logistic Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03_backprop.html">
   03: Back Propagation
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   04: Matrix Notation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="05_relu.html">
   05: ReLU and Other Activation Functions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="06_multiclass.html">
   06:
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/jack89roberts/learning-deep-learning/main?urlpath=tree/notebooks/04_matrix_notation.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/jack89roberts/learning-deep-learning"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/jack89roberts/learning-deep-learning/issues/new?title=Issue%20on%20page%20%2Fnotebooks/04_matrix_notation.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/jack89roberts/learning-deep-learning/edit/main/notebooks/04_matrix_notation.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Edit this page"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="headerbtn__text-container">suggest edit</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/notebooks/04_matrix_notation.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#example-network-and-definitions">
   Example Network and Definitions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#notation-cheat-sheet">
   Notation Cheat Sheet
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#forward-pass">
   Forward Pass
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#value-for-a-single-node-and-data-sample-dot-products">
     Value for a single node and data sample: Dot products
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#values-for-all-nodes-in-a-layer-for-all-data-in-a-batch-matrix-multiplication">
     Values for all nodes in a layer for all data in a batch: Matrix multiplication
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#broadcasting">
     Broadcasting
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#summary">
     Summary
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#backward-pass">
   Backward Pass
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#key-equations">
     Key Equations
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#terms-needed-for-partial-mathcal-l-partial-mathbf-z-l">
     Terms needed for
     <span class="math notranslate nohighlight">
      \(\partial \mathcal{L} / \partial \mathbf{Z}^{[l]}\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#intuition-why-are-these-the-right-equations">
     Intuition - Why are these the right equations?
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#checking-dimensions">
       Checking Dimensions
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#activations">
       Activations
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#linear-predictor">
       Linear Predictor
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#bias">
       Bias
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#weights">
       Weights
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#why-matrices">
   Why matrices?
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>04: Matrix Notation</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#example-network-and-definitions">
   Example Network and Definitions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#notation-cheat-sheet">
   Notation Cheat Sheet
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#forward-pass">
   Forward Pass
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#value-for-a-single-node-and-data-sample-dot-products">
     Value for a single node and data sample: Dot products
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#values-for-all-nodes-in-a-layer-for-all-data-in-a-batch-matrix-multiplication">
     Values for all nodes in a layer for all data in a batch: Matrix multiplication
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#broadcasting">
     Broadcasting
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#summary">
     Summary
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#backward-pass">
   Backward Pass
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#key-equations">
     Key Equations
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#terms-needed-for-partial-mathcal-l-partial-mathbf-z-l">
     Terms needed for
     <span class="math notranslate nohighlight">
      \(\partial \mathcal{L} / \partial \mathbf{Z}^{[l]}\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#intuition-why-are-these-the-right-equations">
     Intuition - Why are these the right equations?
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#checking-dimensions">
       Checking Dimensions
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#activations">
       Activations
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#linear-predictor">
       Linear Predictor
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#bias">
       Bias
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#weights">
       Weights
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#why-matrices">
   Why matrices?
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="matrix-notation">
<h1>04: Matrix Notation<a class="headerlink" href="#matrix-notation" title="Permalink to this headline">#</a></h1>
<p>So far we’ve explicitly written individual terms for every parameter, data feature, and intermediate value in the network in the examples. This makes it clear exactly what is going on at each step, but it gets clumsy, long, and inefficient.</p>
<p>Here we show how we can represent networks with matrices, creating a much simpler (if less explicit) notation.</p>
<section id="example-network-and-definitions">
<h2>Example Network and Definitions<a class="headerlink" href="#example-network-and-definitions" title="Permalink to this headline">#</a></h2>
<p>We’ll refer back to the network below throughout this section. It has an unspecified number of layers (and nodes in each layer), but we assume 2 input data features, 3 nodes in the first layer, and 1 output node for the examples.</p>
<p><img alt="" src="../_images/04_matrix_network.png" /></p>
</section>
<section id="notation-cheat-sheet">
<h2>Notation Cheat Sheet<a class="headerlink" href="#notation-cheat-sheet" title="Permalink to this headline">#</a></h2>
<p>We have labelled the notation we’re going to start to use to represent the values/parameters in whole layers, rather than at individual nodes/connections. The terms are:</p>
<p><strong>Size of the network and dataset:</strong></p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(L\)</span> - the number of layers in the network</p></li>
<li><p><span class="math notranslate nohighlight">\(n^{[l]}\)</span> - the number of nodes in layer <span class="math notranslate nohighlight">\([l]\)</span></p>
<ul>
<li><p><span class="math notranslate nohighlight">\(n^{[0]}\)</span> - the number of input features (the number of nodes <span class="math notranslate nohighlight">\(x_i\)</span> in the input layer)</p></li>
<li><p><span class="math notranslate nohighlight">\(n^{[L]}\)</span> - the number of outputs (one in all the networks we’ve considered so far)</p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(m\)</span> - the number of data samples per gradient descent iteration (aka the batch size)</p></li>
</ul>
<p><strong>Parameters of, and computed values in, the network:</strong></p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf{W}^{[l]}\)</span> - the weights (arrows) between all the activations in layer <span class="math notranslate nohighlight">\([l-1]\)</span> and layer <span class="math notranslate nohighlight">\([l]\)</span>, dimensions: <span class="math notranslate nohighlight">\((n^{[l]}, n^{[l-1]})\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{b}^{[l]}\)</span> - the bias for all the nodes in layer <span class="math notranslate nohighlight">\([l]\)</span>, dimensions: <span class="math notranslate nohighlight">\((n^{[l]}, 1)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{Z}^{[l]}\)</span> - the linear predictor for all the nodes in layer <span class="math notranslate nohighlight">\([l]\)</span> for all the data samples, dimensions: <span class="math notranslate nohighlight">\((n^{[l]}, m)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{A}^{[l]}\)</span> - the activation for all the nodes in layer <span class="math notranslate nohighlight">\([l]\)</span> for all the data samples, dimensions: <span class="math notranslate nohighlight">\((n^{[l]}, m)\)</span></p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\mathbf{X} = \mathbf{A}^{[0]}\)</span> - we can represent the input dataset as the activations of the zeroth layer</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{\hat{y}} = \mathbf{A}^{[L]}\)</span> - the activations of the last layer are the predictions <span class="math notranslate nohighlight">\(\hat{y}\)</span></p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{L}(\mathbf{y}, \mathbf{\hat{y}})\)</span> - the total loss (cost) for all the data samples, dimensions: (1)</p></li>
</ul>
<p><strong>Matrix/vector operations:</strong></p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf{p . q}\)</span> is the dot product of a row vector <span class="math notranslate nohighlight">\(\mathbf{p}\)</span> with dimensions <span class="math notranslate nohighlight">\((1, j)\)</span>, and a column vector <span class="math notranslate nohighlight">\(\mathbf{q}\)</span> with dimensions <span class="math notranslate nohighlight">\((j, 1)\)</span>, and the result is a scalar value</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{PQ}\)</span> is matrix multiplication, where <span class="math notranslate nohighlight">\(\mathbf{P}\)</span> has dimensions <span class="math notranslate nohighlight">\((i, k)\)</span>, <span class="math notranslate nohighlight">\(\mathbf{Q}\)</span> has dimensions <span class="math notranslate nohighlight">\((k, j)\)</span>, and <span class="math notranslate nohighlight">\(\mathbf{PQ}\)</span> has dimensions <span class="math notranslate nohighlight">\((i, j)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{P * Q}\)</span> is element-wise multiplication, where <span class="math notranslate nohighlight">\(\mathbf{P}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{Q}\)</span> have the same dimensions <span class="math notranslate nohighlight">\((i, j)\)</span>, and <span class="math notranslate nohighlight">\(\mathbf{P * Q}\)</span> also has dimensions <span class="math notranslate nohighlight">\((i, j)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{P}^T\)</span> is the <em>transpose</em> of <span class="math notranslate nohighlight">\(\mathbf{P}\)</span>, where <span class="math notranslate nohighlight">\(\mathbf{P}\)</span> has dimensions <span class="math notranslate nohighlight">\((i, j)\)</span> and <span class="math notranslate nohighlight">\(\mathbf{P}^T\)</span> has dimensions <span class="math notranslate nohighlight">\((j, i)\)</span></p></li>
</ul>
</section>
<section id="forward-pass">
<h2>Forward Pass<a class="headerlink" href="#forward-pass" title="Permalink to this headline">#</a></h2>
<section id="value-for-a-single-node-and-data-sample-dot-products">
<h3>Value for a single node and data sample: Dot products<a class="headerlink" href="#value-for-a-single-node-and-data-sample-dot-products" title="Permalink to this headline">#</a></h3>
<p>In the network above, the value of <span class="math notranslate nohighlight">\(z_1^{[1](j)}\)</span> (the first node in the first hidden layer) for a single data point, <span class="math notranslate nohighlight">\((j)\)</span>, can be written as:</p>
<div class="math notranslate nohighlight">
\[
z_1^{[1](j)} = w_{1 \rightarrow 1}^{[1]} x_1^{(j)} + w_{2 \rightarrow 1}^{[1]} x_2^{(j)} + b_1^{[1]}
\]</div>
<p>The first two terms on the right (multiplying the inputs by the weights) can be expressed as a <em>dot product</em>:</p>
<div class="math notranslate nohighlight">
\[
z_1^{[1](j)} = \mathbf{w_{1}}^{[1]} \mathbf{. x}^{(j)} + b_1^{[1]}
\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf{w_1}^{[1]}\)</span> is a <em>row vector</em> of all the weights to the first node in the first layer</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{x}^{(j)}\)</span> is a <em>column vector</em> of all the data feature values for data sample <span class="math notranslate nohighlight">\((j)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(b_1^{[1]}\)</span> is the bias term for the first node in the first layer</p></li>
</ul>
<p>which can be expanded as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
z_1^{[1](j)} =
\begin{bmatrix}w_{1 \rightarrow 1}^{[1]} &amp; w_{2 \rightarrow 1}^{[1]}\end{bmatrix}
\begin{bmatrix}
x_{1}^{(j)} \\
x_{2}^{(j)} \\
\end{bmatrix}
 + b_1^{[1]}
\end{split}\]</div>
<p>and represents the same expression as the first equation above.</p>
</section>
<section id="values-for-all-nodes-in-a-layer-for-all-data-in-a-batch-matrix-multiplication">
<h3>Values for all nodes in a layer for all data in a batch: Matrix multiplication<a class="headerlink" href="#values-for-all-nodes-in-a-layer-for-all-data-in-a-batch-matrix-multiplication" title="Permalink to this headline">#</a></h3>
<p>By moving from vectors to matrices we can represent the terms for a whole layer (rather than a single node) and for a whole dataset in an efficient way.</p>
<p>Sticking to the the first layer in the network above as an example, the six weights (2 input nodes * 3 layer 1 nodes) can be represented as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{W}^{[1]} = 
\begin{bmatrix}
w_{1 \rightarrow 1}^{[1]} &amp; w_{2 \rightarrow 1}^{[1]} \\
w_{1 \rightarrow 2}^{[1]} &amp; w_{2 \rightarrow 2}^{[1]} \\
w_{1 \rightarrow 3}^{[1]} &amp; w_{2 \rightarrow 3}^{[1]}
\end{bmatrix}
\end{split}\]</div>
<p>where:</p>
<ul class="simple">
<li><p>each <em>row</em> corresponds to a node in the current layer (the 1st layer in this case) - there are <span class="math notranslate nohighlight">\(n^{[1]}\)</span> rows.</p></li>
<li><p>each <em>column</em> corresponds to a node in the previous layer (the zeroth layer in this case, aka the data inputs) -  there are <span class="math notranslate nohighlight">\(n^{[0]}\)</span> columns.</p></li>
<li><p>the values are the weights between those nodes (e.g. the value at row 3, column 2 is the weight between the 2nd input, <span class="math notranslate nohighlight">\(x_2\)</span>, and the 3rd node in the first layer).</p></li>
</ul>
<p>And the data as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{X} = \mathbf{A}^{[0]} =
\begin{bmatrix} 
x_{1}^{(1)} &amp; x_{1}^{(2)} &amp; x_{1}^{(3)} &amp; \dots &amp; x_{1}^{(m)} \\
x_{2}^{(1)} &amp; x_{2}^{(2)} &amp; x_{2}^{(3)} &amp; \dots &amp; x_{2}^{(m)} \\
\end{bmatrix}
\end{split}\]</div>
<p>where:</p>
<ul class="simple">
<li><p>each <em>row</em> corresponds to a feature in the data (or more generally a node activation in the previous layer) - there are <span class="math notranslate nohighlight">\(n^{[0]}\)</span> rows.</p></li>
<li><p>each <em>column</em> corresponds to a data sample - there are <span class="math notranslate nohighlight">\(m\)</span> columns.</p></li>
</ul>
<p>The linear predictor values for all the nodes in the first layer can then be expressed as:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{Z}^{[1]} = \mathbf{W}^{[1]} \mathbf{X} + \mathbf{b}^{[1]}
\]</div>
<p>The first term, <span class="math notranslate nohighlight">\(\mathbf{W}^{[1]} \mathbf{X}\)</span>, is a <em>matrix multiplication</em>, that (by the definition of matrix multiplication and given the way we have defined <span class="math notranslate nohighlight">\(\mathbf{W}^{[1]}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>), contains the dot product of the inputs and the weights for every node (each of the 3 nodes in the first layer) and every data sample:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{W}^{[1]} \mathbf{X} =
\begin{bmatrix}
\mathbf{w_{1}}^{[1]} . \mathbf{x}^{(1)} &amp; \mathbf{w_{1}}^{[1]} . \mathbf{x}^{(2)} &amp; \mathbf{w_{1}}^{[1]} . \mathbf{x}^{(3)} &amp; \dots &amp;  \mathbf{w_{1}}^{[1]} . \mathbf{x}^{(m)} \\
\mathbf{w_{2}}^{[1]} . \mathbf{x}^{(1)} &amp; \mathbf{w_{2}}^{[1]} . \mathbf{x}^{(2)} &amp; \mathbf{w_{2}}^{[1]} . \mathbf{x}^{(3)} &amp; \dots &amp;  \mathbf{w_{1}}^{[1]} . \mathbf{x}^{(m)} \\
\mathbf{w_{3}}^{[1]} . \mathbf{x}^{(1)} &amp; \mathbf{w_{3}}^{[1]} . \mathbf{x}^{(2)} &amp; \mathbf{w_{3}}^{[1]} . \mathbf{x}^{(3)} &amp; \dots &amp;  \mathbf{w_{1}}^{[1]} . \mathbf{x}^{(m)}
\end{bmatrix}
\end{split}\]</div>
</section>
<section id="broadcasting">
<h3>Broadcasting<a class="headerlink" href="#broadcasting" title="Permalink to this headline">#</a></h3>
<p><span class="math notranslate nohighlight">\(\mathbf{b}^{[l]}\)</span>, the last term in <span class="math notranslate nohighlight">\(\mathbf{Z}^{[1]}\)</span>, is a <em>column vector</em> containing the bias for each node in the layer:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{b}^{[l]} =
\begin{bmatrix} 
b_{1}^{[1]} \\
b_{2}^{[1]} \\
b_{3}^{[1]} \\
\end{bmatrix}
\end{split}\]</div>
<p>But now there’s a problem, we need to add <span class="math notranslate nohighlight">\(\mathbf{W}^{[1]} \mathbf{X}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{b}^{[l]}\)</span> but they have different dimensions - <span class="math notranslate nohighlight">\((3, m)\)</span> and <span class="math notranslate nohighlight">\((3, 1)\)</span> respectively.</p>
<p>We want to add the same bias values for each data sample. To do this we use <em>broadcasting</em> (which might be familiar to you from <code class="docutils literal notranslate"><span class="pre">numpy</span></code>). In other words, we create a <span class="math notranslate nohighlight">\((3, m)\)</span> matrix by copying <span class="math notranslate nohighlight">\(\mathbf{b}^{[l]}\)</span> <span class="math notranslate nohighlight">\(m\)</span> times:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{bmatrix} 
b_{1}^{[1]} &amp; b_{1}^{[1]} &amp; b_{1}^{[1]} &amp; \dots &amp; b_{1}^{[1]} \\
b_{2}^{[1]} &amp; b_{2}^{[1]} &amp; b_{2}^{[1]} &amp; \dots &amp; b_{2}^{[1]} \\
b_{3}^{[1]} &amp; b_{3}^{[1]} &amp; b_{3}^{[1]} &amp; \dots &amp; b_{3}^{[1]}
\end{bmatrix}
\end{split}\]</div>
<p>where each column is <span class="math notranslate nohighlight">\(\mathbf{b}^{[l]}\)</span>, and there are <span class="math notranslate nohighlight">\(m\)</span> columns. This matrix has the same dimensions as <span class="math notranslate nohighlight">\(\mathbf{W}^{[1]} \mathbf{X}\)</span>, so the two can now be added together.</p>
</section>
<section id="summary">
<h3>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">#</a></h3>
<p>We’ve now seen all the structures and operations that are needed to represent a forward pass through the network for a whole batch of data.</p>
<p>The general form for computing the linear predictor in any layer is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{Z}^{[l]} = \mathbf{W}^{[l]} \mathbf{A}^{[l-1]} + \mathbf{b}^{[l]} \\
(n^{[l]}, m) = (n^{[l]}, n^{[l-1]}) \times (n^{[l-1]}, m) + (n^{[l]}, 1) \\
\end{split}\]</div>
<p>where the second line shows the dimensions of each term.</p>
<p>To compute the node activations <span class="math notranslate nohighlight">\(\mathbf{A^{[l]}}\)</span>, we apply the activation function <span class="math notranslate nohighlight">\(g\)</span> (element-wise):</p>
<div class="math notranslate nohighlight">
\[
\mathbf{A}^{[l]} = g^{[l]}(\mathbf{Z}^{[l]})
\]</div>
<p>Where <span class="math notranslate nohighlight">\(g^{[l]}\)</span> is the activation function for layer <span class="math notranslate nohighlight">\(l\)</span> (different layers may use different activation functions).</p>
</section>
</section>
<section id="backward-pass">
<h2>Backward Pass<a class="headerlink" href="#backward-pass" title="Permalink to this headline">#</a></h2>
<p>We’ll start with the final matrix equations for back propagation, then work our way back to understanding <em>why</em> they’re  correct.</p>
<section id="key-equations">
<h3>Key Equations<a class="headerlink" href="#key-equations" title="Permalink to this headline">#</a></h3>
<div class="math notranslate nohighlight">
\[\begin{split}
\frac{\partial \mathcal{L}}{\partial \mathbf{W}^{[l]}} = \frac{1}{m} \frac{\partial \mathcal{L}}{\partial \mathbf{Z}^{[l]}} \mathbf{A}^{[l-1]^T} \\
\frac{\partial \mathcal{L}}{\partial \mathbf{b}^{[l]}} = \frac{1}{m} \sum_{\mathrm{rows}} \frac{\partial \mathcal{L}}{\partial \mathbf{Z}^{[l]}} \\
\frac{\partial \mathcal{L}}{\partial \mathbf{Z}^{[l]}} = \frac{\partial \mathcal{L}}{\partial \mathbf{A}^{[l]}} * \frac{\partial \mathbf{A}^{[l]}}{\partial \mathbf{Z}^{[l]}}
\end{split}\]</div>
</section>
<section id="terms-needed-for-partial-mathcal-l-partial-mathbf-z-l">
<h3>Terms needed for <span class="math notranslate nohighlight">\(\partial \mathcal{L} / \partial \mathbf{Z}^{[l]}\)</span><a class="headerlink" href="#terms-needed-for-partial-mathcal-l-partial-mathbf-z-l" title="Permalink to this headline">#</a></h3>
<p>For all layers except the last (output) layer:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial \mathbf{A}^{[l]}} = \mathbf{W}^{[l+1]^T} \frac{\partial \mathcal{L}}{\partial \mathbf{Z}^{[l+1]}}
\]</div>
<p>So:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial \mathbf{Z}^{[l]}} = \mathbf{W}^{[l+1]^T} \frac{\partial \mathcal{L}}{\partial \mathbf{Z}^{[l+1]}} * \frac{\partial \mathbf{A}^{[l]}}{\partial \mathbf{Z}^{[l]}}
\]</div>
<p>Where <span class="math notranslate nohighlight">\(\partial \mathbf{A}^{[l]} / \partial \mathbf{Z}^{[l]}\)</span> is the derivative of the activation function. For example, for sigmoid activation:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathbf{A}^{[l]}}{\partial \mathbf{Z}^{[l]}} = \mathbf{A}^{[l]} * (1 - \mathbf{A}^{[l]})
\]</div>
<p>For the output layer (<span class="math notranslate nohighlight">\(L\)</span>), the form of <span class="math notranslate nohighlight">\(\partial \mathcal{L}/\partial \mathbf{A}^{[L]}\)</span> depends on the the loss function used. Often the overall expression for <span class="math notranslate nohighlight">\(\partial \mathcal{L} / \partial \mathbf{Z}^{[L]}\)</span> (which you get by multiplying the loss and final activation function derivatives) has a convenient form. For log loss and sigmoid activation it’s:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial \mathbf{Z}^{[L]}} = \mathbf{A}^{[L]} - \mathbf{Y}
\]</div>
<p>For mean squared error loss and linear activation it’s:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial \mathbf{Z}^{[L]}} = 2\left(\mathbf{Z}^{[L]} - \mathbf{Y}\right)
\]</div>
</section>
<section id="intuition-why-are-these-the-right-equations">
<h3>Intuition - Why are these the right equations?<a class="headerlink" href="#intuition-why-are-these-the-right-equations" title="Permalink to this headline">#</a></h3>
<p>You may have already noticed similarities between the equations in matrix form above and the back propagation equations without using matrix notation in the previous notebook. They are the same equations, of course, but it can be difficult to see how this emerges through matrix multiplication and other operations. I try to solidify the link between the two forms here.</p>
<section id="checking-dimensions">
<h4>Checking Dimensions<a class="headerlink" href="#checking-dimensions" title="Permalink to this headline">#</a></h4>
<p>The notation cheat sheet earlier in this notebook gives the dimensions of all the terms we’re using. Derivatives take on the same dimension as the term in the denominator, e.g.</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial \mathbf{W}^{[l]}}
\]</div>
<p>is a matrix with the same dimensions as <span class="math notranslate nohighlight">\(\mathbf{W}^{[l]}\)</span>, namely <span class="math notranslate nohighlight">\((n^{[l]}, n^{[l-1]})\)</span>, where each element is the derivative of the loss with respect to a single weight.</p>
<p>In the full expression for the weight derivatives in a layer, we can check the dimensions make mathematical sense:
$<span class="math notranslate nohighlight">\(
\frac{\partial \mathcal{L}}{\partial \mathbf{W}^{[l]}} = \frac{1}{m} \frac{\partial \mathcal{L}}{\partial \mathbf{Z}^{[l]}} \mathbf{A}^{[l-1]^T} \\
(n^{[l]}, n^{[l-1]}) = (1) * (n^{[l]}, m) \times (m, n^{[l-1]})
\)</span>$</p>
<p>These are the correct dimensions for a valid matrix multiplication, which gives some reassurance that it’s sensible at least! We can repeat this for the other key equations:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\frac{\partial \mathcal{L}}{\partial \mathbf{b}^{[l]}} = \frac{1}{m} \sum_{\mathrm{rows}} \frac{\partial \mathcal{L}}{\partial \mathbf{Z}^{[l]}} \\
(n^{[l]}, 1) = (1) * \sum_{\mathrm{rows}} (n^{[l]}, m)
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\frac{\partial \mathcal{L}}{\partial \mathbf{Z}^{[l]}} = \frac{\partial \mathcal{L}}{\partial \mathbf{A}^{[l]}} * \frac{\partial \mathbf{A}^{[l]}}{\partial \mathbf{Z}^{[l]}} \\
(n^{[l]}, m) = (n^{[l]}, m) * (n^{[l]}, m)
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\frac{\partial \mathcal{L}}{\partial \mathbf{A}^{[l]}} = \mathbf{W}^{[l+1]^T} \frac{\partial \mathcal{L}}{\partial \mathbf{Z}^{[l+1]}} \\
(n^{[l]}, m) = (n^{[l]}, n^{[l+1]}) \times (n^{[l+1]}, m)
\end{split}\]</div>
</section>
<section id="activations">
<h4>Activations<a class="headerlink" href="#activations" title="Permalink to this headline">#</a></h4>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial \mathbf{A}^{[l]}} = \mathbf{W}^{[l+1]^T} \frac{\partial \mathcal{L}}{\partial \mathbf{Z}^{[l+1]}}
\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{W}^{[l+1]^T} = 
\begin{bmatrix}
w_{1 \rightarrow 1}^{[l+1]} &amp; \dots &amp; w_{1 \rightarrow n^{[l+1]}}^{[l+1]} \\
\vdots &amp; \ddots &amp; \vdots \\
w_{n^{[l]} \rightarrow 1}^{[l+1]} &amp; \dots &amp; w_{n^{[l]} \rightarrow n^{[l+1]}}^{[l+1]} \\
\end{bmatrix}
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\frac{\partial \mathcal{L}}{\partial \mathbf{Z}^{[l+1]}} = 
\begin{bmatrix}
\partial \mathcal{L}/\partial z_1^{[l+1](1)} &amp; \dots &amp; \partial \mathcal{L}/\partial z_1^{[l+1](m)} \\
\vdots &amp; \ddots &amp; \vdots \\
\partial \mathcal{L}/\partial z_{n^{[l+1]}}^{[l+1](1)} &amp; \dots &amp; \partial \mathcal{L}/\partial z_{n^{[l+1]}}^{[l+1](m)} \\
\end{bmatrix}
\end{split}\]</div>
<p>Gradient for the activation of node <span class="math notranslate nohighlight">\(k\)</span> in layer <span class="math notranslate nohighlight">\(l\)</span> for data sample <span class="math notranslate nohighlight">\(m\)</span>, i.e. element <span class="math notranslate nohighlight">\((k, m)\)</span> of <span class="math notranslate nohighlight">\(\partial \mathcal{L} / \partial \mathbf{A}^{[l]}\)</span>):</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial a_k^{[l](m)}} = w_{k \rightarrow 1}^{[l+1]} \frac{\partial \mathcal{L}}{\partial z_1^{[l+1](m)}} + \dots + w_{k \rightarrow n^{[l+1]}}^{[l+1]} \frac{\partial \mathcal{L}}{\partial z_{n^{[l+1]}}^{[l+1](m)}}
\]</div>
<p>This reads as:</p>
<ul class="simple">
<li><p>The loss gradient of the activation of node <span class="math notranslate nohighlight">\(k\)</span> in layer <span class="math notranslate nohighlight">\(l\)</span> for data sample <span class="math notranslate nohighlight">\(m\)</span> is the sum of…</p>
<ul>
<li><p>the loss gradients of the linear predictors for each node in the next layer…</p>
<ul>
<li><p>multiplied by the weights between node <span class="math notranslate nohighlight">\(k\)</span> in layer <span class="math notranslate nohighlight">\(l\)</span> and all the nodes in the next layer</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section id="linear-predictor">
<h4>Linear Predictor<a class="headerlink" href="#linear-predictor" title="Permalink to this headline">#</a></h4>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial \mathbf{Z}^{[l]}} = \frac{\partial \mathcal{L}}{\partial \mathbf{A}^{[l]}} * \frac{\partial \mathbf{A}^{[l]}}{\partial \mathbf{Z}^{[l]}}
\]</div>
<p>This follows from the chain rule (as seen in the previous notebook), and we’ve already derived/explained the two terms on the right above.</p>
</section>
<section id="bias">
<h4>Bias<a class="headerlink" href="#bias" title="Permalink to this headline">#</a></h4>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial \mathbf{b}^{[l]}} = \frac{1}{m} \sum_{\mathrm{rows}} \frac{\partial \mathcal{L}}{\partial \mathbf{Z}^{[l]}}
\]</div>
<p>In the previous notebook we derived that, for a single bias term for a single data point, <span class="math notranslate nohighlight">\(\partial \mathcal{L} / \partial b_k^{[l](m)} = \partial \mathcal{L} / \partial z_k^{[l](m)}\)</span>. The expression here computes the mean of the gradient for each bias term across the whole batch of data.</p>
</section>
<section id="weights">
<h4>Weights<a class="headerlink" href="#weights" title="Permalink to this headline">#</a></h4>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial \mathbf{W}^{[l]}} = \frac{1}{m} \frac{\partial \mathcal{L}}{\partial \mathbf{Z}^{[l]}} \mathbf{A}^{[l-1]^T}
\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\frac{\partial \mathcal{L}}{\partial \mathbf{Z}^{[l]}} = 
\begin{bmatrix}
\partial \mathcal{L}/\partial z_1^{[l](1)} &amp; \dots &amp; \partial \mathcal{L}/\partial z_1^{[l](m)} \\
\vdots &amp; \ddots &amp; \vdots \\
\partial \mathcal{L}/\partial z_{n^{[l]}}^{[l](1)} &amp; \dots &amp; \partial \mathcal{L}/\partial z_{n^{[l]}}^{[l](m)}
\end{bmatrix}
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{A}^{[l-1]^T} =
\begin{bmatrix} 
a_{1}^{[l-1](1)} &amp; \dots &amp; a_{n^{[l-1]}}^{[l-1](1)} \\
\vdots &amp; \ddots &amp; \vdots \\
a_{1}^{[l-1](m)} &amp; \dots &amp; a_{n^{[l-1]}}^{[l-1](m)}
\end{bmatrix}
\end{split}\]</div>
<p>The element <span class="math notranslate nohighlight">\((q, p)\)</span> of <span class="math notranslate nohighlight">\(\partial \mathcal{L} / \partial \mathbf{W}^{[l]}\)</span> (row <span class="math notranslate nohighlight">\(q\)</span>, column <span class="math notranslate nohighlight">\(p\)</span>) contains the gradient of the weight between node <span class="math notranslate nohighlight">\(p\)</span> in layer <span class="math notranslate nohighlight">\([l-1]\)</span> and node <span class="math notranslate nohighlight">\(q\)</span> in layer <span class="math notranslate nohighlight">\([l]\)</span> (i.e. the loss gradient of <span class="math notranslate nohighlight">\(w_{p \rightarrow q}^{[l]}\)</span>), averaged across the whole batch of data:</p>
<div class="math notranslate nohighlight">
\[
\left[ \frac{\partial \mathcal{L}}{\partial \mathbf{W}^{[l]}} \right]_{(q, p)}  = 
\frac{\partial \mathcal{L}}{\partial w_{p \rightarrow q}^{[l]}} = \frac{1}{m} \left( \frac{\partial \mathcal{L}}{\partial z_q^{[l](1)}} a_{p}^{[l-1](1)} + \dots + \frac{\partial \mathcal{L}}{\partial z_q^{[l](m)}} a_{p}^{[l-1](m)} \right)
\]</div>
<p>For why terms like <span class="math notranslate nohighlight">\((\partial \mathcal{L} / \partial z_q^{[l](1)} ) a_{p}^{[l-1](1)}\)</span> are correct for computing the gradient for the weights, you can again refer back to the single data point examples in the previous notebook.</p>
</section>
</section>
</section>
<section id="why-matrices">
<h2>Why matrices?<a class="headerlink" href="#why-matrices" title="Permalink to this headline">#</a></h2>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./notebooks"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="03_backprop.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">03: Back Propagation</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="05_relu.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">05: ReLU and Other Activation Functions</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Jack Roberts<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>