{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ecbba90",
   "metadata": {},
   "source": [
    "# 05:\n",
    "\n",
    "Non-linear regression\n",
    "\n",
    "- intuition fitting arbitrary functions\n",
    "- train/test\n",
    "- regularisation\n",
    "\n",
    "Multi-class Classification\n",
    "\n",
    "- softmax\n",
    "- multi-class\n",
    "- multi-label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c42e59",
   "metadata": {},
   "source": [
    "## Stability of the fit (overfitting)\n",
    "\n",
    "With more flexibility comes more potential for overfitting (poor performance on new data). Here's an example of fitting the two hidden layer network on an increasing number of data points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dc3083c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor n in [5, 10, 20, 30, 40, 50, 100, 250]:\\n    X, y = get_cubic_data(n)\\n    X = torch.tensor(X, dtype=torch.float)\\n    y = torch.tensor(y, dtype=torch.float)\\n    y = y[:, None]\\n\\n    model = nn.Sequential(\\n        nn.Linear(in_features=2, out_features=3),\\n        nn.Sigmoid(),\\n        nn.Linear(in_features=3, out_features=3),\\n        nn.Sigmoid(),\\n        nn.Linear(in_features=3, out_features=1),\\n        #  Sigmoid (applied in loss_fn)\\n    )\\n\\n    learning_rate = 0.1\\n    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\\n    model, loss_history = train(X, y, model, loss_fn, optimizer, 30000)\\n\\n    show_result(X, y, model, loss_history, print_weights=False, suptitle=f\"n = {n}\")\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# previously was at the end of 02 logistic regression\n",
    "\"\"\"\n",
    "for n in [5, 10, 20, 30, 40, 50, 100, 250]:\n",
    "    X, y = get_cubic_data(n)\n",
    "    X = torch.tensor(X, dtype=torch.float)\n",
    "    y = torch.tensor(y, dtype=torch.float)\n",
    "    y = y[:, None]\n",
    "\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(in_features=2, out_features=3),\n",
    "        nn.Sigmoid(),\n",
    "        nn.Linear(in_features=3, out_features=3),\n",
    "        nn.Sigmoid(),\n",
    "        nn.Linear(in_features=3, out_features=1),\n",
    "        #  Sigmoid (applied in loss_fn)\n",
    "    )\n",
    "\n",
    "    learning_rate = 0.1\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    model, loss_history = train(X, y, model, loss_fn, optimizer, 30000)\n",
    "\n",
    "    show_result(X, y, model, loss_history, print_weights=False, suptitle=f\"n = {n}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254fad21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
