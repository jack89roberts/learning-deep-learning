
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>03: Back Propagation &#8212; Learning Deep Learning</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="04: Matrix Notation" href="04_matrix_notation.html" />
    <link rel="prev" title="02: Logistic Regression" href="02_logistic_regression.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Learning Deep Learning</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Learning Deep Learning [WIP!]
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="01_linear_regression.html">
   01: Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="02_logistic_regression.html">
   02: Logistic Regression
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   03: Back Propagation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="04_matrix_notation.html">
   04: Matrix Notation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="05_relu.html">
   04: ReLU and Other Activation Functions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="06_multiclass.html">
   05:
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/jack89roberts/learning-deep-learning/main?urlpath=tree/notebooks/03_backprop.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/jack89roberts/learning-deep-learning"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/jack89roberts/learning-deep-learning/issues/new?title=Issue%20on%20page%20%2Fnotebooks/03_backprop.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/jack89roberts/learning-deep-learning/edit/main/notebooks/03_backprop.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Edit this page"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="headerbtn__text-container">suggest edit</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/notebooks/03_backprop.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#forward-pass-recap">
   Forward Pass: Recap
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gradient-descent">
   Gradient Descent
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#background-chain-rule">
   Background: Chain Rule
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#multi-variate-chain-rule">
     Multi-variate chain rule
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   Back Propagation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-1-computing-the-gradient-for-b-1-2">
     Example 1: Computing the gradient for
     <span class="math notranslate nohighlight">
      \(b_1^{[2]}\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-2-computing-the-gradient-for-w-2-1">
     Example 2: Computing the gradient for
     <span class="math notranslate nohighlight">
      \(w_2^{[1]}\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#multiple-paths">
     Multiple Paths
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#back-propagation-and-efficiency">
     Back Propagation and Efficiency
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#computation-graphs-and-auto-differentiation">
   Computation Graphs and Auto-Differentiation
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>03: Back Propagation</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#forward-pass-recap">
   Forward Pass: Recap
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gradient-descent">
   Gradient Descent
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#background-chain-rule">
   Background: Chain Rule
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#multi-variate-chain-rule">
     Multi-variate chain rule
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   Back Propagation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-1-computing-the-gradient-for-b-1-2">
     Example 1: Computing the gradient for
     <span class="math notranslate nohighlight">
      \(b_1^{[2]}\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-2-computing-the-gradient-for-w-2-1">
     Example 2: Computing the gradient for
     <span class="math notranslate nohighlight">
      \(w_2^{[1]}\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#multiple-paths">
     Multiple Paths
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#back-propagation-and-efficiency">
     Back Propagation and Efficiency
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#computation-graphs-and-auto-differentiation">
   Computation Graphs and Auto-Differentiation
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="back-propagation">
<h1>03: Back Propagation<a class="headerlink" href="#back-propagation" title="Permalink to this headline">#</a></h1>
<p><img alt="" src="../_images/03_forward_backward.png" /></p>
<p>The training loop for a neural network involves:</p>
<ol class="simple">
<li><p>A <strong>forward pass</strong>: Feed the input features (<span class="math notranslate nohighlight">\(x_1\)</span>, <span class="math notranslate nohighlight">\(x_2\)</span>) through all the layers of the network to compute our predictions, <span class="math notranslate nohighlight">\(\hat{y}\)</span>.</p></li>
<li><p>Compute the <strong>loss</strong> (or cost), <span class="math notranslate nohighlight">\(\mathcal{L}(y, \hat{y})\)</span>, a function of the predicted values <span class="math notranslate nohighlight">\(\hat{y}\)</span> and the actual values <span class="math notranslate nohighlight">\(y\)</span>.</p></li>
<li><p>A <strong>backward pass</strong> (or <em>back propagation</em>): Feed the loss <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> back through the network to compute the rate of change of the loss (i.e. the derivative) with respect to the network parameters (the weights and biases for each node, <span class="math notranslate nohighlight">\(w\)</span>, <span class="math notranslate nohighlight">\(b\)</span>)</p></li>
<li><p>Given their derivatives, update the network parameters (<span class="math notranslate nohighlight">\(w\)</span>, <span class="math notranslate nohighlight">\(b\)</span>) using an algorithm like gradient descent.</p></li>
</ol>
<section id="forward-pass-recap">
<h2>Forward Pass: Recap<a class="headerlink" href="#forward-pass-recap" title="Permalink to this headline">#</a></h2>
<p>Each node computes a linear combination of the output of all the nodes in the previous layer, for example:</p>
<div class="math notranslate nohighlight">
\[
z_1^{[1]} = w_{1 \rightarrow 1}^{[1]} x_1 + w_{2 \rightarrow 1}^{[1]} x_2 + b_1^{[1]}
\]</div>
<p>This is passed to an activation function, <span class="math notranslate nohighlight">\(g\)</span>, (assumed to be the same function in all layers here), to create the final output, or “activation”, of each node:</p>
<div class="math notranslate nohighlight">
\[
a_3^{[2]} = g(z_3^{[2]})
\]</div>
<p>For example, <span class="math notranslate nohighlight">\(\hat{y}\)</span>, can be expressed in terms of the activation of the final layer as follows:</p>
<div class="math notranslate nohighlight">
\[
\hat{y} = a_1^{[3]} = g\left(w_{1 \rightarrow 1}^{[3]} a_{1}^{[2]} + w_{2 \rightarrow 1}^{[3]} a_{2}^{[2]} + w_{3 \rightarrow 1}^{[3]} a_{3}^{[2]} + b_1^{[3]}\right)
\]</div>
<p>The terms not introduced above mean:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(w_{j \rightarrow k}^{[l]}\)</span>: The weight between node <span class="math notranslate nohighlight">\(j\)</span> in layer <span class="math notranslate nohighlight">\(l-1\)</span> and node <span class="math notranslate nohighlight">\(k\)</span> in layer <span class="math notranslate nohighlight">\(l\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(a_k^{[l]}\)</span>: The activation of node <span class="math notranslate nohighlight">\(k\)</span> in layer <span class="math notranslate nohighlight">\(l\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(b_k^{[l]}\)</span>: The bias term for node <span class="math notranslate nohighlight">\(k\)</span> in layer <span class="math notranslate nohighlight">\(l\)</span></p></li>
</ul>
</section>
<section id="gradient-descent">
<h2>Gradient Descent<a class="headerlink" href="#gradient-descent" title="Permalink to this headline">#</a></h2>
<p>Let’s consider a simpler network, with one input, two hidden nodes, and one output:</p>
<p><img alt="" src="../_images/03_backprop_example_params.png" /></p>
<p>Here I’ve also included a node after the network’s output to represent the calculation of the loss, <span class="math notranslate nohighlight">\(\mathcal{L}(y, \hat{y})\)</span>, where <span class="math notranslate nohighlight">\(\hat{y} = g(z_1^{[2]})\)</span> is the predicted value from the network and <span class="math notranslate nohighlight">\(y\)</span> the true value.</p>
<p>This network has seven parameters: <span class="math notranslate nohighlight">\(w_1^{[1]}\)</span>, <span class="math notranslate nohighlight">\(w_2^{[1]}\)</span>, <span class="math notranslate nohighlight">\(b_1^{[1]}\)</span>, <span class="math notranslate nohighlight">\(b_2^{[1]}\)</span>, <span class="math notranslate nohighlight">\(w_1^{[2]}\)</span>, <span class="math notranslate nohighlight">\(w_2^{[2]}\)</span>, <span class="math notranslate nohighlight">\(b_1^{[2]}\)</span></p>
<p>In gradient descent we use the partial derivative of the loss function with respect to the parameters to update the network, making small changes to the parameters like:</p>
<div class="math notranslate nohighlight">
\[
w_1^{[1]}  = w_1^{[1]} - \alpha\frac{\partial \mathcal{L}}{\partial w_1^{[1]}}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\alpha\)</span> is the learning rate.</p>
<p>So to perform gradient descent we need the derivatives for each parameter, i.e. we need to compute:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial w_1^{[1]}},
\frac{\partial \mathcal{L}}{\partial w_2^{[1]}},
\frac{\partial \mathcal{L}}{\partial b_1^{[1]}},
\frac{\partial \mathcal{L}}{\partial b_2^{[1]}},
\frac{\partial \mathcal{L}}{\partial w_1^{[2]}},
\frac{\partial \mathcal{L}}{\partial w_2^{[2]}},
\frac{\partial \mathcal{L}}{\partial b_1^{[2]}}
\]</div>
<p>How can we compute all those terms?</p>
</section>
<section id="background-chain-rule">
<h2>Background: Chain Rule<a class="headerlink" href="#background-chain-rule" title="Permalink to this headline">#</a></h2>
<div class="math notranslate nohighlight">
\[
h(x) = f(g(x))
\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\frac{\mathrm{d} h(x)}{\mathrm{d}x} = \frac{\mathrm{d} f(u)}{\mathrm{d}u} \frac{\mathrm{d} g(x)}{\mathrm{d}x} \\
h'(x) = f'(u)g'(x)
\end{split}\]</div>
<p>e.g.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
f(u) = u^2 \\
g(x) = e^x + x  \\
h(x) = f(g(x)) = (e^x + x)^2 \\
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
g'(x) = e^x + 1 \\
f'(u) = 2u \\
u = g(x) = e^x + x \\
h'(x) = 2(e^x + x)(e^x + 1) \\
\end{split}\]</div>
<section id="multi-variate-chain-rule">
<h3>Multi-variate chain rule<a class="headerlink" href="#multi-variate-chain-rule" title="Permalink to this headline">#</a></h3>
</section>
</section>
<section id="id1">
<h2>Back Propagation<a class="headerlink" href="#id1" title="Permalink to this headline">#</a></h2>
<p>Here’s the example network again, but with each edge (arrow) labeled by the partial derviative between the two connected nodes:</p>
<p><img alt="" src="../_images/03_backprop_example_diffs.png" /></p>
<p>To compute the derivative of the loss with respect to any term in the network we can use the chain rule. Starting with the loss on the right, we move “backwards” through the network, multiplying the partial derivatives until we get to the term we want.</p>
<section id="example-1-computing-the-gradient-for-b-1-2">
<h3>Example 1: Computing the gradient for <span class="math notranslate nohighlight">\(b_1^{[2]}\)</span><a class="headerlink" href="#example-1-computing-the-gradient-for-b-1-2" title="Permalink to this headline">#</a></h3>
<div class="math notranslate nohighlight">
\[
\frac{\color{red}{\partial \mathcal{L}}}{\color{blue}{\partial b_1^{[2]}}} = \frac{\color{red}{\partial \mathcal{L}}}{\color{green}{\partial a_1^{[2]}}} \frac{\color{green}{\partial a_1^{[2]}}}{\partial z_1^{[2]}} \frac{\partial z_1^{[2]}}{\color{blue}{\partial b_1^{[2]}}}
\]</div>
<p>Log loss for one data point (remembering that <span class="math notranslate nohighlight">\(\hat{y} = a_1^{[2]}\)</span>):</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathcal{L} = - y \log(a_1^{[2]}) - (1 - y)\log(1 - a_1^{[2]}) \\
\frac{\color{red}{\partial \mathcal{L}}}{\color{green}{\partial a_1^{[2]}}} = -\frac{y}{a_1^{[2]}} + \frac{1-y}{1-a_1^{[2]}}
\end{split}\]</div>
<p>If using a sigmoid activation function:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
a_1^{[2]} = \frac{1}{1+\exp(-z_1^{[2]})} \\
\frac{\color{green}{\partial a_1^{[2]}}}{\partial z_1^{[2]}} = a_1^{[2]} (1 - a_1^{[2]})
\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(z_1^{[2]}\)</span> is a linear combination of its inputs:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
z_1^{[2]} = w_1^{[2]}a_1^{[1]} + w_2^{[2]}a_2^{[1]} + b_1^{[2]} \\
\frac{\partial z_1^{[2]}}{\color{blue}{\partial b_1^{[2]}}} = 1
\end{split}\]</div>
<p>So overall we could write the loss derivative with respect to the bias as:</p>
<div class="math notranslate nohighlight">
\[
\frac{\color{red}{\partial \mathcal{L}}}{\color{blue}{\partial b_1^{[2]}}} = \frac{\color{red}{\partial \mathcal{L}}}{\color{green}{\partial a_1^{[2]}}} \frac{\color{green}{\partial a_1^{[2]}}}{\partial z_1^{[2]}}. 1 = \frac{\color{red}{\partial \mathcal{L}}}{\partial z_1^{[2]}}
\]</div>
</section>
<section id="example-2-computing-the-gradient-for-w-2-1">
<h3>Example 2: Computing the gradient for <span class="math notranslate nohighlight">\(w_2^{[1]}\)</span><a class="headerlink" href="#example-2-computing-the-gradient-for-w-2-1" title="Permalink to this headline">#</a></h3>
<div class="math notranslate nohighlight">
\[
\frac{\color{red}{\partial \mathcal{L}}}{\color{magenta}{\partial w_2^{[1]}}} =
\frac{\color{red}{\partial \mathcal{L}}}{\color{green}{\partial a_1^{[2]}}}
\frac{\color{green}{\partial a_1^{[2]}}}{\partial z_1^{[2]}}
\frac{\partial z_1^{[2]}}{\color{orange}{\partial a_2^{[1]}}}
\frac{\color{orange}{\partial a_2^{[1]}}}{\color{gray}{\partial z_2^{[1]}}}
\frac{\color{gray}{\partial z_2^{[1]}}}{\color{magenta}{\partial w_2^{[1]}}}
\]</div>
<p>We’ve seen the form of all the derivatives above in the first example, except for the last term:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
z_2^{[1]} = w_2^{[1]}x + b_2^{[1]} \\
\frac{\color{gray}{\partial z_2^{[1]}}}{\color{magenta}{\partial w_2^{[1]}}} = x
\end{split}\]</div>
<p>For the weights after the first layer, the inputs <span class="math notranslate nohighlight">\(x\)</span> are replaced by node activations <span class="math notranslate nohighlight">\(a\)</span>. We can relabel <span class="math notranslate nohighlight">\(x = a_1^{[0]}\)</span> to make the general trend clearer.</p>
<p>The first four terms on the right side of the expression for the derivative can be simplified to <span class="math notranslate nohighlight">\(\color{red}{\partial \mathcal{L}} / \color{gray}{\partial z_2^{[1]}}\)</span>. Then we have:</p>
<div class="math notranslate nohighlight">
\[
\frac{\color{red}{\partial \mathcal{L}}}{\color{magenta}{\partial w_2^{[1]}}} =
\frac{\color{red}{\partial \mathcal{L}}}{\color{gray}{\partial z_2^{[1]}}}
\frac{\color{gray}{\partial z_2^{[1]}}}{\color{magenta}{\partial w_2^{[1]}}}
=
\frac{\color{red}{\partial \mathcal{L}}}{\color{gray}{\partial z_2^{[1]}}}
a_1^{[0]}
\]</div>
</section>
<section id="multiple-paths">
<h3>Multiple Paths<a class="headerlink" href="#multiple-paths" title="Permalink to this headline">#</a></h3>
<p>There is one case not covered by the simplified network and examples above - where you have multiple paths from the output (loss) back to the term of interest. Such as this:</p>
<p><img alt="" src="../_images/03_backprop_multipath.png" /></p>
<p>In this case you must sum all the possible paths (this also follows from the multi-variate chain rule).</p>
</section>
<section id="back-propagation-and-efficiency">
<h3>Back Propagation and Efficiency<a class="headerlink" href="#back-propagation-and-efficiency" title="Permalink to this headline">#</a></h3>
<p>It’s important to note that:</p>
<ul class="simple">
<li><p>The derivatives in the two examples share many terms in common (e.g. the derivative of the loss with respect to the final output)</p></li>
<li><p>Each term is a fairly simple combination of quantities that must be computed during the forward pass (like the activation values in hidden layers)</p></li>
</ul>
<p>These properties of back propagation form the basis for efficient implementations in major frameworks (pytorch, Tensorflow, JAX etc.), mostly via:</p>
<ul class="simple">
<li><p>Matrix operations</p></li>
<li><p>Computation graphs</p></li>
<li><p>Caching intermediate values</p></li>
<li><p>Automatic differentiation</p></li>
</ul>
</section>
</section>
<section id="computation-graphs-and-auto-differentiation">
<h2>Computation Graphs and Auto-Differentiation<a class="headerlink" href="#computation-graphs-and-auto-differentiation" title="Permalink to this headline">#</a></h2>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./notebooks"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="02_logistic_regression.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">02: Logistic Regression</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="04_matrix_notation.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">04: Matrix Notation</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Jack Roberts<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>