
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>03: Back Propagation &#8212; Learning Deep Learning</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="04: Matrix Notation" href="04_matrix_notation.html" />
    <link rel="prev" title="02: Logistic Regression" href="02_logistic_regression.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Learning Deep Learning</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Learning Deep Learning [WIP!]
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Neural Networks
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="01_linear_regression.html">
   01: Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="02_logistic_regression.html">
   02: Logistic Regression
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   03: Back Propagation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="04_matrix_notation.html">
   04: Matrix Notation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="05_activation.html">
   05: Activation Functions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="06_nonlinear_regression.html">
   06: Non-linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="07_multiclass.html">
   07: Multiple Outputs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="08_regularisation.html">
   08 Regularisation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="09_optimisers.html">
   09: Optimisation Algorithms
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="10_experiments.html">
   10: Training and Tuning Networks
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/jack89roberts/learning-deep-learning/main?urlpath=tree/notebooks/03_backprop.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/jack89roberts/learning-deep-learning"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/jack89roberts/learning-deep-learning/issues/new?title=Issue%20on%20page%20%2Fnotebooks/03_backprop.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/jack89roberts/learning-deep-learning/edit/main/notebooks/03_backprop.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Edit this page"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="headerbtn__text-container">suggest edit</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/notebooks/03_backprop.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#forward-pass-recap">
   Forward Pass: Recap
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gradient-descent">
   Gradient Descent
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#chain-rule-for-derivatives">
   Chain Rule (for derivatives)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#case-1-f-is-a-function-of-g-and-g-is-a-function-of-x">
     Case 1:
     <span class="math notranslate nohighlight">
      \(f\)
     </span>
     is a function of
     <span class="math notranslate nohighlight">
      \(g\)
     </span>
     , and
     <span class="math notranslate nohighlight">
      \(g\)
     </span>
     is a function of
     <span class="math notranslate nohighlight">
      \(x\)
     </span>
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#example">
       Example
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#case-2-f-is-a-function-of-g-and-h-which-are-both-functions-of-x">
     Case 2:
     <span class="math notranslate nohighlight">
      \(f\)
     </span>
     is a function of
     <span class="math notranslate nohighlight">
      \(g\)
     </span>
     and
     <span class="math notranslate nohighlight">
      \(h\)
     </span>
     , which are both functions of
     <span class="math notranslate nohighlight">
      \(x\)
     </span>
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id1">
       Example
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   Back Propagation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-1-computing-the-gradient-for-b-1-2">
     Example 1: Computing the gradient for
     <span class="math notranslate nohighlight">
      \(b_1^{[2]}\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-2-computing-the-gradient-for-w-2-1">
     Example 2: Computing the gradient for
     <span class="math notranslate nohighlight">
      \(w_2^{[1]}\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#multiple-paths">
     Multiple Paths
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#back-propagation-and-efficiency">
     Back Propagation and Efficiency
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#computation-graphs-and-auto-differentiation">
   Computation Graphs and Auto-Differentiation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id3">
     Example
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#forward-pass">
       Forward pass
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#backward-pass">
       Backward pass
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pytorch">
     Pytorch
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id4">
       Forward Pass
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id5">
       Backward Pass
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>03: Back Propagation</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#forward-pass-recap">
   Forward Pass: Recap
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gradient-descent">
   Gradient Descent
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#chain-rule-for-derivatives">
   Chain Rule (for derivatives)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#case-1-f-is-a-function-of-g-and-g-is-a-function-of-x">
     Case 1:
     <span class="math notranslate nohighlight">
      \(f\)
     </span>
     is a function of
     <span class="math notranslate nohighlight">
      \(g\)
     </span>
     , and
     <span class="math notranslate nohighlight">
      \(g\)
     </span>
     is a function of
     <span class="math notranslate nohighlight">
      \(x\)
     </span>
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#example">
       Example
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#case-2-f-is-a-function-of-g-and-h-which-are-both-functions-of-x">
     Case 2:
     <span class="math notranslate nohighlight">
      \(f\)
     </span>
     is a function of
     <span class="math notranslate nohighlight">
      \(g\)
     </span>
     and
     <span class="math notranslate nohighlight">
      \(h\)
     </span>
     , which are both functions of
     <span class="math notranslate nohighlight">
      \(x\)
     </span>
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id1">
       Example
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   Back Propagation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-1-computing-the-gradient-for-b-1-2">
     Example 1: Computing the gradient for
     <span class="math notranslate nohighlight">
      \(b_1^{[2]}\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-2-computing-the-gradient-for-w-2-1">
     Example 2: Computing the gradient for
     <span class="math notranslate nohighlight">
      \(w_2^{[1]}\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#multiple-paths">
     Multiple Paths
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#back-propagation-and-efficiency">
     Back Propagation and Efficiency
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#computation-graphs-and-auto-differentiation">
   Computation Graphs and Auto-Differentiation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id3">
     Example
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#forward-pass">
       Forward pass
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#backward-pass">
       Backward pass
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pytorch">
     Pytorch
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id4">
       Forward Pass
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id5">
       Backward Pass
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="back-propagation">
<h1>03: Back Propagation<a class="headerlink" href="#back-propagation" title="Permalink to this headline">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
</pre></div>
</div>
</div>
</div>
<p><img alt="" src="../_images/03_forward_backward.png" /></p>
<p>The training loop for a neural network involves:</p>
<ol class="simple">
<li><p>A <strong>forward pass</strong>: Feed the input features (<span class="math notranslate nohighlight">\(x_1\)</span>, <span class="math notranslate nohighlight">\(x_2\)</span>) through all the layers of the network to compute our predictions, <span class="math notranslate nohighlight">\(\hat{y}\)</span>.</p></li>
<li><p>Compute the <strong>loss</strong> (or cost), <span class="math notranslate nohighlight">\(\mathcal{L}(y, \hat{y})\)</span>, a function of the predicted values <span class="math notranslate nohighlight">\(\hat{y}\)</span> and the actual values <span class="math notranslate nohighlight">\(y\)</span>.</p></li>
<li><p>A <strong>backward pass</strong> (or <em>back propagation</em>): Feed the loss <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> back through the network to compute the rate of change of the loss (i.e. the derivative) with respect to the network parameters (the weights and biases for each node, <span class="math notranslate nohighlight">\(w\)</span>, <span class="math notranslate nohighlight">\(b\)</span>)</p></li>
<li><p>Given their derivatives, update the network parameters (<span class="math notranslate nohighlight">\(w\)</span>, <span class="math notranslate nohighlight">\(b\)</span>) using an algorithm like gradient descent.</p></li>
</ol>
<section id="forward-pass-recap">
<h2>Forward Pass: Recap<a class="headerlink" href="#forward-pass-recap" title="Permalink to this headline">#</a></h2>
<p>Each node computes a linear combination of the output of all the nodes in the previous layer, for example:</p>
<div class="math notranslate nohighlight">
\[
z_1^{[1]} = w_{1 \rightarrow 1}^{[1]} x_1 + w_{2 \rightarrow 1}^{[1]} x_2 + b_1^{[1]}
\]</div>
<p>This is passed to an activation function, <span class="math notranslate nohighlight">\(g\)</span>, (assumed to be the same function in all layers here), to create the final output, or “activation”, of each node:</p>
<div class="math notranslate nohighlight">
\[
a_3^{[2]} = g(z_3^{[2]})
\]</div>
<p>For example, <span class="math notranslate nohighlight">\(\hat{y}\)</span>, can be expressed in terms of the activation of the final layer as follows:</p>
<div class="math notranslate nohighlight">
\[
\hat{y} = a_1^{[3]} = g\left(w_{1 \rightarrow 1}^{[3]} a_{1}^{[2]} + w_{2 \rightarrow 1}^{[3]} a_{2}^{[2]} + w_{3 \rightarrow 1}^{[3]} a_{3}^{[2]} + b_1^{[3]}\right)
\]</div>
<p>The terms not introduced above mean:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(w_{j \rightarrow k}^{[l]}\)</span>: The weight between node <span class="math notranslate nohighlight">\(j\)</span> in layer <span class="math notranslate nohighlight">\(l-1\)</span> and node <span class="math notranslate nohighlight">\(k\)</span> in layer <span class="math notranslate nohighlight">\(l\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(a_k^{[l]}\)</span>: The activation of node <span class="math notranslate nohighlight">\(k\)</span> in layer <span class="math notranslate nohighlight">\(l\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(b_k^{[l]}\)</span>: The bias term for node <span class="math notranslate nohighlight">\(k\)</span> in layer <span class="math notranslate nohighlight">\(l\)</span></p></li>
</ul>
</section>
<section id="gradient-descent">
<h2>Gradient Descent<a class="headerlink" href="#gradient-descent" title="Permalink to this headline">#</a></h2>
<p>Let’s consider a simpler network, with one input, two hidden nodes, and one output:</p>
<p><img alt="" src="../_images/03_backprop_example_params.png" /></p>
<p>Here I’ve also included a node after the network’s output to represent the calculation of the loss, <span class="math notranslate nohighlight">\(\mathcal{L}(y, \hat{y})\)</span>, where <span class="math notranslate nohighlight">\(\hat{y} = g(z_1^{[2]})\)</span> is the predicted value from the network and <span class="math notranslate nohighlight">\(y\)</span> the true value.</p>
<p>This network has seven parameters: <span class="math notranslate nohighlight">\(w_1^{[1]}\)</span>, <span class="math notranslate nohighlight">\(w_2^{[1]}\)</span>, <span class="math notranslate nohighlight">\(b_1^{[1]}\)</span>, <span class="math notranslate nohighlight">\(b_2^{[1]}\)</span>, <span class="math notranslate nohighlight">\(w_1^{[2]}\)</span>, <span class="math notranslate nohighlight">\(w_2^{[2]}\)</span>, <span class="math notranslate nohighlight">\(b_1^{[2]}\)</span></p>
<p>In gradient descent we use the partial derivative of the loss function with respect to the parameters to update the network, making small changes to the parameters like:</p>
<div class="math notranslate nohighlight">
\[
w_1^{[1]}  = w_1^{[1]} - \alpha\frac{\partial \mathcal{L}}{\partial w_1^{[1]}}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\alpha\)</span> is the learning rate.</p>
<p>So to perform gradient descent we need the derivatives for each parameter, i.e. we need to compute:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial w_1^{[1]}},
\frac{\partial \mathcal{L}}{\partial w_2^{[1]}},
\frac{\partial \mathcal{L}}{\partial b_1^{[1]}},
\frac{\partial \mathcal{L}}{\partial b_2^{[1]}},
\frac{\partial \mathcal{L}}{\partial w_1^{[2]}},
\frac{\partial \mathcal{L}}{\partial w_2^{[2]}},
\frac{\partial \mathcal{L}}{\partial b_1^{[2]}}
\]</div>
<p>How can we compute all those terms?</p>
</section>
<section id="chain-rule-for-derivatives">
<h2>Chain Rule (for derivatives)<a class="headerlink" href="#chain-rule-for-derivatives" title="Permalink to this headline">#</a></h2>
<section id="case-1-f-is-a-function-of-g-and-g-is-a-function-of-x">
<h3>Case 1: <span class="math notranslate nohighlight">\(f\)</span> is a function of <span class="math notranslate nohighlight">\(g\)</span>, and <span class="math notranslate nohighlight">\(g\)</span> is a function of <span class="math notranslate nohighlight">\(x\)</span><a class="headerlink" href="#case-1-f-is-a-function-of-g-and-g-is-a-function-of-x" title="Permalink to this headline">#</a></h3>
<div class="math notranslate nohighlight">
\[
f = f(g(x))
\]</div>
<p>The chain rule states that the derivative of <span class="math notranslate nohighlight">\(w\)</span> with respect to <span class="math notranslate nohighlight">\(x\)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[
\frac{\mathrm{d} f}{\mathrm{d}x} = \frac{\mathrm{d} f}{\mathrm{d} g} \frac{\mathrm{d} g}{\mathrm{d} x}
\]</div>
<section id="example">
<h4>Example<a class="headerlink" href="#example" title="Permalink to this headline">#</a></h4>
<p>Find the derivative of <span class="math notranslate nohighlight">\(f(x) = (e^x + x)^2\)</span>:</p>
<p>Let:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
g(x) = e^x + x \\
f(g) = g^2 \\
\end{split}\]</div>
<p>Then by chain rule:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\frac{\mathrm{d} f}{\mathrm{d} g} = 2g \\
\frac{\mathrm{d} g}{\mathrm{d} x} = e^x + 1 \\
\frac{\mathrm{d} f}{\mathrm{d}x} = \frac{\mathrm{d} f}{\mathrm{d} g} \frac{\mathrm{d} g}{\mathrm{d} x} = 2g(e^x + 1) \\
\frac{\mathrm{d} f}{\mathrm{d}x} = 2(e^x + x)(e^x + 1) 
\end{split}\]</div>
</section>
</section>
<section id="case-2-f-is-a-function-of-g-and-h-which-are-both-functions-of-x">
<h3>Case 2: <span class="math notranslate nohighlight">\(f\)</span> is a function of <span class="math notranslate nohighlight">\(g\)</span> and <span class="math notranslate nohighlight">\(h\)</span>, which are both functions of <span class="math notranslate nohighlight">\(x\)</span><a class="headerlink" href="#case-2-f-is-a-function-of-g-and-h-which-are-both-functions-of-x" title="Permalink to this headline">#</a></h3>
<div class="math notranslate nohighlight">
\[
f = f(g(x), h(x))
\]</div>
<p>To find the derivative of <span class="math notranslate nohighlight">\(f\)</span> with respect to <span class="math notranslate nohighlight">\(x\)</span>, the chain rule states that you must sum over its (partial) derivatives for each input (<span class="math notranslate nohighlight">\(g\)</span>, <span class="math notranslate nohighlight">\(h\)</span>):</p>
<div class="math notranslate nohighlight">
\[
\frac{\mathrm{d} f}{\mathrm{d}x} = \frac{\partial f}{\partial g} \frac{\mathrm{d} g}{\mathrm{d} x} + \frac{\partial f}{\partial h} \frac{\mathrm{d} h}{\mathrm{d} x}
\]</div>
<p>This is the <em>multi-variable</em> chain rule.</p>
<section id="id1">
<h4>Example<a class="headerlink" href="#id1" title="Permalink to this headline">#</a></h4>
<p>Find the derivative of:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
f(x) = x^2(3x+1) - \sin(x^2) \\
\end{split}\]</div>
<p>which can be written as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
f(g, h) = h g - \sin(h) \\
g(x) = 3x + 1 \\
h(x) = x^2
\end{split}\]</div>
<p>Then by chain rule:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\frac{\partial f}{\partial g} = h\\
\frac{\mathrm{d} g}{\mathrm{d} x} = 3 \\
\frac{\partial f}{\partial h} = g - \cos(h) \\
\frac{\mathrm{d} h}{\mathrm{d} x} = 2x \\
\frac{\mathrm{d} f}{\mathrm{d}x} = \frac{\partial f}{\partial g} \frac{\mathrm{d} g}{\mathrm{d} x} + \frac{\partial f}{\partial h} \frac{\mathrm{d} h}{\mathrm{d} x} =
3h + 2x\left(g-\cos(h)\right) \\
\frac{\mathrm{d} f}{\mathrm{d}x} = 3x^2 + 2x\left(3x + 1 -\cos(x^2)\right)
\end{split}\]</div>
</section>
</section>
</section>
<section id="id2">
<h2>Back Propagation<a class="headerlink" href="#id2" title="Permalink to this headline">#</a></h2>
<p>Here’s the example network again, but with each edge (arrow) labeled by the partial derviative between the two connected nodes:</p>
<p><img alt="" src="../_images/03_backprop_example_diffs.png" /></p>
<p>To compute the derivative of the loss with respect to any term in the network we can use the chain rule. Starting with the loss on the right, we move “backwards” through the network, multiplying the partial derivatives until we get to the term we want.</p>
<section id="example-1-computing-the-gradient-for-b-1-2">
<h3>Example 1: Computing the gradient for <span class="math notranslate nohighlight">\(b_1^{[2]}\)</span><a class="headerlink" href="#example-1-computing-the-gradient-for-b-1-2" title="Permalink to this headline">#</a></h3>
<p>Why the chain rule/why multiply backwards along arrows?</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathcal{L}\)</span> is a function of <span class="math notranslate nohighlight">\(a_1^{[2]}\)</span>,  <span class="math notranslate nohighlight">\(a_1^{[2]}\)</span> is a function of <span class="math notranslate nohighlight">\(z_1^{[2]}\)</span>, and <span class="math notranslate nohighlight">\(z_1^{[2]}\)</span> is a function of <span class="math notranslate nohighlight">\(b_1^{[2]}\)</span></p></li>
<li><p>We could write that like <span class="math notranslate nohighlight">\(\mathcal{L}\left(a_1^{[2]}\left(z_1^{[2]}\left(b_1^{[2]}\right)\right)\right)\)</span>, which has the form of “case 1” of the chain rule above except with 3 functions instead of 2, i.e. a composed function like <span class="math notranslate nohighlight">\(f\left(g\left(h\left(x\right)\right)\right)\)</span></p></li>
</ul>
<p>So, applying the chain rule and multiplying derivatives backwards along the diagram:</p>
<div class="math notranslate nohighlight">
\[
\frac{\color{red}{\partial \mathcal{L}}}{\color{blue}{\partial b_1^{[2]}}} = \frac{\partial z_1^{[2]}}{\color{blue}{\partial b_1^{[2]}}} \frac{\color{green}{\partial a_1^{[2]}}}{\partial z_1^{[2]}} \frac{\color{red}{\partial \mathcal{L}}}{\color{green}{\partial a_1^{[2]}}}
\]</div>
<p>Log loss for one data point (remembering that <span class="math notranslate nohighlight">\(\hat{y} = a_1^{[2]}\)</span>):</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathcal{L} = - y \log(a_1^{[2]}) - (1 - y)\log(1 - a_1^{[2]}) \\
\frac{\color{red}{\partial \mathcal{L}}}{\color{green}{\partial a_1^{[2]}}} = -\frac{y}{a_1^{[2]}} + \frac{1-y}{1-a_1^{[2]}}
\end{split}\]</div>
<p>If using a sigmoid activation function:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
a_1^{[2]} = \frac{1}{1+\exp(-z_1^{[2]})} \\
\frac{\color{green}{\partial a_1^{[2]}}}{\partial z_1^{[2]}} = a_1^{[2]} (1 - a_1^{[2]})
\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(z_1^{[2]}\)</span> is a linear combination of its inputs:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
z_1^{[2]} = w_1^{[2]}a_1^{[1]} + w_2^{[2]}a_2^{[1]} + b_1^{[2]} \\
\frac{\partial z_1^{[2]}}{\color{blue}{\partial b_1^{[2]}}} = 1
\end{split}\]</div>
<p>So overall we could write the loss derivative with respect to the bias as:</p>
<div class="math notranslate nohighlight">
\[
\frac{\color{red}{\partial \mathcal{L}}}{\color{blue}{\partial b_1^{[2]}}} =
1 . \frac{\color{green}{\partial a_1^{[2]}}}{\partial z_1^{[2]}}
\frac{\color{red}{\partial \mathcal{L}}}{\color{green}{\partial a_1^{[2]}}} =
\frac{\color{red}{\partial \mathcal{L}}}{\partial z_1^{[2]}}
\]</div>
<p>Substituting in the earlier results above and simplifying gives:</p>
<div class="math notranslate nohighlight">
\[
\frac{\color{red}{\partial \mathcal{L}}}{\color{blue}{\partial b_1^{[2]}}} =
\frac{\color{red}{\partial \mathcal{L}}}{\partial z_1^{[2]}} =
a_1^{[2]} - y
\]</div>
</section>
<section id="example-2-computing-the-gradient-for-w-2-1">
<h3>Example 2: Computing the gradient for <span class="math notranslate nohighlight">\(w_2^{[1]}\)</span><a class="headerlink" href="#example-2-computing-the-gradient-for-w-2-1" title="Permalink to this headline">#</a></h3>
<div class="math notranslate nohighlight">
\[
\frac{\color{red}{\partial \mathcal{L}}}{\color{magenta}{\partial w_2^{[1]}}} =
\frac{\color{gray}{\partial z_2^{[1]}}}{\color{magenta}{\partial w_2^{[1]}}}
\frac{\color{orange}{\partial a_2^{[1]}}}{\color{gray}{\partial z_2^{[1]}}}
\frac{\partial z_1^{[2]}}{\color{orange}{\partial a_2^{[1]}}}
\frac{\color{green}{\partial a_1^{[2]}}}{\partial z_1^{[2]}}
\frac{\color{red}{\partial \mathcal{L}}}{\color{green}{\partial a_1^{[2]}}}
\]</div>
<p>We’ve seen the form of all the derivatives above in the first example, except for the first and third terms:</p>
<p><strong>First term:</strong></p>
<div class="math notranslate nohighlight">
\[\begin{split}
z_2^{[1]} = w_2^{[1]}x + b_2^{[1]} \\
\frac{\color{gray}{\partial z_2^{[1]}}}{\color{magenta}{\partial w_2^{[1]}}} = x = a_1^{[0]}
\end{split}\]</div>
<p>For the weights after the first layer, the inputs <span class="math notranslate nohighlight">\(x\)</span> are replaced by node activations <span class="math notranslate nohighlight">\(a\)</span>. We can relabel <span class="math notranslate nohighlight">\(x = a_1^{[0]}\)</span> to make the general trend clearer.</p>
<p><strong>Third term:</strong></p>
<div class="math notranslate nohighlight">
\[\begin{split}
z_1^{[2]} = w_1^{[2]} a_1^{[1]} + w_2^{[2]} a_2^{[1]} + b_1^{[2]} \\
\frac{\partial z_1^{[2]}}{\color{orange}{\partial a_2^{[1]}}} = w_2^{[2]}
\end{split}\]</div>
<p><strong>Overall:</strong></p>
<p>The last four terms on the right side of the expression for the derivative can be simplified to <span class="math notranslate nohighlight">\(\color{red}{\partial \mathcal{L}} / \color{gray}{\partial z_2^{[1]}}\)</span>. Then we have:</p>
<div class="math notranslate nohighlight">
\[
\frac{\color{red}{\partial \mathcal{L}}}{\color{magenta}{\partial w_2^{[1]}}} =
\frac{\color{gray}{\partial z_2^{[1]}}}{\color{magenta}{\partial w_2^{[1]}}}
\frac{\color{red}{\partial \mathcal{L}}}{\color{gray}{\partial z_2^{[1]}}}
=
a_1^{[0]}\frac{\color{red}{\partial \mathcal{L}}}{\color{gray}{\partial z_2^{[1]}}}
\]</div>
<p>We can also compute that <span class="math notranslate nohighlight">\(\frac{\color{gray}{\partial z_2^{[1]}}}{\color{magenta}{\partial b_2^{[1]}}} = 1\)</span> (see example 1), so it follows that <span class="math notranslate nohighlight">\(\frac{\color{red}{\partial \mathcal{L}}}{\color{magenta}{\partial b_2^{[1]}}} =\frac{\color{red}{\partial \mathcal{L}}}{\color{gray}{\partial z_2^{[1]}}}\)</span>.</p>
<p>From the previous results (here and in example 1) we can derive <span class="math notranslate nohighlight">\(\color{red}{\partial \mathcal{L}} / \color{gray}{\partial z_2^{[1]}}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\frac{\color{red}{\partial \mathcal{L}}}{\color{gray}{\partial z_2^{[1]}}} =
\frac{\color{orange}{\partial a_2^{[1]}}}{\color{gray}{\partial z_2^{[1]}}}
\frac{\partial z_1^{[2]}}{\color{orange}{\partial a_2^{[1]}}}
\frac{\color{green}{\partial a_1^{[2]}}}{\partial z_1^{[2]}}
\frac{\color{red}{\partial \mathcal{L}}}{\color{green}{\partial a_1^{[2]}}} =
\frac{\color{orange}{\partial a_2^{[1]}}}{\color{gray}{\partial z_2^{[1]}}}
\frac{\partial z_1^{[2]}}{\color{orange}{\partial a_2^{[1]}}}
\frac{\color{red}{\partial \mathcal{L}}}{\partial z_1^{[2]}} \\
\frac{\color{red}{\partial \mathcal{L}}}{\color{gray}{\partial z_2^{[1]}}} =
a_2^{[1]} (1 - a_2^{[1]}) w_2^{[2]}\frac{\color{red}{\partial \mathcal{L}}}{\partial z_1^{[2]}}
\end{split}\]</div>
<p>Note we already derived the value of the last term, <span class="math notranslate nohighlight">\(\color{red}{\partial \mathcal{L}} / \partial z_1^{[2]}\)</span>, in Example 1, so we don’t need to re-compute it here.</p>
</section>
<section id="multiple-paths">
<h3>Multiple Paths<a class="headerlink" href="#multiple-paths" title="Permalink to this headline">#</a></h3>
<p>There is one case not covered by the simplified network and examples above - where you have multiple paths from the output (loss) back to the term of interest. Such as this:</p>
<p><img alt="" src="../_images/03_backprop_multipath.png" /></p>
<p>In this case you must sum all the possible paths.</p>
<p>Why?</p>
<p>This also follows from the multi-variable chain rule (case 2 from earlier):</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathcal{L}\)</span> is a function of <span class="math notranslate nohighlight">\(a_1^{[3]}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(a_1^{[3]}\)</span> is a function of <span class="math notranslate nohighlight">\(z_1^{[3]}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(z_1^{[3]}\)</span> is a function of <span class="math notranslate nohighlight">\(a_1^{[2]}\)</span> <em>and</em> <span class="math notranslate nohighlight">\(a_2^{[2]}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(a_1^{[2]}\)</span> <em>and</em> <span class="math notranslate nohighlight">\(a_2^{[2]}\)</span> are functions of <span class="math notranslate nohighlight">\(b_1^{[1]}\)</span> (through <span class="math notranslate nohighlight">\(z_1^{[2]}\)</span> and <span class="math notranslate nohighlight">\(z_2^{[2]}\)</span>)</p></li>
</ul>
<p>This could be written:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}\left(a_1^{[3]}\left(z_1^{[3]}\left(a_1^{[2]}\left(b_1^{[1]}\right), a_2^{[2]}\left(b_1^{[1]}\right)\right)\right)\right)
\]</div>
<p>which has the form:</p>
<div class="math notranslate nohighlight">
\[
f\left(g\left(x\right), h\left(x\right)\right)
\]</div>
<p>where:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
f(g, h) = \mathcal{L}\left(a_1^{[3]}\left(z_1^{[3]}\left(g, h\right)\right)\right) \\
g(x) = a_1^{[2]}(x) \\
h(x) = a_2^{[2]}(x) \\
x = b_1^{[1]}
\end{split}\]</div>
<p>(here <span class="math notranslate nohighlight">\(x\)</span> is just an arbitrary label for a variable, <em>not</em> the input data)</p>
</section>
<section id="back-propagation-and-efficiency">
<h3>Back Propagation and Efficiency<a class="headerlink" href="#back-propagation-and-efficiency" title="Permalink to this headline">#</a></h3>
<p>It’s important to note that:</p>
<ul class="simple">
<li><p>The derivatives in layer <span class="math notranslate nohighlight">\(l\)</span> depend on the derivatives in layer <span class="math notranslate nohighlight">\(l + 1\)</span> (so we pass gradients “backwards” through the network)</p></li>
<li><p>Each term is a fairly simple combination of quantities that must be computed during the forward pass (like the activation values in hidden layers)</p></li>
</ul>
<p>These properties of back propagation form the basis for efficient implementations in major frameworks (pytorch, Tensorflow, JAX etc.), mostly via:</p>
<ul class="simple">
<li><p>Matrix operations</p></li>
<li><p>Computation graphs</p></li>
<li><p>Caching intermediate values</p></li>
<li><p>Automatic differentiation</p></li>
</ul>
</section>
</section>
<section id="computation-graphs-and-auto-differentiation">
<h2>Computation Graphs and Auto-Differentiation<a class="headerlink" href="#computation-graphs-and-auto-differentiation" title="Permalink to this headline">#</a></h2>
<p>In the background, large frameworks like pytorch use computation graphs and “auto-differentiation” to be able to compute gradients efficiently.</p>
<section id="id3">
<h3>Example<a class="headerlink" href="#id3" title="Permalink to this headline">#</a></h3>
<p>Here’s an example of a simple logistic regression (one layer) network in the form of a <em>computation graph</em>:</p>
<a class="reference internal image-reference" href="../_images/03_computation_graph.png"><img alt="Computation graph" src="../_images/03_computation_graph.png" style="width: 500px;" /></a>
<ul class="simple">
<li><p>Each node represents either an input variable/parameter (white/clear background), or an operation that applies to one or more of the previously defined values. In this graph the operations are summation (<span class="math notranslate nohighlight">\(+\)</span>), multiplication (<span class="math notranslate nohighlight">\(*\)</span>), sigmoid (<span class="math notranslate nohighlight">\(g\)</span>), and log loss (<span class="math notranslate nohighlight">\(\mathcal{L}\)</span>).</p></li>
<li><p>The values of all the nodes on a row must be known before the next row can be calculated.</p></li>
</ul>
<p>In the computation graph we’ll store:</p>
<ul class="simple">
<li><p>The relationships between all the nodes (how they are connected and the operations that are performed on them)</p></li>
<li><p>The value of each node for a given input</p></li>
<li><p>The gradient of each node for a given input, with respect to the final node</p></li>
</ul>
<p>Having all the node values and the relationships between the nodes let’s us compute the gradient at each node efficiently:</p>
<section id="forward-pass">
<h4>Forward pass<a class="headerlink" href="#forward-pass" title="Permalink to this headline">#</a></h4>
<p>When doing a forward (top to bottom) pass through the network we store the values computed at all nodes (i.e. including the intermediate values on each row):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Forward pass</span>

<span class="c1"># =================</span>
<span class="c1"># Row 1 in diagram</span>
<span class="c1"># =================</span>
<span class="n">x1</span> <span class="o">=</span> <span class="mf">1.5</span>
<span class="n">x2</span> <span class="o">=</span> <span class="mf">2.5</span>

<span class="n">w1</span> <span class="o">=</span> <span class="o">-</span><span class="mi">4</span>
<span class="n">w2</span> <span class="o">=</span> <span class="mi">3</span>

<span class="c1"># =================</span>
<span class="c1"># Row 2 in diagram</span>
<span class="c1"># =================</span>
<span class="n">w1x1</span> <span class="o">=</span> <span class="n">w1</span> <span class="o">*</span> <span class="n">x1</span>
<span class="n">w2x2</span> <span class="o">=</span> <span class="n">w2</span> <span class="o">*</span> <span class="n">x2</span>

<span class="n">b</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>

<span class="c1"># =================</span>
<span class="c1"># Row 3 in diagram</span>
<span class="c1"># =================</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">w1x1</span> <span class="o">+</span> <span class="n">w2x2</span> <span class="o">+</span> <span class="n">b</span>

<span class="c1"># =================</span>
<span class="c1"># Row 4 in diagram</span>
<span class="c1"># =================</span>
<span class="n">yhat</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span>  <span class="c1"># sigmoid</span>

<span class="n">y</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1"># =================</span>
<span class="c1"># Row 5 in diagram</span>
<span class="c1"># =================</span>
<span class="n">L</span> <span class="o">=</span> <span class="o">-</span><span class="n">y</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">yhat</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">yhat</span><span class="p">)</span>  <span class="c1"># log loss</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;yhat = </span><span class="si">{</span><span class="n">yhat</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;L = </span><span class="si">{</span><span class="n">L</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>yhat = 0.6225
L = 0.4741
</pre></div>
</div>
</div>
</div>
</section>
<section id="backward-pass">
<h4>Backward pass<a class="headerlink" href="#backward-pass" title="Permalink to this headline">#</a></h4>
<p>Now we can use the node values from the forward pass, our knowledge of the computation graph, and the chain rule to compute the gradients:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Backward pass</span>

<span class="c1"># =================</span>
<span class="c1"># Row 5 in diagram</span>
<span class="c1"># =================</span>
<span class="n">dL_dyhat</span> <span class="o">=</span> <span class="o">-</span><span class="n">y</span> <span class="o">/</span> <span class="n">yhat</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">yhat</span><span class="p">)</span>  <span class="c1"># derivative of log loss</span>

<span class="c1"># =================</span>
<span class="c1"># Row 4 in diagram</span>
<span class="c1"># =================</span>
<span class="n">dyhat_dz</span> <span class="o">=</span> <span class="n">yhat</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">yhat</span><span class="p">)</span>  <span class="c1"># derivative of sigmoid</span>

<span class="n">dL_dz</span> <span class="o">=</span> <span class="n">dyhat_dz</span> <span class="o">*</span> <span class="n">dL_dyhat</span>

<span class="c1"># =================</span>
<span class="c1"># Row 3 in diagram</span>
<span class="c1"># =================</span>
<span class="n">dz_dw1x1</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># summation nodes pass the same gradient backwards</span>
<span class="n">dz_dw2x2</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># e.g. z = w1x1 + w2x2 + b, dz/d(w2x2) = 1</span>
<span class="n">dz_db</span> <span class="o">=</span> <span class="mi">1</span>

<span class="n">dL_dw1x1</span> <span class="o">=</span> <span class="n">dz_dw1x1</span> <span class="o">*</span> <span class="n">dL_dz</span>
<span class="n">dL_dw2x2</span> <span class="o">=</span> <span class="n">dz_dw2x2</span> <span class="o">*</span> <span class="n">dL_dz</span>
<span class="n">dL_db</span> <span class="o">=</span> <span class="n">dz_db</span> <span class="o">*</span> <span class="n">dL_dz</span>

<span class="c1"># =================</span>
<span class="c1"># Row 2 in diagram</span>
<span class="c1"># =================</span>
<span class="n">dw1x1_dw1</span> <span class="o">=</span> <span class="n">x1</span>  <span class="c1"># multiplication node gradients take the value of the other input</span>
<span class="n">dw2x2_dw2</span> <span class="o">=</span> <span class="n">x2</span>  <span class="c1"># e.g. d(w2x2) / d(w2) = x2</span>

<span class="n">dL_dw1</span> <span class="o">=</span> <span class="n">dw1x1_dw1</span> <span class="o">*</span> <span class="n">dL_dw1x1</span>
<span class="n">dL_dw2</span> <span class="o">=</span> <span class="n">dw2x2_dw2</span> <span class="o">*</span> <span class="n">dL_dw2x2</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;dL_dw1 = </span><span class="si">{</span><span class="n">dL_dw1</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;dL_dw2 = </span><span class="si">{</span><span class="n">dL_dw2</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;dL_db = </span><span class="si">{</span><span class="n">dL_db</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>dL_dw1 = -0.5663
dL_dw2 = -0.9439
dL_db = -0.3775
</pre></div>
</div>
</div>
</div>
<p>This is an extremely verbose way of representing this, we’ll see matrix notation in the next notebook that will let us represent networks in a much more concise way.</p>
</section>
</section>
<section id="pytorch">
<h3>Pytorch<a class="headerlink" href="#pytorch" title="Permalink to this headline">#</a></h3>
<p>We can manually do the same operations in pytorch and with pytorch’s tensor class, being careful to:</p>
<ul class="simple">
<li><p>Set <code class="docutils literal notranslate"><span class="pre">requires_grad=True</span></code> for the parameters we’re interested in the gradients of (<code class="docutils literal notranslate"><span class="pre">w1</span></code>, <code class="docutils literal notranslate"><span class="pre">w2</span></code>, and <code class="docutils literal notranslate"><span class="pre">b</span></code>) - we’ll come back to this later</p></li>
<li><p>Use torch’s implementations of sigmoid (<code class="docutils literal notranslate"><span class="pre">torch.sigmoid</span></code>) and log loss (<code class="docutils literal notranslate"><span class="pre">torch.nn.functional.binary_cross_entropy</span></code>).</p></li>
</ul>
<section id="id4">
<h4>Forward Pass<a class="headerlink" href="#id4" title="Permalink to this headline">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Forward pass</span>

<span class="c1"># =================</span>
<span class="c1"># Row 1 in diagram</span>
<span class="c1"># =================</span>
<span class="n">x1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.5</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">2.5</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">w1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">4.0</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">w2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">3.0</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># =================</span>
<span class="c1"># Row 2 in diagram</span>
<span class="c1"># =================</span>
<span class="n">w1x1</span> <span class="o">=</span> <span class="n">w1</span> <span class="o">*</span> <span class="n">x1</span>
<span class="n">w2x2</span> <span class="o">=</span> <span class="n">w2</span> <span class="o">*</span> <span class="n">x2</span>

<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">1.0</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># =================</span>
<span class="c1"># Row 3 in diagram</span>
<span class="c1"># =================</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">w1x1</span> <span class="o">+</span> <span class="n">w2x2</span> <span class="o">+</span> <span class="n">b</span>

<span class="c1"># =================</span>
<span class="c1"># Row 4 in diagram</span>
<span class="c1"># =================</span>
<span class="n">yhat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># =================</span>
<span class="c1"># Row 5 in diagram</span>
<span class="c1"># =================</span>
<span class="n">L</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">binary_cross_entropy</span><span class="p">(</span><span class="n">yhat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;yhat = </span><span class="si">{</span><span class="n">yhat</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;L = </span><span class="si">{</span><span class="n">L</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>yhat = tensor([0.6225], grad_fn=&lt;SigmoidBackward0&gt;)
L = 0.4741
</pre></div>
</div>
</div>
</div>
<p>Note the values are the same as our own version above.</p>
</section>
<section id="id5">
<h4>Backward Pass<a class="headerlink" href="#id5" title="Permalink to this headline">#</a></h4>
<p>Now for the magic - in the background pytorch has built a computation graph from the variables we’ve defined and can compute the gradients (do a backward pass) for us automatically (for the parameters where we’ve specified <code class="docutils literal notranslate"><span class="pre">requires_grad=True</span></code>). You’ll notice that the <code class="docutils literal notranslate"><span class="pre">yhat</span></code> tensor above contains both its value and a function for computing gradients with respect to it (<code class="docutils literal notranslate"><span class="pre">&lt;SigmoidBackward0&gt;</span></code>).</p>
<p>To do the backward pass and compute the gradients we just need to do:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">L</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;dL_dw1 = </span><span class="si">{</span><span class="n">w1</span><span class="o">.</span><span class="n">grad</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;dL_dw2 = </span><span class="si">{</span><span class="n">w2</span><span class="o">.</span><span class="n">grad</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;dL_db = </span><span class="si">{</span><span class="n">b</span><span class="o">.</span><span class="n">grad</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>dL_dw1 = tensor([-0.5663])
dL_dw2 = tensor([-0.9439])
dL_db = tensor([-0.3775])
</pre></div>
</div>
</div>
</div>
<p>Again, these match the gradients in our own version, but with a lot fewer lines of code (for us) 🎉</p>
<p>You might remember seeing <code class="docutils literal notranslate"><span class="pre">.backward()</span></code> before in the linear/logistic regression notebooks - hopefully this gives you the intuition for what it’s doing!</p>
</section>
</section>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://www.coursera.org/learn/neural-networks-deep-learning/home/info"><em>“Neural Networks and Deep Learning”</em>, DeepLearning.AI, Andrew Ng (Coursera)</a></p></li>
<li><p><a class="reference external" href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi"><em>“Neural Networks”</em>, 3Blue1Brown (YouTube)</a></p></li>
<li><p><a class="reference external" href="https://www.youtube.com/watch?v=VMj-3S1tku0"><em>“The spelled-out intro to neural networks and backpropagation: building micrograd”</em>, Andrej Karpathy (YouTube)</a></p></li>
<li><p><a class="reference external" href="https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html"><em>“A Gentle Introduction to torch.autograd”</em> (PyTorch)</a></p></li>
<li><p><a class="reference external" href="http://colah.github.io/posts/2015-08-Backprop/"><em>“Calculus on Computational Graphs: Backpropagation”</em>, Christopher Olah</a></p></li>
<li><p><a class="reference external" href="https://simple-english-machine-learning.readthedocs.io/en/latest/neural-networks/computational-graphs.html"><em>“Computational graphs and gradient flows”</em>, Simple English Machine Learning</a></p></li>
<li><p><a class="reference external" href="https://math.hmc.edu/calculus/hmc-mathematics-calculus-online-tutorials/multivariable-calculus/multi-variable-chain-rule/"><em>“Multi-variable Chain Rule”</em>, Harvey Mudd College</a></p></li>
<li><p><a class="reference external" href="https://jonaslalin.com/2021/12/10/feedforward-neural-networks-part-1/"><em>“Feedforward Neural Networks in Depth, Part 1: Forward and Backward Propagations”</em>, Jonas Lalin</a></p></li>
<li><p><a class="reference external" href="http://neuralnetworksanddeeplearning.com/chap2.html"><em>“How the backpropagation algorithm works”</em>, Michael Nielsen (Neural Networks and Deep Learning)</a></p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./notebooks"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="02_logistic_regression.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">02: Logistic Regression</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="04_matrix_notation.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">04: Matrix Notation</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Jack Roberts<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>