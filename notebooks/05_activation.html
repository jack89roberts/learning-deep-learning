
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>05: Gradients and Activation Functions &#8212; Learning Deep Learning</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="06: Non-linear Regression" href="06_nonlinear_regression.html" />
    <link rel="prev" title="04: Matrix Notation" href="04_matrix_notation.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Learning Deep Learning</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Learning Deep Learning [WIP!]
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Fully Connected Neural Networks
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="01_linear_regression.html">
   01: Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="02_logistic_regression.html">
   02: Logistic Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03_backprop.html">
   03: Back Propagation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="04_matrix_notation.html">
   04: Matrix Notation
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   05: Gradients and Activation Functions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="06_nonlinear_regression.html">
   06: Non-linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="07_multiclass.html">
   07: Multiple Outputs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="08_regularisation.html">
   08 Regularisation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="09_optimisers.html">
   09: Optimisation Algorithms
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="10_experiments.html">
   10: Training and Tuning Networks
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/jack89roberts/learning-deep-learning/main?urlpath=tree/notebooks/05_activation.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/jack89roberts/learning-deep-learning"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/jack89roberts/learning-deep-learning/issues/new?title=Issue%20on%20page%20%2Fnotebooks/05_activation.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/jack89roberts/learning-deep-learning/edit/main/notebooks/05_activation.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Edit this page"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="headerbtn__text-container">suggest edit</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/notebooks/05_activation.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#activation-functions-recap">
   Activation Functions: Recap
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#repeating-terms-in-back-propagation">
   Repeating Terms in Back Propagation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-gradient-of-the-sigmoid-function">
   The Gradient of the Sigmoid Function
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#vanishing-gradients">
   Vanishing Gradients
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#why-are-vanishing-gradients-a-problem">
   Why are Vanishing Gradients a Problem?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#relu">
   ReLU
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#layer-network-example-with-relu">
   4-Layer Network Example with ReLU
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exploding-gradients">
   Exploding Gradients
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#other-activation-functions">
   Other Activation Functions
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>05: Gradients and Activation Functions</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#activation-functions-recap">
   Activation Functions: Recap
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#repeating-terms-in-back-propagation">
   Repeating Terms in Back Propagation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-gradient-of-the-sigmoid-function">
   The Gradient of the Sigmoid Function
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#vanishing-gradients">
   Vanishing Gradients
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#why-are-vanishing-gradients-a-problem">
   Why are Vanishing Gradients a Problem?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#relu">
   ReLU
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#layer-network-example-with-relu">
   4-Layer Network Example with ReLU
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exploding-gradients">
   Exploding Gradients
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#other-activation-functions">
   Other Activation Functions
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="gradients-and-activation-functions">
<h1>05: Gradients and Activation Functions<a class="headerlink" href="#gradients-and-activation-functions" title="Permalink to this headline">#</a></h1>
<p>Possible structure:</p>
<ul class="simple">
<li><p>Vanishing gradients</p>
<ul>
<li><p>Sigmoid</p></li>
<li><p>through backprop</p></li>
</ul>
</li>
<li><p>ReLu</p>
<ul>
<li><p>(leaky ReLu) - maybe</p></li>
</ul>
</li>
<li><p>Exploding gradients</p>
<ul>
<li><p>weight initialisation</p></li>
<li><p>gradient clipping</p></li>
</ul>
</li>
<li><p>(Normalisation?) - maybe</p>
<ul>
<li><p>(Tanh)</p></li>
<li><p>(Batch norm)</p></li>
</ul>
</li>
<li><p>What activation function to use</p>
<ul>
<li><p>output layer</p></li>
<li><p>hidden layers</p></li>
</ul>
</li>
</ul>
<section id="activation-functions-recap">
<h2>Activation Functions: Recap<a class="headerlink" href="#activation-functions-recap" title="Permalink to this headline">#</a></h2>
<p>Remember that the activation function, <span class="math notranslate nohighlight">\(g(z)\)</span>, is applied to the linear combination of a node’s inputs, <span class="math notranslate nohighlight">\(z\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{A}^{[l]} = g(\mathbf{Z}^{[l]})
\]</div>
<p>And so far we have seen two different activation functions:</p>
<ul class="simple">
<li><p><strong>Linear activation:</strong> <span class="math notranslate nohighlight">\(g(\mathbf{Z}^{[l]}) = \mathbf{Z}^{[l]}\)</span></p>
<ul>
<li><p>Output layer: Used for regression problems</p></li>
<li><p>Hidden layers: Usually not used (as then only linear functions can be fitted)</p></li>
</ul>
</li>
<li><p><strong>Sigmoid activation:</strong> <span class="math notranslate nohighlight">\(g(\mathbf{Z}^{[l]}) = \mathrm{sigmoid}\left(\mathbf{Z}^{[l]}\right) = \sigma\left(\mathbf{Z}^{[l]}\right)\)</span></p>
<ul>
<li><p>Output layer: Used for (binary) classification problems</p></li>
<li><p>Hidden layers: <em>Can</em> be used to fit non-linear functions, but generally not used (we’ll see why later)</p></li>
</ul>
</li>
</ul>
<p>To compute the <span class="math notranslate nohighlight">\(\color{blue}{\partial \mathbf{A}^{[l]} / \partial \mathbf{Z}^{[l]}}\)</span> terms we need to know the derivative of the activation function.</p>
</section>
<section id="repeating-terms-in-back-propagation">
<h2>Repeating Terms in Back Propagation<a class="headerlink" href="#repeating-terms-in-back-propagation" title="Permalink to this headline">#</a></h2>
<p>As an example, let’s consider a four-layer network:</p>
<p><img alt="" src="../_images/05_4layers.png" /></p>
<p>Each layer is represented by only one node in the diagram (but can actually contain many nodes without changing the equation below).</p>
<p>Here is the loss-gradient for the weights in the <strong>first</strong> layer of the <strong>four</strong>-layer network, in matrix notation:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial \mathbf{W}^{[1]}} =
\frac{1}{m}
\left\{
\color{red}{\mathbf{W}^{[2]^T}}
\left(
\color{red}{\mathbf{W}^{[3]^T}}
\left[
\color{red}{\mathbf{W}^{[4]^T}}
\left\{
\frac{\partial \mathcal{L}}{\partial \mathbf{A}^{[4]}} * \color{blue}{\frac{\partial \mathbf{A}^{[4]}}{\partial \mathbf{Z}^{[4]}}}
\right\}
* \color{blue}{\frac{\partial \mathbf{A}^{[3]}}{\partial \mathbf{Z}^{[3]}}}
\right]
* \color{blue}{\frac{\partial \mathbf{A}^{[2]}}{\partial \mathbf{Z}^{[2]}}}
\right) * \color{blue}{\frac{\partial \mathbf{A}^{[1]}}{\partial \mathbf{Z}^{[1]}}}
\right\}
\mathbf{A}^{[0]^T}
\]</div>
<p>where two groups of similar terms are highlighted:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\color{blue}{\partial \mathbf{A}^{[l]} / \partial \mathbf{Z}^{[l]}}\)</span>: <span style="color:blue">The gradient of the node activations in a layer (with respect to the node’s inputs)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\color{red}{\mathbf{W}^{[l]^T}}\)</span>: <span style="color:red">The weights in a layer</span>.</p></li>
</ul>
<p>and the other terms are:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\partial \mathcal{L} / \partial \mathbf{A}^{[4]} = \partial \mathcal{L} / \partial \mathbf{\hat{y}}\)</span>: The gradient of the loss with respect to our predictions (aka the activations of the last, in this case the fourth, layer)</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{A}^{[0]^T} = \mathbf{X}^T\)</span>: The input data (aka the activations of the zeroth layer).</p></li>
<li><p><span class="math notranslate nohighlight">\(1/m\)</span>: To take the mean gradient across the data batch (which has <span class="math notranslate nohighlight">\(m\)</span> samples)</p></li>
</ul>
<p>In this notebook, we’re going to focus on how, in certain situations, the form of this equation can make it difficult to learn (optimise) the weights of the network, and in particular the weights in the early layers of a deep network.</p>
</section>
<section id="the-gradient-of-the-sigmoid-function">
<h2>The Gradient of the Sigmoid Function<a class="headerlink" href="#the-gradient-of-the-sigmoid-function" title="Permalink to this headline">#</a></h2>
<p>We’re going to start by considering the activation gradient terms (<span class="math notranslate nohighlight">\(\color{blue}{\partial \mathbf{A}^{[l]} / \partial \mathbf{Z}^{[l]}}\)</span>) above.</p>
<p>The sigmoid function, <span class="math notranslate nohighlight">\(\sigma(z)\)</span>, is defined as:</p>
<div class="math notranslate nohighlight">
\[
\sigma(z) = \frac{1}{1 + \exp(-z)}
\]</div>
<p>and its derivative is:</p>
<div class="math notranslate nohighlight">
\[
\frac{\mathrm{d}\sigma}{\mathrm{d}z} = \sigma(z) \left[1 - \sigma(z) \right]
\]</div>
<p>They look like this:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>


<span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">dsigmoid_dz</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">))</span>


<span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;$\sigma(z)$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;z&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Sigmoid&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">dsigmoid_dz</span><span class="p">(</span><span class="n">z</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;orange&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;$\sigma&#39;(z)$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;z&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Gradient&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0.5, 1.0, &#39;Gradient&#39;)
</pre></div>
</div>
<img alt="../_images/05_activation_4_1.png" src="../_images/05_activation_4_1.png" />
</div>
</div>
<p>Note that:</p>
<ul class="simple">
<li><p>The maximum gradient of the sigmoid function is 0.25 (<span class="math notranslate nohighlight">\(0 \leq \mathrm{d}\sigma / \mathrm{d}z \leq 0.25\)</span>)</p></li>
<li><p>The gradient is close to zero for most values of <span class="math notranslate nohighlight">\(z\)</span> (especially for <span class="math notranslate nohighlight">\(z &lt; -5\)</span> and <span class="math notranslate nohighlight">\(z &gt; 5\)</span>)</p></li>
</ul>
</section>
<section id="vanishing-gradients">
<h2>Vanishing Gradients<a class="headerlink" href="#vanishing-gradients" title="Permalink to this headline">#</a></h2>
<p>To help gain intuition about why the form of the sigmoid’s gradient can be problematic, let’s consider the case where:</p>
<ul class="simple">
<li><p>our four-layer example network really does only have one node per layer</p></li>
<li><p>all the weights are one (<span class="math notranslate nohighlight">\(\mathbf{W}^{[1]} = \mathbf{W}^{[2]} = \mathbf{W}^{[3]} = \mathbf{W}^{[4]} = 1\)</span>)</p></li>
<li><p>all the biases are zero (<span class="math notranslate nohighlight">\(\mathbf{b}^{[1]} = \mathbf{b}^{[2]} = \mathbf{b}^{[3]} = \mathbf{b}^{[4]} = 0\)</span>)</p></li>
<li><p>there’s only one input (<span class="math notranslate nohighlight">\(x\)</span>).</p></li>
</ul>
<p>In this case the network computes:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{A^{[0]}} = \mathbf{Z}^{[1]} = x \\
\mathbf{A^{[1]}} = \mathbf{Z}^{[2]} = \sigma(x) \\
\mathbf{A^{[2]}} = \mathbf{Z}^{[3]} = \sigma(\sigma(x)) \\
\mathbf{A^{[3]}} = \mathbf{Z}^{[4]} = \sigma(\sigma(\sigma(x))) \\
\mathbf{A^{[4]}} = \hat{y} = \sigma(\sigma(\sigma((x)))) \\
\end{split}\]</div>
<p>And the loss gradient for the weights in each layer (for one data sample <span class="math notranslate nohighlight">\(x\)</span>) are:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial \mathbf{W}^{[4]}} =
\frac{\partial \mathcal{L}}{\partial \mathbf{A}^{[4]}}
\color{blue}{\frac{\partial \mathbf{A}^{[4]}}{\partial \mathbf{Z}^{[4]}}}
\mathbf{A}^{[3]}
\leq \frac{1}{4} \frac{\partial \mathcal{L}}{\partial \mathbf{A}^{[4]}}
\]</div>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial \mathbf{W}^{[3]}} =
\frac{\partial \mathcal{L}}{\partial \mathbf{A}^{[4]}}
\color{blue}{\frac{\partial \mathbf{A}^{[4]}}{\partial \mathbf{Z}^{[4]}}}
\color{blue}{\frac{\partial \mathbf{A}^{[3]}}{\partial \mathbf{Z}^{[3]}}}
\mathbf{A}^{[2]}
\leq \frac{1}{16} \frac{\partial \mathcal{L}}{\partial \mathbf{A}^{[4]}}
\]</div>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial \mathbf{W}^{[2]}} =
\frac{\partial \mathcal{L}}{\partial \mathbf{A}^{[4]}}
\color{blue}{\frac{\partial \mathbf{A}^{[4]}}{\partial \mathbf{Z}^{[4]}}}
\color{blue}{\frac{\partial \mathbf{A}^{[3]}}{\partial \mathbf{Z}^{[3]}}}
\color{blue}{\frac{\partial \mathbf{A}^{[2]}}{\partial \mathbf{Z}^{[2]}}}
\mathbf{A}^{[1]}
\leq \frac{1}{64} \frac{\partial \mathcal{L}}{\partial \mathbf{A}^{[4]}}
\]</div>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial \mathbf{W}^{[1]}} =
\frac{\partial \mathcal{L}}{\partial \mathbf{A}^{[4]}}
\color{blue}{\frac{\partial \mathbf{A}^{[4]}}{\partial \mathbf{Z}^{[4]}}}
\color{blue}{\frac{\partial \mathbf{A}^{[3]}}{\partial \mathbf{Z}^{[3]}}}
\color{blue}{\frac{\partial \mathbf{A}^{[2]}}{\partial \mathbf{Z}^{[2]}}}
\color{blue}{\frac{\partial \mathbf{A}^{[1]}}{\partial \mathbf{Z}^{[1]}}}
x
\leq \frac{1}{256} \frac{\partial \mathcal{L}}{\partial \mathbf{A}^{[4]}} x
\]</div>
<p>Where the inequalities follow from <span class="math notranslate nohighlight">\(\mathbf{A}^{[l]} = \sigma(\mathbf{Z}^{[l]}) \leq 1\)</span> and <span class="math notranslate nohighlight">\(\color{blue}{\partial \mathbf{A}^{[l]} / \partial \mathbf{Z}^{[l]}} = \partial \sigma(\mathbf{Z}^{[l]}) / \partial \mathbf{Z}^{[l]} \leq 0.25\)</span>.</p>
<p>Or visually, here are the activations and gradients in each layer plotted, assuming log loss is used and the true class is <span class="math notranslate nohighlight">\(y = 0\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">activations_gradients</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">layers</span><span class="p">,</span> <span class="n">g</span><span class="p">,</span> <span class="n">dg_dz</span><span class="p">,</span> <span class="n">dL_dyhat</span><span class="p">):</span>
    <span class="c1"># forward pass</span>
    <span class="n">A</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="n">x</span><span class="p">}</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">layers</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">Z</span><span class="p">[</span><span class="n">l</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">l</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
        <span class="n">A</span><span class="p">[</span><span class="n">l</span><span class="p">]</span> <span class="o">=</span> <span class="n">g</span><span class="p">(</span><span class="n">Z</span><span class="p">[</span><span class="n">l</span><span class="p">])</span>
    
    <span class="c1"># always sigmoid output layer</span>
    <span class="n">Z</span><span class="p">[</span><span class="n">layers</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
    <span class="n">A</span><span class="p">[</span><span class="n">layers</span><span class="p">]</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">Z</span><span class="p">[</span><span class="n">layers</span><span class="p">])</span>

    <span class="c1"># backward pass</span>
    <span class="c1"># always sigmoid output layer</span>
    <span class="n">dL_dZ</span> <span class="o">=</span> <span class="p">{</span><span class="n">layers</span><span class="p">:</span> <span class="n">dL_dyhat</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">A</span><span class="p">[</span><span class="n">layers</span><span class="p">])</span> <span class="o">*</span> <span class="n">dsigmoid_dz</span><span class="p">(</span><span class="n">Z</span><span class="p">[</span><span class="n">layers</span><span class="p">])}</span>
    <span class="n">dL_dW</span> <span class="o">=</span> <span class="p">{</span><span class="n">layers</span><span class="p">:</span> <span class="n">dL_dZ</span><span class="p">[</span><span class="n">layers</span><span class="p">]</span> <span class="o">*</span> <span class="n">A</span><span class="p">[</span><span class="n">layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]}</span>
    <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">dL_dZ</span><span class="p">[</span><span class="n">l</span><span class="p">]</span> <span class="o">=</span> <span class="n">dL_dZ</span><span class="p">[</span><span class="n">l</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">dg_dz</span><span class="p">(</span><span class="n">Z</span><span class="p">[</span><span class="n">l</span><span class="p">])</span>
        <span class="n">dL_dW</span><span class="p">[</span><span class="n">l</span><span class="p">]</span> <span class="o">=</span> <span class="n">dL_dZ</span><span class="p">[</span><span class="n">l</span><span class="p">]</span> <span class="o">*</span> <span class="n">A</span><span class="p">[</span><span class="n">l</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">A</span><span class="p">,</span> <span class="n">dL_dW</span>


<span class="k">def</span> <span class="nf">dL_dyhat</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">yhat</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Log loss derivative&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="o">-</span> <span class="n">y</span> <span class="o">/</span> <span class="n">yhat</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">yhat</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">plot_activations_gradients</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">dL_dW</span><span class="p">):</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

    <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">layers</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">A</span><span class="p">[</span><span class="n">l</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Layer </span><span class="si">{</span><span class="n">l</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dL_dW</span><span class="p">[</span><span class="n">l</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Layer </span><span class="si">{</span><span class="n">l</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Layer Activations&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span>
        <span class="sa">r</span><span class="s2">&quot;Layer Loss-Gradients ($\partial \mathcal</span><span class="si">{L}</span><span class="s2"> / \partial W^{[l]}$) if &quot;</span>
        <span class="sa">f</span><span class="s2">&quot;y = </span><span class="si">{</span><span class="n">y</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">layers</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">A</span><span class="p">,</span> <span class="n">dL_dW</span> <span class="o">=</span> <span class="n">activations_gradients</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">layers</span><span class="p">,</span> <span class="n">sigmoid</span><span class="p">,</span> <span class="n">dsigmoid_dz</span><span class="p">,</span> <span class="n">dL_dyhat</span><span class="p">)</span>

<span class="n">plot_activations_gradients</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">dL_dW</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/05_activation_8_0.png" src="../_images/05_activation_8_0.png" />
</div>
</div>
<p>In both the equations and the plot we can see the gradients are getting closer and closer to zero as we move to earlier layers in the network - they are <em><strong>vanishing</strong></em>.</p>
</section>
<section id="why-are-vanishing-gradients-a-problem">
<h2>Why are Vanishing Gradients a Problem?<a class="headerlink" href="#why-are-vanishing-gradients-a-problem" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>We want to train bigger, deeper networks to learn more complex functions</p>
<ul>
<li><p>We update the weights of the network with gradient descent, computing the derivative of the loss with respect to the weights</p>
<ul>
<li><p>If the gradients are close to zero in a layer we won’t change the weights in those layers</p>
<ul>
<li><p>With sigmoid activation the gradients get closer and closer to zero as we move back through the layers</p>
<ul>
<li><p>We stop learning (don’t update the weights) in earlier layers!</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section id="relu">
<h2>ReLU<a class="headerlink" href="#relu" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">z</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">r</span><span class="p">[</span><span class="n">r</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">return</span> <span class="n">r</span>


<span class="k">def</span> <span class="nf">drelu_dz</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="n">dr</span> <span class="o">=</span> <span class="n">z</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">dr</span><span class="p">[</span><span class="n">z</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">dr</span><span class="p">[</span><span class="n">z</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">dr</span>


<span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">relu</span><span class="p">(</span><span class="n">z</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;$\sigma(z)$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;z&quot;</span><span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">drelu_dz</span><span class="p">(</span><span class="n">z</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;orange&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;$\sigma&#39;(z)$&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;z&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0.5, 0, &#39;z&#39;)
</pre></div>
</div>
<img alt="../_images/05_activation_12_1.png" src="../_images/05_activation_12_1.png" />
</div>
</div>
</section>
<section id="layer-network-example-with-relu">
<h2>4-Layer Network Example with ReLU<a class="headerlink" href="#layer-network-example-with-relu" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>but still sigmoid on output</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">layers</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">A</span><span class="p">,</span> <span class="n">dL_dW</span> <span class="o">=</span> <span class="n">activations_gradients</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">layers</span><span class="p">,</span> <span class="n">relu</span><span class="p">,</span> <span class="n">drelu_dz</span><span class="p">,</span> <span class="n">dL_dyhat</span><span class="p">)</span>

<span class="n">plot_activations_gradients</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">dL_dW</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/05_activation_14_0.png" src="../_images/05_activation_14_0.png" />
</div>
</div>
</section>
<section id="exploding-gradients">
<h2>Exploding Gradients<a class="headerlink" href="#exploding-gradients" title="Permalink to this headline">#</a></h2>
</section>
<section id="other-activation-functions">
<h2>Other Activation Functions<a class="headerlink" href="#other-activation-functions" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>tanh, normalisation</p></li>
<li><p>leaky relu</p></li>
<li><p>others</p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./notebooks"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="04_matrix_notation.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">04: Matrix Notation</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="06_nonlinear_regression.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">06: Non-linear Regression</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Jack Roberts<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>